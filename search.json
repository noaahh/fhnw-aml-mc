[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Data Import & Wrangling",
    "section": "",
    "text": "import random\nfrom collections import OrderedDict\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_theme()\n# plt.style.use('seaborn-white')\n# plt.style.use('ggplot')\n\ndata_reduction = OrderedDict()\n\nSEED = 1337\n\ndef seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything(SEED)"
  },
  {
    "objectID": "main.html#data-preparation-non-transactional-data",
    "href": "main.html#data-preparation-non-transactional-data",
    "title": "Data Import & Wrangling",
    "section": "1 Data Preparation: Non-Transactional Data",
    "text": "1 Data Preparation: Non-Transactional Data\n\norders_pivot_df = orders_df.pivot_table(\n    index=\"account_id\",\n    columns=\"k_symbol\",\n    values=\"debited_amount\",\n    aggfunc=\"sum\",\n    fill_value=0,\n)\n\norders_pivot_df.columns = [\n    f\"k_symbol_debited_sum_{col.lower()}\" for col in orders_pivot_df.columns\n]\norders_pivot_df = orders_pivot_df.reset_index()  # Use created index as account_id\norders_pivot_df.head()\n\n\n\n\n\n\n\n\naccount_id\nk_symbol_debited_sum_household\nk_symbol_debited_sum_insurance_payment\nk_symbol_debited_sum_leasing\nk_symbol_debited_sum_loan_payment\nk_symbol_debited_sum_na\n\n\n\n\n0\n1\n2452.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2\n7266.0\n0.0\n0.0\n3372.7\n0.0\n\n\n2\n3\n1135.0\n3539.0\n0.0\n0.0\n327.0\n\n\n3\n4\n3363.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n5\n2668.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ndef merge_non_transactional_data(\n    clients, districts, dispositions, accounts, orders, loans, cards\n):\n    # Rename district_id for clarity in clients and accounts DataFrames\n    clients = clients.rename(columns={\"district_id\": \"client_district_id\"})\n    accounts = accounts.rename(columns={\"district_id\": \"account_district_id\"})\n\n    # Prepare districts dataframe for merge with prefix for clients and accounts\n    districts_client_prefixed = districts.add_prefix(\"client_\")\n    districts_account_prefixed = districts.add_prefix(\"account_\")\n\n    # Merge district information for clients and accounts with prefixed columns\n    clients_with_districts = pd.merge(\n        clients,\n        districts_client_prefixed,\n        left_on=\"client_district_id\",\n        right_on=\"client_district_id\",\n        how=\"left\",\n    )\n    accounts_with_districts = pd.merge(\n        accounts,\n        districts_account_prefixed,\n        left_on=\"account_district_id\",\n        right_on=\"account_district_id\",\n        how=\"left\",\n    )\n\n    # Merge cards with dispositions and prefix card-related columns to avoid confusion\n    cards_prefixed = cards.add_prefix(\"card_\")\n    dispositions_with_cards = pd.merge(\n        dispositions,\n        cards_prefixed,\n        left_on=\"disp_id\",\n        right_on=\"card_disp_id\",\n        how=\"left\",\n    )\n\n    # Merge clients (with district info) with dispositions and cards\n    # Assuming dispositions might have columns that overlap with clients, prefix those if necessary\n    clients_dispositions_cards = pd.merge(\n        dispositions_with_cards, clients_with_districts, on=\"client_id\", how=\"left\"\n    )\n\n    # Merge the above with accounts (with district info) on account_id\n    accounts_clients_cards = pd.merge(\n        accounts_with_districts, clients_dispositions_cards, on=\"account_id\", how=\"left\"\n    )\n\n    # Merge orders DataFrame, assuming orders might contain columns that could overlap, prefix as needed\n    orders_prefixed = orders.add_prefix(\"order_\")\n    comprehensive_df_with_orders = pd.merge(\n        accounts_clients_cards,\n        orders_prefixed,\n        left_on=\"account_id\",\n        right_on=\"order_account_id\",\n        how=\"left\",\n    )\n\n    # Merge loans with the comprehensive dataframe (now including orders) on account_id\n    # Prefix loan-related columns to maintain clarity\n    loans_prefixed = loans.add_prefix(\"loan_\")\n    final_df = pd.merge(\n        comprehensive_df_with_orders,\n        loans_prefixed,\n        left_on=\"account_id\",\n        right_on=\"loan_account_id\",\n        how=\"left\",\n    )\n\n    final_df[\"account_created\"] = pd.to_datetime(final_df[\"account_created\"])\n    final_df[\"card_issued\"] = pd.to_datetime(final_df[\"card_issued\"])\n    final_df[\"has_card\"] = final_df[\"card_issued\"].notna()\n    return final_df\n\n\nnon_transactional_df = merge_non_transactional_data(\n    clients_df,\n    districts_df,\n    dispositions_df,\n    accounts_df,\n    orders_pivot_df,\n    loans_df,\n    cards_df,\n)\nnon_transactional_df.to_csv(\"data/non_transactional.csv\", index=False)\nnon_transactional_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4500 entries, 0 to 4499\nData columns (total 59 columns):\n #   Column                                        Non-Null Count  Dtype         \n---  ------                                        --------------  -----         \n 0   account_id                                    4500 non-null   int64         \n 1   account_district_id                           4500 non-null   int64         \n 2   account_frequency                             4500 non-null   object        \n 3   account_created                               4500 non-null   datetime64[ns]\n 4   account_district_name                         4500 non-null   object        \n 5   account_region                                4500 non-null   object        \n 6   account_inhabitants                           4500 non-null   int64         \n 7   account_small_municipalities                  4500 non-null   int64         \n 8   account_medium_municipalities                 4500 non-null   int64         \n 9   account_large_municipalities                  4500 non-null   int64         \n 10  account_huge_municipalities                   4500 non-null   int64         \n 11  account_cities                                4500 non-null   int64         \n 12  account_ratio_urban_inhabitants               4500 non-null   float64       \n 13  account_average_salary                        4500 non-null   int64         \n 14  account_unemployment_rate_1995                4452 non-null   float64       \n 15  account_unemployment_rate_1996                4500 non-null   float64       \n 16  account_entrepreneurs_per_1000_inhabitants    4500 non-null   int64         \n 17  account_crimes_committed_1995                 4452 non-null   float64       \n 18  account_crimes_committed_1996                 4500 non-null   int64         \n 19  disp_id                                       4500 non-null   int64         \n 20  client_id                                     4500 non-null   int64         \n 21  type                                          4500 non-null   object        \n 22  card_card_id                                  892 non-null    float64       \n 23  card_disp_id                                  892 non-null    float64       \n 24  card_type                                     892 non-null    object        \n 25  card_issued                                   892 non-null    datetime64[ns]\n 26  client_district_id                            4500 non-null   int64         \n 27  sex                                           4500 non-null   object        \n 28  birth_date                                    4500 non-null   datetime64[ns]\n 29  age                                           4500 non-null   int64         \n 30  client_district_name                          4500 non-null   object        \n 31  client_region                                 4500 non-null   object        \n 32  client_inhabitants                            4500 non-null   int64         \n 33  client_small_municipalities                   4500 non-null   int64         \n 34  client_medium_municipalities                  4500 non-null   int64         \n 35  client_large_municipalities                   4500 non-null   int64         \n 36  client_huge_municipalities                    4500 non-null   int64         \n 37  client_cities                                 4500 non-null   int64         \n 38  client_ratio_urban_inhabitants                4500 non-null   float64       \n 39  client_average_salary                         4500 non-null   int64         \n 40  client_unemployment_rate_1995                 4448 non-null   float64       \n 41  client_unemployment_rate_1996                 4500 non-null   float64       \n 42  client_entrepreneurs_per_1000_inhabitants     4500 non-null   int64         \n 43  client_crimes_committed_1995                  4448 non-null   float64       \n 44  client_crimes_committed_1996                  4500 non-null   int64         \n 45  order_account_id                              3758 non-null   float64       \n 46  order_k_symbol_debited_sum_household          3758 non-null   float64       \n 47  order_k_symbol_debited_sum_insurance_payment  3758 non-null   float64       \n 48  order_k_symbol_debited_sum_leasing            3758 non-null   float64       \n 49  order_k_symbol_debited_sum_loan_payment       3758 non-null   float64       \n 50  order_k_symbol_debited_sum_na                 3758 non-null   float64       \n 51  loan_loan_id                                  682 non-null    float64       \n 52  loan_account_id                               682 non-null    float64       \n 53  loan_granted_date                             682 non-null    datetime64[ns]\n 54  loan_amount                                   682 non-null    float64       \n 55  loan_duration                                 682 non-null    float64       \n 56  loan_monthly_payments                         682 non-null    float64       \n 57  loan_status                                   682 non-null    object        \n 58  has_card                                      4500 non-null   bool          \ndtypes: bool(1), datetime64[ns](4), float64(21), int64(24), object(9)\nmemory usage: 2.0+ MB"
  },
  {
    "objectID": "main.html#exploratory-data-analysis",
    "href": "main.html#exploratory-data-analysis",
    "title": "Data Import & Wrangling",
    "section": "2 Exploratory Data Analysis",
    "text": "2 Exploratory Data Analysis\n\n2.1 Non-transactional Data\n\n2.1.1 Card Holders\n\nplt.figure(figsize=(10, 6))\nplt.title(\"Number of Clients by Card Type\")\nsns.barplot(\n    x=[\"No Card\", \"Classic/Gold Card Holders\", \"Junior Card Holders\"],\n    y=[\n        non_transactional_df[\"card_type\"].isna().sum(),\n        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n    ],\n)\n# ensure that the number of clients is shown on the bars\nfor i, v in enumerate(\n    [\n        non_transactional_df[\"card_type\"].isna().sum(),\n        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n    ]\n):\n    plt.text(i, v + 10, str(v), ha=\"center\", va=\"bottom\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the distribution of card holders in general we can see that the most clients are not in a possession of a credit card.\n\nplt.figure(figsize=(10, 6))\nplt.title(\n    f'Distribution of Age for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\"age\"],\n    kde=True,\n    bins=30,\n)\nplt.xlabel(\"Age of Client (presumably in 1999)\")\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the age distribution of Junior Card holders paints a picture on this group, however only looking at the current age may be misleading as we need to understand how old they were when the card was issued to determine if they could have been eligble for a Classic/Gold card (at least 18 when the card was issued).\n\nnon_transactional_df[\"card_issued\"] = pd.to_datetime(\n    non_transactional_df[\"card_issued\"]\n)\n\nnon_transactional_df[\"age_at_card_issuance\"] = (\n    non_transactional_df[\"card_issued\"] - non_transactional_df[\"birth_date\"]\n)\nnon_transactional_df[\"age_at_card_issuance\"] = (\n    non_transactional_df[\"age_at_card_issuance\"].dt.days // 365\n)\n\nplt.figure(figsize=(10, 6))\nplt.title(\n    f'Distribution of Age at Card Issuance for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=30,\n)\nplt.xlabel(\"Age at Card Issuance\")\nplt.show()\n\n\n\n\n\n\n\n\nHere we can see that roughly 1/3 of the Junior Card holders were not of legal age (assuming legal age is 18) when receiving their Junior Card.\n\nplt.figure(figsize=(10, 6))\nplt.title(\n    f\"Distribution of Age at Card Issuance for All Card Types\\n total count = {len(non_transactional_df)}\"\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=10,\n    color=\"blue\",\n    label=\"Junior Card Holders\",\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] != \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=30,\n    color=\"red\",\n    label=\"Non-Junior Card Holders\",\n)\nplt.legend()\nplt.xlabel(\"Age at Card Issuance\")\nplt.show()\n\n\n\n\n\n\n\n\nComparing the age at issue date between Junior and non-Junior (Classic/Gold) card holders shows that there is no overlap between the two groups, which makes intutively sense.\nTherefore removing the subset of Junior Cards seems as valid as there is no reason to believe that there are Junior Cards issued wrongly, the subset being relatively small compared to the remaining issued cards and the fact that our target is specifically Classic/Gold Card owners.\n\nbefore_len = len(non_transactional_df)\nnon_transactional_df = non_transactional_df[\n    non_transactional_df[\"card_type\"] != \"junior\"\n]\ndata_reduction[\"Junior Card Holders\"] = -(before_len - len(non_transactional_df))\ndel before_len\n\nLooking at the age distribution of Junior card holders and their occurence in comparison it seems valid to remove them as they are not the target group and make up a small subset of the complete dataset.\n\n\n2.1.2 Time factors on Card Status\nThe time between creating an account and issuing a card may also be important when filtering customers based on their history. We should avoid filtering out potentially interesting periods and understand how the timespans between account creation and card issuance are distributed.\n\nnon_transactional_w_cards_df = non_transactional_df[\n    non_transactional_df[\"card_issued\"].notna()\n    & non_transactional_df[\"account_created\"].notna()\n]\nnon_transactional_w_cards_df[\"duration_days\"] = (\n    non_transactional_w_cards_df[\"card_issued\"]\n    - non_transactional_w_cards_df[\"account_created\"]\n).dt.days\n\nplt.figure(figsize=(12, 8))\nsns.histplot(\n    non_transactional_w_cards_df[\"duration_days\"], bins=50, edgecolor=\"black\", kde=True\n)\nplt.title(\"Distribution of Duration Between Account Creation and Card Issuance\")\nplt.xlabel(\"Duration in Days\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1827/3373247402.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nThe histogram displays a distribution with multiple peaks, indicating that there are several typical time frames for card issuance after account creation. The highest peak occurs within the first 250 days, suggesting that a significant number of cards are issued during this period. The frequency decreases as duration increases, with noticeable peaks that may correspond to specific processing batch cycles or policy changes over time. The distribution also has a long tail, suggesting that in some cases, card issuance can take a very long time.\nAnalyzing the length of time a client has been with the bank in relation to their account creation date and card ownership can provide valuable insights for a bank’s customer relationship management and product targeting strategies. Long-standing clients may exhibit different banking behaviors, such as product adoption and loyalty patterns, compared to newer clients.\n\nmax_account_creation_date = non_transactional_df[\"card_issued\"].max()\n\nnon_transactional_df[\"client_tenure_years_relative\"] = (\n    max_account_creation_date - non_transactional_df[\"account_created\"]\n).dt.days / 365.25\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=non_transactional_df,\n    x=\"client_tenure_years_relative\",\n    hue=\"has_card\",\n    multiple=\"stack\",\n    binwidth=1,\n    stat=\"percent\",\n)\n\n# Call the function to add labels\nadd_percentage_labels(ax, non_transactional_df[\"has_card\"].unique())\n\n# Additional plot formatting\nplt.title(\"Client Tenure Relative to Latest Card Issued Date and Card Ownership\")\nplt.xlabel(\"Client Tenure (Years, Relative to Latest Card Issuance)\")\nplt.ylabel(\"Percentage of Clients\")\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart shows the tenure of clients in years, categorized by whether they own a credit card (True) or not (False). Each bar represents the percentage of clients within a specific tenure range, allowing for comparison of the distribution of card ownership among clients with different lengths of association with the bank.\n\n\n2.1.3 Demographics\nUsing the available demographic data, we can investigate the potential correlation between demographic data and card status. The average salary may indicate a difference between cardholders and non-cardholders, as it is reasonable to assume that cardholders have a higher average salary than non-cardholders.\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=\"has_card\", y=\"client_average_salary\", data=non_transactional_df)\nplt.title(\"Average Salary in Client's Region by Card Ownership\")\nplt.xlabel(\"Has Card\")\nplt.ylabel(\"Average Salary\")\nplt.xticks([0, 1], [\"No Card Owner\", \"Card Owner\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot compares the average salaries of clients who own a credit card with those who do not. Both groups have a substantial overlap in salary ranges, suggesting that while there might be a trend for card owners to have higher salaries, the difference is not significant. The median salary for card owners is slightly higher than that for non-card owners, as indicated by the median line within the respective boxes.\nBoth distributions have outliers on the higher end, indicating that some individuals have salaries significantly above the average in both groups. However, these outliers do not dominate the general trend.\nIt should also be noted that this plot assumes that the average salary of the region’s clients remained constant over the years, which is unlikely to be true.\nThe group of bar charts represents the distribution of credit card ownership across various demographics, showing the percentage of clients with and without cards within different age groups, sexes, and regions.\n\nnon_transactional_df[\"age_group\"] = pd.cut(\n    non_transactional_df[\"age\"],\n    bins=[0, 25, 40, 55, 70, 100],\n    labels=[\"&lt;25\", \"25-40\", \"40-55\", \"55-70\", \"&gt;70\"],\n)\n\nplt.figure(figsize=(15, 15))\n\n# Age Group\nplt.subplot(3, 1, 1)\nage_group_counts = (\n    non_transactional_df.groupby([\"age_group\", \"has_card\"]).size().unstack(fill_value=0)\n)\nage_group_percentages = (age_group_counts.T / age_group_counts.sum(axis=1)).T * 100\nage_group_plot = age_group_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nage_group_plot.set_title(\"Card Ownership by Age Group\")\nage_group_plot.set_ylabel(\"Percentage\")\nadd_percentage_labels(age_group_plot, non_transactional_df[\"has_card\"].unique())\n\n# Sex\nplt.subplot(3, 1, 2)\nsex_counts = (\n    non_transactional_df.groupby([\"sex\", \"has_card\"]).size().unstack(fill_value=0)\n)\nsex_percentages = (sex_counts.T / sex_counts.sum(axis=1)).T * 100\nsex_plot = sex_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nsex_plot.set_title(\"Card Ownership by Sex\")\nsex_plot.set_ylabel(\"Percentage\")\nadd_percentage_labels(sex_plot, non_transactional_df[\"has_card\"].unique())\n\n# Client Region\nplt.subplot(3, 1, 3)\nregion_counts = (\n    non_transactional_df.groupby([\"client_region\", \"has_card\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_percentages = (region_counts.T / region_counts.sum(axis=1)).T * 100\nregion_plot = region_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nregion_plot.set_title(\"Card Ownership by Client Region\")\nregion_plot.set_ylabel(\"Percentage\")\nregion_plot.tick_params(axis=\"x\", rotation=45)\nadd_percentage_labels(region_plot, non_transactional_df[\"has_card\"].unique())\n\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1827/3329244181.py:12: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nCard Ownership by Age Group: The bar chart displays the proportion of cardholders in different age groups. The percentage of cardholders is lowest in the age group of over 70, followed by the age group of 55-70, indicating that card ownership is more prevalent among younger demographics.\nCard Ownership by Sex: The bar chart shows the breakdown of card ownership by sex. The data reveals that the percentage of cardholders is comparable between both sexes, and no significant difference is present.\nCard Ownership by Region The bar chart at the bottom illustrates card ownership across different regions, showing a relatively consistent pattern among most regions.\n\n\n2.1.4 Impact of Loans / Debt\n\nsimplified_loan_status_mapping = {\n    \"Contract finished, no problems\": \"Finished\",\n    \"Contract finished, loan not paid\": \"Not Paid\",\n    \"Contract running, OK thus-far\": \"Running\",\n    \"Contract running, client in debt\": \"In Debt\",\n    \"No Loan\": \"No Loan\",\n}\n\nnon_transactional_df[\"loan_status_simplified\"] = non_transactional_df[\n    \"loan_status\"\n].map(simplified_loan_status_mapping)\n\n## this variable wants to kill itself\nloan_status_simplified_card_ownership_counts = (\n    non_transactional_df.groupby([\"loan_status_simplified\", \"has_card\"])\n    .size()\n    .unstack(fill_value=0)\n)\nloan_status_simplified_card_ownership_percentages = (\n    loan_status_simplified_card_ownership_counts.T\n    / loan_status_simplified_card_ownership_counts.sum(axis=1)\n).T * 100\n\nloan_status_simplified_card_ownership_percentages.plot(\n    kind=\"bar\", stacked=True, figsize=(10, 6)\n)\nplt.title(\"Interaction Between Simplified Loan Status and Card Ownership\")\nplt.xlabel(\"Simplified Loan Status\")\nplt.ylabel(\"Percentage of Clients\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Has Card\", labels=[\"No Card\", \"Has Card\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2 Transactional Data\nTODO: Add more EDA for transactional data\n\nzero_amount_transactions_df = transactions_df[transactions_df[\"amount\"] == 0]\n\nzero_amount_transactions_info = {\n    \"total_zero_amount_transactions\": len(zero_amount_transactions_df),\n    \"unique_accounts_with_zero_amount\": zero_amount_transactions_df[\n        \"account_id\"\n    ].nunique(),\n    \"transaction_type_distribution\": zero_amount_transactions_df[\n        \"transaction_type\"\n    ].value_counts(normalize=True),\n    \"operation_distribution\": zero_amount_transactions_df[\"operation\"].value_counts(\n        normalize=True\n    ),\n    \"k_symbol_distribution\": zero_amount_transactions_df[\"k_symbol\"].value_counts(\n        normalize=True\n    ),\n}\n\nzero_amount_transactions_info, len(zero_amount_transactions_info)\n\n({'total_zero_amount_transactions': 14,\n  'unique_accounts_with_zero_amount': 12,\n  'transaction_type_distribution': transaction_type\n  Withdrawal    0.714286\n  Credit        0.285714\n  Name: proportion, dtype: float64,\n  'operation_distribution': operation\n  Withdrawal in Cash    0.714286\n  NA                    0.285714\n  Name: proportion, dtype: float64,\n  'k_symbol_distribution': k_symbol\n  Sanction Interest    0.714286\n  Interest Credited    0.285714\n  Name: proportion, dtype: float64},\n 5)\n\n\n\naccounts_with_zero_amount_transactions = accounts_df[\n    accounts_df[\"account_id\"].isin(zero_amount_transactions_df[\"account_id\"].unique())\n]\naccounts_with_zero_amount_transactions\n\n\n\n\n\n\n\n\naccount_id\ndistrict_id\naccount_frequency\naccount_created\n\n\n\n\n178\n5369\n54\nMONTHLY_ISSUANCE\n1993-02-25\n\n\n289\n5483\n13\nMONTHLY_ISSUANCE\n1993-03-28\n\n\n496\n5129\n68\nMONTHLY_ISSUANCE\n1993-06-08\n\n\n513\n1475\n1\nWEEKLY_ISSUANCE\n1993-06-14\n\n\n799\n9337\n30\nMONTHLY_ISSUANCE\n1993-09-13\n\n\n896\n102\n11\nMONTHLY_ISSUANCE\n1993-10-16\n\n\n986\n8957\n1\nMONTHLY_ISSUANCE\n1993-11-13\n\n\n2033\n5125\n1\nMONTHLY_ISSUANCE\n1995-09-14\n\n\n2300\n9051\n5\nWEEKLY_ISSUANCE\n1996-01-17\n\n\n2651\n3859\n53\nMONTHLY_ISSUANCE\n1996-04-23\n\n\n3212\n6083\n6\nWEEKLY_ISSUANCE\n1996-09-19\n\n\n3342\n1330\n68\nMONTHLY_ISSUANCE\n1996-10-22\n\n\n\n\n\n\n\n\n# Clean up unnecessary variables\ndel accounts_with_zero_amount_transactions\ndel zero_amount_transactions_df\ndel zero_amount_transactions_info\n\nValidating first transactions where the amount equals the balance is essential for the integrity of our aggregated data analysis. This specific assertion underpins the reliability of our subsequent aggregation operations by ensuring each account’s financial history starts from a verifiable point.\n\ndef validate_first_transactions(transactions):\n    \"\"\"\n    Validates that for each account in the transactions DataFrame, there is at least\n    one transaction where the amount equals the balance on the account's first transaction date.\n\n    Parameters:\n    - transactions (pd.DataFrame): DataFrame containing transaction data with columns\n      'account_id', 'date', 'amount', and 'balance'.\n\n    Raises:\n    - AssertionError: If not every account has a first transaction where the amount equals the balance.\n    \"\"\"\n\n    first_dates = (\n        transactions.groupby(\"account_id\")[\"date\"].min().reset_index(name=\"first_date\")\n    )\n\n    first_trans = pd.merge(transactions, first_dates, how=\"left\", on=[\"account_id\"])\n\n    first_trans_filtered = first_trans[\n        (first_trans[\"date\"] == first_trans[\"first_date\"])\n        & (first_trans[\"amount\"] == first_trans[\"balance\"])\n    ]\n\n    first_trans_filtered = first_trans_filtered.drop_duplicates(subset=[\"account_id\"])\n\n    unique_accounts = transactions[\"account_id\"].nunique()\n    assert (\n        unique_accounts == first_trans_filtered[\"account_id\"].nunique()\n    ), \"Not every account has a first transaction where the amount equals the balance.\"\n\n    return \"Validation successful: Each account has a first transaction where the amount equals the balance.\"\n\n\nvalidate_first_transactions(transactions_df)\n\n'Validation successful: Each account has a first transaction where the amount equals the balance.'\n\n\nWe can confirm the truth of the assertions made. It is certain that there is a transaction with an amount equal to the balance in the transaction history of any account on the first date.\n\n### DEPENDENCY 1 TODO REMOVE FOR MERGE \nimport json\n# save transactions_df to temp as parquet\n\ntransactions_df.to_parquet(\"temp/transactions.parquet\")\naccounts_df.to_parquet(\"temp/accounts.parquet\")\nnon_transactional_df.to_parquet(\"temp/non_transactional.parquet\")\n\n# save data reduction\nwith open(\"temp/data_reduction.json\", \"w\") as f:\n    json.dump(data_reduction, f)\n\n\n### DEPENDENCY #TODO REMOVE FOR MERGE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\n\ntransactions_df = pd.read_parquet(\"temp/transactions.parquet\")\naccounts_df = pd.read_parquet(\"temp/accounts.parquet\")\nnon_transactional_df = pd.read_parquet(\"temp/non_transactional.parquet\")\n# read data_reduction from temp/data_reduction.json\nwith open(\"temp/data_reduction.json\", \"r\") as f:\n    data_reduction = json.load(f)"
  },
  {
    "objectID": "main.html#data-preparation-transactional-data",
    "href": "main.html#data-preparation-transactional-data",
    "title": "Data Import & Wrangling",
    "section": "3 Data Preparation: Transactional Data",
    "text": "3 Data Preparation: Transactional Data\n\n3.1 Set artificial issue date for non-card holders\n\ndef add_months_since_account_to_card(df):\n    df[\"months_since_account_to_card\"] = df.apply(\n        lambda row: (\n            (\n                row[\"card_issued\"].to_period(\"M\")\n                - row[\"account_created\"].to_period(\"M\")\n            ).n\n            if pd.notnull(row[\"card_issued\"]) and pd.notnull(row[\"account_created\"])\n            else np.nan\n        ),\n        axis=1,\n    )\n    return df\n\n\ndef filter_clients_without_sufficient_history(\n    non_transactional_df, min_history_months=25\n):\n    if \"months_since_account_to_card\" not in non_transactional_df.columns:\n        print(\n            \"Warning: months_since_account_to_card column not found. Calculating history length.\"\n        )\n        non_transactional_df = add_months_since_account_to_card(non_transactional_df)\n\n    count_before = len(non_transactional_df)\n    filtered_df = non_transactional_df[\n        non_transactional_df[\"months_since_account_to_card\"].isnull()\n        | (non_transactional_df[\"months_since_account_to_card\"] &gt;= min_history_months)\n    ]\n    print(\n        f\"Filtered out {count_before - len(filtered_df)} records with less than {min_history_months} months of history. Percentage: {(count_before - len(filtered_df)) / count_before * 100:.2f}%.\"\n    )\n    return filtered_df\n\n\nbefore_len = len(non_transactional_df)\nnon_transactional_w_sufficient_history_df = filter_clients_without_sufficient_history(\n    non_transactional_df\n)\ndata_reduction[\"Clients without sufficient history\"] = -(\n    before_len - len(non_transactional_w_sufficient_history_df)\n)\ndel before_len\n\nWarning: months_since_account_to_card column not found. Calculating history length.\nFiltered out 419 records with less than 25 months of history. Percentage: 9.62%.\n\n\n\nnon_transactional_w_card_df = non_transactional_w_sufficient_history_df.dropna(\n    subset=[\"card_issued\"]\n).copy()\n\nplt.figure(figsize=(12, 8))\nsns.histplot(\n    non_transactional_w_card_df[\"months_since_account_to_card\"], kde=True, bins=30\n)\nplt.title(\n    \"Distribution of Months from Account Creation to Card Issuance (for Card Holders)\"\n)\nplt.xlabel(\"Months\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.2 Match by similar transaction activity\nThe following approaches were considered to match non-card holders with card holders:\n\nLooking at the distributions above extract the amount of history a buyer most likely has at the issue data of the card\nFor each non buyer, find a buyer which was active in a similar time window (Jaccard similarity on the Year-Month sets). Instead of looking at the full activity of a buyer, we only look at the pre-purchase activity as there is reason to believe that clients may change their patterns after purchasing date and therefore add unwanted bias.\n\nThe second approach is chosen as it is provides an intuitive way to match clients based on their activity which is not only explainable but also provides a way to match clients based on their behavior. It strikes a balance of not finding a perfect match but a good enough match to focus on the discriminative features of the data.\nThe following image serves as an technical overview of the matching process: \nThe process emphasizes matching based on the timing of activity, rather than a wide array of characteristics. By identifying when both existing cardholders and non-cardholders interacted with the bank, we can infer a level of behavioral alignment that extends beyond mere transactional data. This alignment suggests a shared response to external conditions.\nThe resolution of the activity matrix is a binary matrix where each row represents a client and each column represents a month. A value of 1 indicates activity in a given month, while 0 indicates inactivity. Therefore we concentrate on the periods during which clients engage with the bank in the form of transactions\nAssumption: This assumes that clients active during similar periods might be influenced by the same economic and societal conditions, providing a more nuanced foundation for establishing connections between current cardholders and potential new ones.\n\n3.2.1 Construction of the Activity Matrix\nThe activity matrix serves as the foundation of our matching process, mapping out the engagement of clients with our services over time. It is constructed from transaction data, organizing client interactions into a structured format that highlights periods of activity.\n\nData Aggregation: We start with transaction data, which records each client’s interactions across various months. This data includes every transaction made by both current cardholders and potential non-cardholders.\nTemporal Transformation: Each transaction is associated with a specific date. These dates are then transformed into monthly periods, consolidating daily transactions into a monthly view of activity. This step simplifies the data, focusing on the presence of activity within each month rather than the specific dates or frequencies of transactions.\nMatrix Structure: The transformed data is arranged into a matrix format. Rows represent individual clients, identified by their account IDs. Columns correspond to monthly periods, spanning the entire range of months covered by the transaction data.\nActivity Indication: In the matrix, a cell value is set to indicate the presence of activity for a given client in a given month. If a client made one or more transactions in a month, the corresponding cell is marked to reflect this activity. The absence of transactions for a client in a month leaves the cell unmarked.\nBinary Representation: The final step involves converting the activity indicators into a binary format. Active months are represented by a ‘1’, indicating the presence of transactions, while inactive months are denoted by a ‘0’, indicating no transactions.\n\nThe heatmap provided offers a visual representation of the activity matrix for clients, depicting the levels of engagement over various periods.\n\nDiagonal Trend: There is a distinct diagonal pattern, indicating that newer accounts (those created more recently) have fewer periods of activity. This makes sense as these accounts have not had the opportunity to transact over the earlier periods displayed on the heatmap.\nDarker Areas (Purple): These represent periods of inactivity where clients did not engage. The darker the shade, the less activity occurred in that particular period for the corresponding set of accounts.\nBrighter Areas (Yellow): In contrast, the brighter areas denote periods of activity. A brighter shade implies more clients were active during that period.\nAccount Creation Date: Clients are sorted by their account creation date. Those who joined earlier are at the top, while more recent clients appear toward the bottom of the heatmap.\n\n\ndef prepare_activity_matrix(transactions):\n    \"\"\"\n    Create an activity matrix from transaction data.\n\n    The function transforms transaction data into a binary matrix that indicates\n    whether an account was active in a given month.\n\n    Parameters:\n    - transactions (pd.DataFrame): A DataFrame containing the transaction data.\n\n    Returns:\n    - pd.DataFrame: An activity matrix with accounts as rows and months as columns.\n    \"\"\"\n    transactions[\"month_year\"] = transactions[\"date\"].dt.to_period(\"M\")\n    transactions[\"active\"] = 1\n\n    activity_matrix = transactions.pivot_table(\n        index=\"account_id\", columns=\"month_year\", values=\"active\", fill_value=0\n    )\n\n    activity_matrix.columns = [f\"active_{str(col)}\" for col in activity_matrix.columns]\n    return activity_matrix\n\n\ndef plot_activity_matrix(activity_matrix):\n    sparse_matrix = activity_matrix.astype(bool)\n    plt.figure(figsize=(20, 10))\n    sns.heatmap(sparse_matrix, cmap=\"viridis\", cbar=True, yticklabels=False)\n    plt.title(f\"Activity Matrix across all clients sorted by account creation date\")\n    plt.xlabel(\"Period\")\n    plt.ylabel(\"Accounts\")\n    plt.show()\n\n\nactivity_matrix = prepare_activity_matrix(transactions_df)\nplot_activity_matrix(activity_matrix)\n\n\n\n\n\n\n\n\n\n\n3.2.2 Eligibility Criteria\nAfter constructing the activity matrix, we check for eligibility of non-cardholders to be matched with cardholders. This ensures alignment for later model construction. The eligibility criteria are as follows:\n\nAccount History: Non-cardholders must have an established history of interaction, with at least 25 months of history between account creation and card issuance (12 months (= New customer period) + 13 months (= one year of history) + 1 month (Lag period)).\nAccount Creation Date: The account creation date of a non-cardholder must precede the card issuance date of the cardholder as this is a prerequisite for the matching process to work correctly when we set the issue date for non-card holders.\n\n\nfrom sklearn.metrics import pairwise_distances\nfrom tqdm import tqdm\n\nELIGIBILITY_THRESHOLD_HIST_MONTHS = 25\n\n\ndef check_eligibility_for_matching(non_cardholder, cardholder, verbose=False):\n    \"\"\"\n    Determine if a non-cardholder is eligible for matching with a cardholder.\n\n    This function checks whether the card issuance to a cardholder occurred at least\n    25 months after the non-cardholder's account was created.\n\n    Parameters:\n    - non_cardholder (pd.Series): A data series containing the non-cardholder's details.\n    - cardholder (pd.Series): A data series containing the cardholder's details.\n    - verbose (bool): If True, print detailed eligibility information. Default is False.\n\n    Returns:\n    - bool: True if the non-cardholder is eligible for matching, False otherwise.\n    \"\"\"\n    if cardholder[\"card_issued\"] &lt;= non_cardholder[\"account_created\"]:\n        return False\n\n    period_diff = (\n        cardholder[\"card_issued\"].to_period(\"M\")\n        - non_cardholder[\"account_created\"].to_period(\"M\")\n    ).n\n\n    if verbose:\n        print(\n            f\"Card issued: {cardholder['card_issued']}, Account created: {non_cardholder['account_created']}, Period diff: {period_diff}, Eligible: {period_diff &gt;= ELIGIBILITY_THRESHOLD_HIST_MONTHS}\"\n        )\n\n    return period_diff &gt;= ELIGIBILITY_THRESHOLD_HIST_MONTHS\n\n\n\n3.2.3 Matching Process\nNext up we will implement the matching process. Our matching utilizes the Jaccard similarity index to compare activity patterns: We compare a vector representing an existing cardholder’s monthly activity against a matrix of non-cardholders’ activity patterns. Here we only consider the activity from the first transaction period across all customers to the card issue date.\nThe Jaccard similarity index is calculated as the intersection of active months divided by the union of active months between the two clients. This index ranges from 0 to 1, with higher values indicating greater similarity in activity patterns.\n\\[J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\nThe function match_cardholders_with_non_cardholders will perform the following steps:\n\nData Preparation: The function prepares the activity matrix and splits the non-cardholders into two groups: those with and without cards.\nMatching Process: For each cardholder, the function calculates the Jaccard similarity between their activity pattern and those of eligible non-cardholders. It then selects the top N similar non-cardholders and randomly assigns one match per cardholder.\nMatch Selection: The function selects a non-cardholder match for each cardholder based on the Jaccard similarity scores. It ensures that each non-cardholder is matched only once and that the top N similar non-cardholders are considered for matching.\n\nThe selection among the top N similar non-cardholders is done randomly to avoid bias. This process is defined in the select_non_cardholders function.\nThe function also checks for the eligibility as defined above.\nIf no eligible non-cardholders are found, the function prints a warning message.\n\nOutput: The function returns a list of tuples containing the matched cardholder and non-cardholder client IDs along with their similarity scores.\n\n\ndef select_non_cardholders(\n    distances,\n    eligible_non_cardholders,\n    matches,\n    matched_applicants,\n    cardholder,\n    without_card_activity,\n    top_n,\n):\n    \"\"\"\n    Randomly select a non-cardholder match for a cardholder from the top N eligible candidates.\n\n    Parameters:\n    - distances (np.array): An array of Jaccard distances between a cardholder and non-cardholders.\n    - eligible_non_cardholders (list): A list of indices for non-cardholders who are eligible for matching.\n    - matches (list): A list to which the match will be appended.\n    - matched_applicants (set): A set of indices for non-cardholders who have already been matched.\n    - cardholder (pd.Series): The data series of the current cardholder.\n    - without_card_activity (pd.DataFrame): A DataFrame of non-cardholders without card issuance.\n    - top_n (int): The number of top similar non-cardholders to consider for matching.\n\n    Returns:\n    - None: The matches list is updated in place with the selected match.\n    \"\"\"\n    eligible_distances = distances[eligible_non_cardholders]\n    sorted_indices = np.argsort(eligible_distances)[:top_n]\n\n    if sorted_indices.size &gt; 0:\n        selected_index = np.random.choice(sorted_indices)\n        actual_selected_index = eligible_non_cardholders[selected_index]\n\n        if actual_selected_index not in matched_applicants:\n            matched_applicants.add(actual_selected_index)\n            applicant = without_card_activity.iloc[actual_selected_index]\n            similarity = 1 - eligible_distances[selected_index]\n\n            matches.append(\n                (cardholder[\"client_id\"], applicant[\"client_id\"], similarity)\n            )\n\n\ndef match_cardholders_with_non_cardholders(non_transactional, transactions, top_n=5):\n    \"\"\"\n    Match cardholders with non-cardholders based on the similarity of their activity patterns.\n\n    The function creates an activity matrix, identifies eligible non-cardholders, calculates\n    the Jaccard similarity to find matches, and randomly selects one match per cardholder\n    from the top N similar non-cardholders.\n\n    Parameters:\n    - non_transactional (pd.DataFrame): A DataFrame containing non-cardholders.\n    - transactions (pd.DataFrame): A DataFrame containing transactional data.\n    - top_n (int): The number of top similar non-cardholders to consider for matching.\n\n    Returns:\n    - list: A list of tuples with the cardholder and matched non-cardholder client IDs and similarity scores.\n    \"\"\"\n    with_card = non_transactional[non_transactional[\"card_issued\"].notna()]\n    without_card = non_transactional[non_transactional[\"card_issued\"].isna()]\n\n    activity_matrix = prepare_activity_matrix(transactions)\n\n    with_card_activity = with_card.join(activity_matrix, on=\"account_id\", how=\"left\")\n    without_card_activity = without_card.join(\n        activity_matrix, on=\"account_id\", how=\"left\"\n    )\n\n    matched_non_cardholders = set()\n    matches = []\n\n    for idx, cardholder in tqdm(\n        with_card_activity.iterrows(),\n        total=len(with_card_activity),\n        desc=\"Matching cardholders\",\n    ):\n        issue_period = cardholder[\"card_issued\"].to_period(\"M\")\n        eligible_cols = [\n            col\n            for col in activity_matrix\n            if col.startswith(\"active\") and pd.Period(col.split(\"_\")[1]) &lt;= issue_period\n        ]\n\n        if not eligible_cols:\n            print(\n                f\"No eligible months found for cardholder client_id {cardholder['client_id']}.\"\n            )\n            continue\n        \n        cardholder_vector = cardholder[eligible_cols].values.reshape(1, -1)\n        non_cardholder_matrix = without_card_activity[eligible_cols].values\n        \n        cardholder_vector = np.where(cardholder_vector &gt; 0, 1, 0).astype(bool)\n        non_cardholder_matrix = np.where(non_cardholder_matrix &gt; 0, 1, 0).astype(bool)\n\n        assert (\n            cardholder_vector.shape[1] == non_cardholder_matrix.shape[1]\n        ), \"Dimension mismatch between cardholder and applicant activity matrix.\"\n\n        distances = pairwise_distances(\n            cardholder_vector, non_cardholder_matrix, \n            metric=\"jaccard\", n_jobs=-1 \n        ).flatten()\n        eligible_non_cardholders = [\n            i\n            for i, applicant in without_card_activity.iterrows()\n            if check_eligibility_for_matching(applicant, cardholder)\n            and i not in matched_non_cardholders\n        ]\n\n        if eligible_non_cardholders:\n            select_non_cardholders(\n                distances,\n                eligible_non_cardholders,\n                matches,\n                matched_non_cardholders,\n                cardholder,\n                without_card_activity,\n                top_n,\n            )\n        else:\n            print(\n                f\"No eligible non-cardholders found for cardholder client_id {cardholder['client_id']}.\"\n            )\n\n    return matches\n\nTODO: Visualise the matching process\nThe matching process is executed, and the results are stored in the matched_non_card_holders_df DataFrame. The percentage of clients with a card issued before and after matching is calculated to assess the impact of the matching process. We expect the percentage of clients with a card issued to increase by 100% after matching, as each non-cardholder should be matched with a cardholder.\nLast but not least we set the artificial card issue date for each non-cardholder based on the matching results.\n\ndef set_artificial_issue_dates(non_transactional_df, matches):\n    \"\"\"\n    Augment the non-transactional DataFrame with artificial card issue dates based on matching results.\n\n    Each matched non-cardholder is assigned a card issue date corresponding to their matched\n    cardholder. The 'has_card' flag for each non-cardholder is updated accordingly.\n\n    Parameters:\n    - non_transactional_df (pd.DataFrame): The DataFrame of non-cardholders to augment.\n    - matches (list): A list of tuples containing the matched cardholder and non-cardholder IDs and similarity scores.\n\n    Returns:\n    - pd.DataFrame: The augmented DataFrame with artificial card issue dates.\n    \"\"\"\n    augmented_df = non_transactional_df.copy()\n    augmented_df[\"has_card\"] = True\n\n    for cardholder_id, non_cardholder_id, _ in matches:\n        card_issue_date = augmented_df.loc[\n            augmented_df[\"client_id\"] == cardholder_id, \"card_issued\"\n        ].values[0]\n        augmented_df.loc[\n            augmented_df[\"client_id\"] == non_cardholder_id, [\"card_issued\", \"has_card\"]\n        ] = [card_issue_date, False]\n\n    return augmented_df\n\n\nmatched_non_card_holders_df = match_cardholders_with_non_cardholders(\n    non_transactional_w_sufficient_history_df, transactions_df\n)\n\nprint(\n    f\"Percentage of clients with card issued: {non_transactional_w_sufficient_history_df['card_issued'].notna().mean() * 100:.2f}%\"\n)\nmatched_non_card_holders_w_issue_date_df = set_artificial_issue_dates(\n    non_transactional_w_sufficient_history_df, matched_non_card_holders_df\n)\nprint(\n    f\"Percentage of clients with card issued after matching: {matched_non_card_holders_w_issue_date_df['card_issued'].notna().mean() * 100:.2f}%\"\n)\n\nMatching cardholders:   0%|          | 0/328 [00:00&lt;?, ?it/s]Matching cardholders:   0%|          | 1/328 [00:00&lt;01:55,  2.83it/s]Matching cardholders:   1%|          | 2/328 [00:00&lt;01:52,  2.90it/s]Matching cardholders:   1%|          | 3/328 [00:01&lt;01:51,  2.92it/s]Matching cardholders:   1%|          | 4/328 [00:01&lt;01:43,  3.13it/s]Matching cardholders:   2%|▏         | 5/328 [00:01&lt;02:01,  2.65it/s]Matching cardholders:   2%|▏         | 6/328 [00:02&lt;01:56,  2.76it/s]Matching cardholders:   2%|▏         | 7/328 [00:02&lt;01:53,  2.84it/s]Matching cardholders:   2%|▏         | 8/328 [00:02&lt;01:50,  2.89it/s]Matching cardholders:   3%|▎         | 9/328 [00:03&lt;01:37,  3.26it/s]Matching cardholders:   3%|▎         | 10/328 [00:03&lt;01:29,  3.57it/s]Matching cardholders:   3%|▎         | 11/328 [00:03&lt;01:34,  3.35it/s]Matching cardholders:   4%|▎         | 12/328 [00:03&lt;01:38,  3.22it/s]Matching cardholders:   4%|▍         | 13/328 [00:04&lt;01:39,  3.15it/s]Matching cardholders:   4%|▍         | 14/328 [00:04&lt;01:36,  3.24it/s]Matching cardholders:   5%|▍         | 15/328 [00:04&lt;01:39,  3.15it/s]Matching cardholders:   5%|▍         | 16/328 [00:05&lt;01:29,  3.49it/s]Matching cardholders:   5%|▌         | 17/328 [00:05&lt;01:22,  3.78it/s]Matching cardholders:   5%|▌         | 18/328 [00:05&lt;01:18,  3.94it/s]Matching cardholders:   6%|▌         | 19/328 [00:05&lt;01:24,  3.65it/s]Matching cardholders:   6%|▌         | 20/328 [00:06&lt;01:28,  3.47it/s]Matching cardholders:   6%|▋         | 21/328 [00:06&lt;01:33,  3.30it/s]Matching cardholders:   7%|▋         | 22/328 [00:06&lt;01:35,  3.20it/s]Matching cardholders:   7%|▋         | 23/328 [00:07&lt;01:38,  3.11it/s]Matching cardholders:   7%|▋         | 24/328 [00:07&lt;01:39,  3.06it/s]Matching cardholders:   8%|▊         | 25/328 [00:07&lt;01:39,  3.04it/s]Matching cardholders:   8%|▊         | 26/328 [00:08&lt;01:29,  3.36it/s]Matching cardholders:   8%|▊         | 27/328 [00:08&lt;01:24,  3.56it/s]Matching cardholders:   9%|▊         | 28/328 [00:08&lt;01:40,  2.98it/s]Matching cardholders:   9%|▉         | 29/328 [00:09&lt;01:37,  3.08it/s]Matching cardholders:   9%|▉         | 30/328 [00:09&lt;01:37,  3.05it/s]Matching cardholders:   9%|▉         | 31/328 [00:09&lt;01:34,  3.14it/s]Matching cardholders:  10%|▉         | 32/328 [00:09&lt;01:25,  3.44it/s]Matching cardholders:  10%|█         | 33/328 [00:10&lt;01:27,  3.35it/s]Matching cardholders:  10%|█         | 34/328 [00:10&lt;01:27,  3.34it/s]Matching cardholders:  11%|█         | 35/328 [00:10&lt;01:30,  3.25it/s]Matching cardholders:  11%|█         | 36/328 [00:11&lt;01:23,  3.51it/s]Matching cardholders:  11%|█▏        | 37/328 [00:11&lt;01:27,  3.34it/s]Matching cardholders:  12%|█▏        | 38/328 [00:11&lt;01:32,  3.13it/s]Matching cardholders:  12%|█▏        | 39/328 [00:12&lt;01:28,  3.28it/s]Matching cardholders:  12%|█▏        | 40/328 [00:12&lt;01:28,  3.24it/s]Matching cardholders:  12%|█▎        | 41/328 [00:12&lt;01:30,  3.18it/s]Matching cardholders:  13%|█▎        | 42/328 [00:13&lt;01:30,  3.15it/s]Matching cardholders:  13%|█▎        | 43/328 [00:13&lt;01:38,  2.90it/s]Matching cardholders:  13%|█▎        | 44/328 [00:13&lt;01:30,  3.14it/s]Matching cardholders:  14%|█▎        | 45/328 [00:13&lt;01:24,  3.33it/s]Matching cardholders:  14%|█▍        | 46/328 [00:14&lt;01:20,  3.50it/s]Matching cardholders:  14%|█▍        | 47/328 [00:14&lt;01:24,  3.32it/s]Matching cardholders:  15%|█▍        | 48/328 [00:14&lt;01:27,  3.21it/s]Matching cardholders:  15%|█▍        | 49/328 [00:15&lt;01:29,  3.12it/s]Matching cardholders:  15%|█▌        | 50/328 [00:15&lt;01:28,  3.12it/s]Matching cardholders:  16%|█▌        | 51/328 [00:15&lt;01:35,  2.89it/s]Matching cardholders:  16%|█▌        | 52/328 [00:16&lt;01:26,  3.18it/s]Matching cardholders:  16%|█▌        | 53/328 [00:16&lt;01:27,  3.14it/s]Matching cardholders:  16%|█▋        | 54/328 [00:16&lt;01:20,  3.39it/s]Matching cardholders:  17%|█▋        | 55/328 [00:17&lt;01:22,  3.32it/s]Matching cardholders:  17%|█▋        | 56/328 [00:17&lt;01:24,  3.22it/s]Matching cardholders:  17%|█▋        | 57/328 [00:17&lt;01:26,  3.13it/s]Matching cardholders:  18%|█▊        | 58/328 [00:18&lt;01:23,  3.24it/s]Matching cardholders:  18%|█▊        | 59/328 [00:18&lt;01:16,  3.50it/s]Matching cardholders:  18%|█▊        | 60/328 [00:18&lt;01:16,  3.50it/s]Matching cardholders:  19%|█▊        | 61/328 [00:18&lt;01:13,  3.65it/s]Matching cardholders:  19%|█▉        | 62/328 [00:19&lt;01:17,  3.42it/s]Matching cardholders:  19%|█▉        | 63/328 [00:19&lt;01:20,  3.30it/s]Matching cardholders:  20%|█▉        | 64/328 [00:19&lt;01:21,  3.23it/s]Matching cardholders:  20%|█▉        | 65/328 [00:20&lt;01:22,  3.17it/s]Matching cardholders:  20%|██        | 66/328 [00:20&lt;01:21,  3.23it/s]Matching cardholders:  20%|██        | 67/328 [00:20&lt;01:22,  3.15it/s]Matching cardholders:  21%|██        | 68/328 [00:21&lt;01:24,  3.09it/s]Matching cardholders:  21%|██        | 69/328 [00:21&lt;01:20,  3.20it/s]Matching cardholders:  21%|██▏       | 70/328 [00:21&lt;01:19,  3.23it/s]Matching cardholders:  22%|██▏       | 71/328 [00:22&lt;01:21,  3.15it/s]Matching cardholders:  22%|██▏       | 72/328 [00:22&lt;01:16,  3.33it/s]Matching cardholders:  22%|██▏       | 73/328 [00:22&lt;01:19,  3.20it/s]Matching cardholders:  23%|██▎       | 74/328 [00:23&lt;01:27,  2.89it/s]Matching cardholders:  23%|██▎       | 75/328 [00:23&lt;01:25,  2.95it/s]Matching cardholders:  23%|██▎       | 76/328 [00:23&lt;01:25,  2.95it/s]Matching cardholders:  23%|██▎       | 77/328 [00:24&lt;01:23,  3.01it/s]Matching cardholders:  24%|██▍       | 78/328 [00:24&lt;01:21,  3.07it/s]Matching cardholders:  24%|██▍       | 79/328 [00:24&lt;01:21,  3.04it/s]Matching cardholders:  24%|██▍       | 80/328 [00:25&lt;01:20,  3.07it/s]Matching cardholders:  25%|██▍       | 81/328 [00:25&lt;01:16,  3.24it/s]Matching cardholders:  25%|██▌       | 82/328 [00:25&lt;01:15,  3.25it/s]Matching cardholders:  25%|██▌       | 83/328 [00:25&lt;01:17,  3.15it/s]Matching cardholders:  26%|██▌       | 84/328 [00:26&lt;01:19,  3.07it/s]Matching cardholders:  26%|██▌       | 85/328 [00:26&lt;01:16,  3.17it/s]Matching cardholders:  26%|██▌       | 86/328 [00:26&lt;01:12,  3.34it/s]Matching cardholders:  27%|██▋       | 87/328 [00:27&lt;01:10,  3.42it/s]Matching cardholders:  27%|██▋       | 88/328 [00:27&lt;01:05,  3.64it/s]Matching cardholders:  27%|██▋       | 89/328 [00:27&lt;01:03,  3.77it/s]Matching cardholders:  27%|██▋       | 90/328 [00:27&lt;01:07,  3.53it/s]Matching cardholders:  28%|██▊       | 91/328 [00:28&lt;01:10,  3.36it/s]Matching cardholders:  28%|██▊       | 92/328 [00:28&lt;01:11,  3.30it/s]Matching cardholders:  28%|██▊       | 93/328 [00:28&lt;01:13,  3.20it/s]Matching cardholders:  29%|██▊       | 94/328 [00:29&lt;01:14,  3.15it/s]Matching cardholders:  29%|██▉       | 95/328 [00:29&lt;01:15,  3.09it/s]Matching cardholders:  29%|██▉       | 96/328 [00:29&lt;01:16,  3.05it/s]Matching cardholders:  30%|██▉       | 97/328 [00:30&lt;01:24,  2.72it/s]Matching cardholders:  30%|██▉       | 98/328 [00:30&lt;01:22,  2.79it/s]Matching cardholders:  30%|███       | 99/328 [00:31&lt;01:20,  2.86it/s]Matching cardholders:  30%|███       | 100/328 [00:31&lt;01:11,  3.19it/s]Matching cardholders:  31%|███       | 101/328 [00:31&lt;01:08,  3.30it/s]Matching cardholders:  31%|███       | 102/328 [00:31&lt;01:11,  3.17it/s]Matching cardholders:  31%|███▏      | 103/328 [00:32&lt;01:12,  3.12it/s]Matching cardholders:  32%|███▏      | 104/328 [00:32&lt;01:12,  3.08it/s]Matching cardholders:  32%|███▏      | 105/328 [00:32&lt;01:09,  3.20it/s]Matching cardholders:  32%|███▏      | 106/328 [00:33&lt;01:10,  3.14it/s]Matching cardholders:  33%|███▎      | 107/328 [00:33&lt;01:09,  3.17it/s]Matching cardholders:  33%|███▎      | 108/328 [00:33&lt;01:04,  3.42it/s]Matching cardholders:  33%|███▎      | 109/328 [00:33&lt;01:00,  3.62it/s]Matching cardholders:  34%|███▎      | 110/328 [00:34&lt;01:04,  3.39it/s]Matching cardholders:  34%|███▍      | 111/328 [00:34&lt;01:06,  3.26it/s]Matching cardholders:  34%|███▍      | 112/328 [00:34&lt;01:06,  3.24it/s]Matching cardholders:  34%|███▍      | 113/328 [00:35&lt;01:03,  3.36it/s]Matching cardholders:  35%|███▍      | 114/328 [00:35&lt;01:05,  3.25it/s]Matching cardholders:  35%|███▌      | 115/328 [00:35&lt;01:07,  3.16it/s]Matching cardholders:  35%|███▌      | 116/328 [00:36&lt;01:05,  3.23it/s]Matching cardholders:  36%|███▌      | 117/328 [00:36&lt;01:06,  3.19it/s]Matching cardholders:  36%|███▌      | 118/328 [00:36&lt;01:00,  3.46it/s]Matching cardholders:  36%|███▋      | 119/328 [00:37&lt;01:03,  3.27it/s]Matching cardholders:  37%|███▋      | 120/328 [00:37&lt;01:08,  3.03it/s]Matching cardholders:  37%|███▋      | 121/328 [00:37&lt;01:08,  3.02it/s]Matching cardholders:  37%|███▋      | 122/328 [00:38&lt;01:03,  3.24it/s]Matching cardholders:  38%|███▊      | 123/328 [00:38&lt;01:02,  3.27it/s]Matching cardholders:  38%|███▊      | 124/328 [00:38&lt;01:01,  3.31it/s]Matching cardholders:  38%|███▊      | 125/328 [00:38&lt;01:02,  3.25it/s]Matching cardholders:  38%|███▊      | 126/328 [00:39&lt;01:03,  3.17it/s]Matching cardholders:  39%|███▊      | 127/328 [00:39&lt;00:59,  3.38it/s]Matching cardholders:  39%|███▉      | 128/328 [00:39&lt;00:56,  3.52it/s]Matching cardholders:  39%|███▉      | 129/328 [00:40&lt;00:59,  3.33it/s]Matching cardholders:  40%|███▉      | 130/328 [00:40&lt;01:00,  3.29it/s]Matching cardholders:  40%|███▉      | 131/328 [00:40&lt;01:01,  3.19it/s]Matching cardholders:  40%|████      | 132/328 [00:41&lt;00:57,  3.40it/s]Matching cardholders:  41%|████      | 133/328 [00:41&lt;00:59,  3.26it/s]Matching cardholders:  41%|████      | 134/328 [00:41&lt;01:01,  3.17it/s]Matching cardholders:  41%|████      | 135/328 [00:41&lt;00:59,  3.23it/s]Matching cardholders:  41%|████▏     | 136/328 [00:42&lt;01:00,  3.15it/s]Matching cardholders:  42%|████▏     | 137/328 [00:42&lt;01:01,  3.11it/s]Matching cardholders:  42%|████▏     | 138/328 [00:42&lt;01:02,  3.05it/s]Matching cardholders:  42%|████▏     | 139/328 [00:43&lt;01:02,  3.02it/s]Matching cardholders:  43%|████▎     | 140/328 [00:43&lt;01:01,  3.06it/s]Matching cardholders:  43%|████▎     | 141/328 [00:43&lt;00:57,  3.27it/s]Matching cardholders:  43%|████▎     | 142/328 [00:44&lt;00:56,  3.31it/s]Matching cardholders:  44%|████▎     | 143/328 [00:44&lt;01:04,  2.87it/s]Matching cardholders:  44%|████▍     | 144/328 [00:44&lt;01:03,  2.88it/s]Matching cardholders:  44%|████▍     | 145/328 [00:45&lt;00:59,  3.06it/s]Matching cardholders:  45%|████▍     | 146/328 [00:45&lt;00:58,  3.09it/s]Matching cardholders:  45%|████▍     | 147/328 [00:45&lt;00:59,  3.03it/s]Matching cardholders:  45%|████▌     | 148/328 [00:46&lt;00:56,  3.17it/s]Matching cardholders:  45%|████▌     | 149/328 [00:46&lt;00:54,  3.26it/s]Matching cardholders:  46%|████▌     | 150/328 [00:46&lt;00:56,  3.16it/s]Matching cardholders:  46%|████▌     | 151/328 [00:47&lt;00:53,  3.33it/s]Matching cardholders:  46%|████▋     | 152/328 [00:47&lt;00:54,  3.23it/s]Matching cardholders:  47%|████▋     | 153/328 [00:47&lt;00:51,  3.39it/s]Matching cardholders:  47%|████▋     | 154/328 [00:48&lt;00:52,  3.33it/s]Matching cardholders:  47%|████▋     | 155/328 [00:48&lt;00:53,  3.22it/s]Matching cardholders:  48%|████▊     | 156/328 [00:48&lt;00:54,  3.15it/s]Matching cardholders:  48%|████▊     | 157/328 [00:49&lt;00:55,  3.11it/s]Matching cardholders:  48%|████▊     | 158/328 [00:49&lt;00:54,  3.15it/s]Matching cardholders:  48%|████▊     | 159/328 [00:49&lt;00:52,  3.25it/s]Matching cardholders:  49%|████▉     | 160/328 [00:49&lt;00:50,  3.30it/s]Matching cardholders:  49%|████▉     | 161/328 [00:50&lt;00:49,  3.41it/s]Matching cardholders:  49%|████▉     | 162/328 [00:50&lt;00:49,  3.38it/s]Matching cardholders:  50%|████▉     | 163/328 [00:50&lt;00:47,  3.49it/s]Matching cardholders:  50%|█████     | 164/328 [00:51&lt;00:49,  3.30it/s]Matching cardholders:  50%|█████     | 165/328 [00:51&lt;00:50,  3.21it/s]Matching cardholders:  51%|█████     | 166/328 [00:51&lt;00:57,  2.79it/s]Matching cardholders:  51%|█████     | 167/328 [00:52&lt;00:55,  2.89it/s]Matching cardholders:  51%|█████     | 168/328 [00:52&lt;00:54,  2.94it/s]Matching cardholders:  52%|█████▏    | 169/328 [00:52&lt;00:54,  2.94it/s]Matching cardholders:  52%|█████▏    | 170/328 [00:53&lt;00:53,  2.96it/s]Matching cardholders:  52%|█████▏    | 171/328 [00:53&lt;00:52,  3.00it/s]Matching cardholders:  52%|█████▏    | 172/328 [00:53&lt;00:51,  3.02it/s]Matching cardholders:  53%|█████▎    | 173/328 [00:54&lt;00:48,  3.17it/s]Matching cardholders:  53%|█████▎    | 174/328 [00:54&lt;00:46,  3.28it/s]Matching cardholders:  53%|█████▎    | 175/328 [00:54&lt;00:48,  3.17it/s]Matching cardholders:  54%|█████▎    | 176/328 [00:55&lt;00:49,  3.08it/s]Matching cardholders:  54%|█████▍    | 177/328 [00:55&lt;00:49,  3.03it/s]Matching cardholders:  54%|█████▍    | 178/328 [00:55&lt;00:49,  3.01it/s]Matching cardholders:  55%|█████▍    | 179/328 [00:56&lt;00:49,  3.00it/s]Matching cardholders:  55%|█████▍    | 180/328 [00:56&lt;00:49,  2.99it/s]Matching cardholders:  55%|█████▌    | 181/328 [00:56&lt;00:46,  3.13it/s]Matching cardholders:  55%|█████▌    | 182/328 [00:57&lt;00:46,  3.17it/s]Matching cardholders:  56%|█████▌    | 183/328 [00:57&lt;00:46,  3.10it/s]Matching cardholders:  56%|█████▌    | 184/328 [00:57&lt;00:47,  3.06it/s]Matching cardholders:  56%|█████▋    | 185/328 [00:58&lt;00:46,  3.07it/s]Matching cardholders:  57%|█████▋    | 186/328 [00:58&lt;00:45,  3.12it/s]Matching cardholders:  57%|█████▋    | 187/328 [00:58&lt;00:45,  3.08it/s]Matching cardholders:  57%|█████▋    | 188/328 [00:59&lt;00:45,  3.05it/s]Matching cardholders:  58%|█████▊    | 189/328 [00:59&lt;00:48,  2.89it/s]Matching cardholders:  58%|█████▊    | 190/328 [00:59&lt;00:47,  2.93it/s]Matching cardholders:  58%|█████▊    | 191/328 [01:00&lt;00:46,  2.94it/s]Matching cardholders:  59%|█████▊    | 192/328 [01:00&lt;00:44,  3.04it/s]Matching cardholders:  59%|█████▉    | 193/328 [01:00&lt;00:43,  3.11it/s]Matching cardholders:  59%|█████▉    | 194/328 [01:00&lt;00:42,  3.16it/s]Matching cardholders:  59%|█████▉    | 195/328 [01:01&lt;00:43,  3.09it/s]Matching cardholders:  60%|█████▉    | 196/328 [01:01&lt;00:42,  3.13it/s]Matching cardholders:  60%|██████    | 197/328 [01:01&lt;00:41,  3.17it/s]Matching cardholders:  60%|██████    | 198/328 [01:02&lt;00:42,  3.09it/s]Matching cardholders:  61%|██████    | 199/328 [01:02&lt;00:40,  3.15it/s]Matching cardholders:  61%|██████    | 200/328 [01:02&lt;00:41,  3.11it/s]Matching cardholders:  61%|██████▏   | 201/328 [01:03&lt;00:41,  3.05it/s]Matching cardholders:  62%|██████▏   | 202/328 [01:03&lt;00:40,  3.11it/s]Matching cardholders:  62%|██████▏   | 203/328 [01:03&lt;00:40,  3.06it/s]Matching cardholders:  62%|██████▏   | 204/328 [01:04&lt;00:41,  3.02it/s]Matching cardholders:  62%|██████▎   | 205/328 [01:04&lt;00:41,  3.00it/s]Matching cardholders:  63%|██████▎   | 206/328 [01:04&lt;00:40,  2.99it/s]Matching cardholders:  63%|██████▎   | 207/328 [01:05&lt;00:40,  2.98it/s]Matching cardholders:  63%|██████▎   | 208/328 [01:05&lt;00:39,  3.07it/s]Matching cardholders:  64%|██████▎   | 209/328 [01:05&lt;00:39,  3.04it/s]Matching cardholders:  64%|██████▍   | 210/328 [01:06&lt;00:39,  3.00it/s]Matching cardholders:  64%|██████▍   | 211/328 [01:06&lt;00:38,  3.00it/s]Matching cardholders:  65%|██████▍   | 212/328 [01:07&lt;00:42,  2.75it/s]Matching cardholders:  65%|██████▍   | 213/328 [01:07&lt;00:41,  2.79it/s]Matching cardholders:  65%|██████▌   | 214/328 [01:07&lt;00:40,  2.83it/s]Matching cardholders:  66%|██████▌   | 215/328 [01:08&lt;00:39,  2.87it/s]Matching cardholders:  66%|██████▌   | 216/328 [01:08&lt;00:38,  2.89it/s]Matching cardholders:  66%|██████▌   | 217/328 [01:08&lt;00:38,  2.91it/s]Matching cardholders:  66%|██████▋   | 218/328 [01:09&lt;00:37,  2.96it/s]Matching cardholders:  67%|██████▋   | 219/328 [01:09&lt;00:36,  3.02it/s]Matching cardholders:  67%|██████▋   | 220/328 [01:09&lt;00:35,  3.01it/s]Matching cardholders:  67%|██████▋   | 221/328 [01:10&lt;00:35,  3.00it/s]Matching cardholders:  68%|██████▊   | 222/328 [01:10&lt;00:35,  3.01it/s]Matching cardholders:  68%|██████▊   | 223/328 [01:10&lt;00:34,  3.07it/s]Matching cardholders:  68%|██████▊   | 224/328 [01:10&lt;00:34,  3.04it/s]Matching cardholders:  69%|██████▊   | 225/328 [01:11&lt;00:34,  3.01it/s]Matching cardholders:  69%|██████▉   | 226/328 [01:11&lt;00:35,  2.90it/s]Matching cardholders:  69%|██████▉   | 227/328 [01:12&lt;00:34,  2.92it/s]Matching cardholders:  70%|██████▉   | 228/328 [01:12&lt;00:34,  2.92it/s]Matching cardholders:  70%|██████▉   | 229/328 [01:12&lt;00:33,  2.98it/s]Matching cardholders:  70%|███████   | 230/328 [01:13&lt;00:32,  2.98it/s]Matching cardholders:  70%|███████   | 231/328 [01:13&lt;00:32,  2.96it/s]Matching cardholders:  71%|███████   | 232/328 [01:13&lt;00:32,  2.96it/s]Matching cardholders:  71%|███████   | 233/328 [01:14&lt;00:31,  3.01it/s]Matching cardholders:  71%|███████▏  | 234/328 [01:14&lt;00:31,  2.97it/s]Matching cardholders:  72%|███████▏  | 235/328 [01:14&lt;00:35,  2.65it/s]Matching cardholders:  72%|███████▏  | 236/328 [01:15&lt;00:33,  2.72it/s]Matching cardholders:  72%|███████▏  | 237/328 [01:15&lt;00:32,  2.78it/s]Matching cardholders:  73%|███████▎  | 238/328 [01:15&lt;00:31,  2.82it/s]Matching cardholders:  73%|███████▎  | 239/328 [01:16&lt;00:31,  2.85it/s]Matching cardholders:  73%|███████▎  | 240/328 [01:16&lt;00:30,  2.88it/s]Matching cardholders:  73%|███████▎  | 241/328 [01:16&lt;00:29,  2.93it/s]Matching cardholders:  74%|███████▍  | 242/328 [01:17&lt;00:29,  2.94it/s]Matching cardholders:  74%|███████▍  | 243/328 [01:17&lt;00:28,  2.95it/s]Matching cardholders:  74%|███████▍  | 244/328 [01:17&lt;00:28,  2.97it/s]Matching cardholders:  75%|███████▍  | 245/328 [01:18&lt;00:28,  2.96it/s]Matching cardholders:  75%|███████▌  | 246/328 [01:18&lt;00:27,  2.96it/s]Matching cardholders:  75%|███████▌  | 247/328 [01:18&lt;00:27,  2.96it/s]Matching cardholders:  76%|███████▌  | 248/328 [01:19&lt;00:27,  2.94it/s]Matching cardholders:  76%|███████▌  | 249/328 [01:19&lt;00:26,  2.96it/s]Matching cardholders:  76%|███████▌  | 250/328 [01:19&lt;00:26,  2.96it/s]Matching cardholders:  77%|███████▋  | 251/328 [01:20&lt;00:26,  2.87it/s]Matching cardholders:  77%|███████▋  | 252/328 [01:20&lt;00:26,  2.87it/s]Matching cardholders:  77%|███████▋  | 253/328 [01:20&lt;00:25,  2.90it/s]Matching cardholders:  77%|███████▋  | 254/328 [01:21&lt;00:25,  2.90it/s]Matching cardholders:  78%|███████▊  | 255/328 [01:21&lt;00:24,  2.93it/s]Matching cardholders:  78%|███████▊  | 256/328 [01:22&lt;00:24,  2.94it/s]Matching cardholders:  78%|███████▊  | 257/328 [01:22&lt;00:24,  2.94it/s]Matching cardholders:  79%|███████▊  | 258/328 [01:22&lt;00:25,  2.71it/s]Matching cardholders:  79%|███████▉  | 259/328 [01:23&lt;00:24,  2.79it/s]Matching cardholders:  79%|███████▉  | 260/328 [01:23&lt;00:23,  2.84it/s]Matching cardholders:  80%|███████▉  | 261/328 [01:23&lt;00:23,  2.88it/s]Matching cardholders:  80%|███████▉  | 262/328 [01:24&lt;00:22,  2.91it/s]Matching cardholders:  80%|████████  | 263/328 [01:24&lt;00:22,  2.92it/s]Matching cardholders:  80%|████████  | 264/328 [01:24&lt;00:21,  2.93it/s]Matching cardholders:  81%|████████  | 265/328 [01:25&lt;00:21,  2.93it/s]Matching cardholders:  81%|████████  | 266/328 [01:25&lt;00:21,  2.94it/s]Matching cardholders:  81%|████████▏ | 267/328 [01:25&lt;00:20,  2.92it/s]Matching cardholders:  82%|████████▏ | 268/328 [01:26&lt;00:20,  2.92it/s]Matching cardholders:  82%|████████▏ | 269/328 [01:26&lt;00:20,  2.92it/s]Matching cardholders:  82%|████████▏ | 270/328 [01:26&lt;00:19,  2.93it/s]Matching cardholders:  83%|████████▎ | 271/328 [01:27&lt;00:19,  2.92it/s]Matching cardholders:  83%|████████▎ | 272/328 [01:27&lt;00:19,  2.91it/s]Matching cardholders:  83%|████████▎ | 273/328 [01:27&lt;00:18,  2.92it/s]Matching cardholders:  84%|████████▎ | 274/328 [01:28&lt;00:22,  2.44it/s]Matching cardholders:  84%|████████▍ | 275/328 [01:28&lt;00:20,  2.56it/s]Matching cardholders:  84%|████████▍ | 276/328 [01:29&lt;00:19,  2.66it/s]Matching cardholders:  84%|████████▍ | 277/328 [01:29&lt;00:18,  2.73it/s]Matching cardholders:  85%|████████▍ | 278/328 [01:29&lt;00:17,  2.79it/s]Matching cardholders:  85%|████████▌ | 279/328 [01:30&lt;00:17,  2.84it/s]Matching cardholders:  85%|████████▌ | 280/328 [01:30&lt;00:16,  2.87it/s]Matching cardholders:  86%|████████▌ | 281/328 [01:30&lt;00:17,  2.65it/s]Matching cardholders:  86%|████████▌ | 282/328 [01:31&lt;00:16,  2.73it/s]Matching cardholders:  86%|████████▋ | 283/328 [01:31&lt;00:16,  2.80it/s]Matching cardholders:  87%|████████▋ | 284/328 [01:31&lt;00:15,  2.86it/s]Matching cardholders:  87%|████████▋ | 285/328 [01:32&lt;00:14,  2.90it/s]Matching cardholders:  87%|████████▋ | 286/328 [01:32&lt;00:14,  2.92it/s]Matching cardholders:  88%|████████▊ | 287/328 [01:32&lt;00:13,  2.94it/s]Matching cardholders:  88%|████████▊ | 288/328 [01:33&lt;00:13,  2.95it/s]Matching cardholders:  88%|████████▊ | 289/328 [01:33&lt;00:13,  2.95it/s]Matching cardholders:  88%|████████▊ | 290/328 [01:33&lt;00:12,  2.95it/s]Matching cardholders:  89%|████████▊ | 291/328 [01:34&lt;00:12,  2.95it/s]Matching cardholders:  89%|████████▉ | 292/328 [01:34&lt;00:12,  2.95it/s]Matching cardholders:  89%|████████▉ | 293/328 [01:34&lt;00:11,  2.95it/s]Matching cardholders:  90%|████████▉ | 294/328 [01:35&lt;00:11,  2.97it/s]Matching cardholders:  90%|████████▉ | 295/328 [01:35&lt;00:11,  2.97it/s]Matching cardholders:  90%|█████████ | 296/328 [01:35&lt;00:10,  2.97it/s]Matching cardholders:  91%|█████████ | 297/328 [01:36&lt;00:10,  2.96it/s]Matching cardholders:  91%|█████████ | 298/328 [01:36&lt;00:10,  2.96it/s]Matching cardholders:  91%|█████████ | 299/328 [01:37&lt;00:09,  2.95it/s]Matching cardholders:  91%|█████████▏| 300/328 [01:37&lt;00:09,  2.95it/s]Matching cardholders:  92%|█████████▏| 301/328 [01:37&lt;00:09,  2.94it/s]Matching cardholders:  92%|█████████▏| 302/328 [01:38&lt;00:08,  2.95it/s]Matching cardholders:  92%|█████████▏| 303/328 [01:38&lt;00:08,  2.95it/s]Matching cardholders:  93%|█████████▎| 304/328 [01:38&lt;00:08,  2.73it/s]Matching cardholders:  93%|█████████▎| 305/328 [01:39&lt;00:08,  2.80it/s]Matching cardholders:  93%|█████████▎| 306/328 [01:39&lt;00:07,  2.85it/s]Matching cardholders:  94%|█████████▎| 307/328 [01:39&lt;00:07,  2.90it/s]Matching cardholders:  94%|█████████▍| 308/328 [01:40&lt;00:06,  2.93it/s]Matching cardholders:  94%|█████████▍| 309/328 [01:40&lt;00:06,  2.95it/s]Matching cardholders:  95%|█████████▍| 310/328 [01:40&lt;00:06,  2.95it/s]Matching cardholders:  95%|█████████▍| 311/328 [01:41&lt;00:05,  2.97it/s]Matching cardholders:  95%|█████████▌| 312/328 [01:41&lt;00:05,  2.98it/s]Matching cardholders:  95%|█████████▌| 313/328 [01:41&lt;00:05,  2.98it/s]Matching cardholders:  96%|█████████▌| 314/328 [01:42&lt;00:04,  2.97it/s]Matching cardholders:  96%|█████████▌| 315/328 [01:42&lt;00:04,  2.98it/s]Matching cardholders:  96%|█████████▋| 316/328 [01:42&lt;00:04,  2.98it/s]Matching cardholders:  97%|█████████▋| 317/328 [01:43&lt;00:03,  2.98it/s]Matching cardholders:  97%|█████████▋| 318/328 [01:43&lt;00:03,  2.97it/s]Matching cardholders:  97%|█████████▋| 319/328 [01:43&lt;00:03,  2.97it/s]Matching cardholders:  98%|█████████▊| 320/328 [01:44&lt;00:02,  2.98it/s]Matching cardholders:  98%|█████████▊| 321/328 [01:44&lt;00:02,  2.99it/s]Matching cardholders:  98%|█████████▊| 322/328 [01:44&lt;00:02,  2.98it/s]Matching cardholders:  98%|█████████▊| 323/328 [01:45&lt;00:01,  2.98it/s]Matching cardholders:  99%|█████████▉| 324/328 [01:45&lt;00:01,  2.98it/s]Matching cardholders:  99%|█████████▉| 325/328 [01:45&lt;00:01,  2.97it/s]Matching cardholders:  99%|█████████▉| 326/328 [01:46&lt;00:00,  2.97it/s]Matching cardholders: 100%|█████████▉| 327/328 [01:46&lt;00:00,  2.69it/s]Matching cardholders: 100%|██████████| 328/328 [01:46&lt;00:00,  2.77it/s]Matching cardholders: 100%|██████████| 328/328 [01:46&lt;00:00,  3.07it/s]\n\n\nPercentage of clients with card issued: 8.33%\nPercentage of clients with card issued after matching: 16.67%\n\n\nAfter each non-cardholder got the artifical card issued date assigned we drop the remaining non-cardholders without a match.\n\nbefore_len = len(matched_non_card_holders_w_issue_date_df)\nprint(-(before_len - len(matched_non_card_holders_w_issue_date_df)))\nmatched_non_card_holders_w_issue_date_df = (\n    matched_non_card_holders_w_issue_date_df.dropna(subset=[\"card_issued\"])\n)\ndata_reduction[\"Non-cardholders without match\"] = -(\n    before_len - len(matched_non_card_holders_w_issue_date_df)\n)\ndel before_len\n\n0\n\n\n\n\n\n3.3 Aggregate on a Monthly Basis\nAfter matching cardholders with non-cardholders and setting artificial card issue dates, we aggregate the transactional data on a monthly basis. This aggregation provides a comprehensive overview of financial activities for each account, facilitating further model development providing us with a fixed of features to work with.\nThe function aggregate_transactions_monthly is designed to process and summarize financial transactions on a monthly basis for each account within a dataset. The explanation of its workings, step by step, is as follows:\n\nSorting Transactions: Initially, the function sorts the transactions in the provided DataFrame transactions_df based on account_id and the transaction date. This ensures that all transactions for a given account are ordered chronologically, which is crucial for accurate monthly aggregation and cumulative balance calculation.\nMonthly Grouping: Each transaction’s date is then converted to a monthly period using dt.to_period(\"M\"). This step categorizes each transaction by the month and year it occurred, facilitating the aggregation of transactions on a monthly basis.\nAggregation of Monthly Data: The function groups the sorted transactions by account_id and the newly created month column. For each group, it calculates several metrics:\n\nvolume: The sum of all transactions’ amounts for the month, representing the total money flow.\ntotal_abs_amount: The sum of the absolute values of the transactions’ amounts, indicating the total amount of money moved, disregarding the direction.\ntransaction_count: The count of transactions, providing a sense of activity level.\npositive_transaction_count and negative_transaction_count: The counts of positive (inflows) and negative (outflows) transactions, respectively. This distinction can help identify the balance between income and expenses.\nStatistical measures like average_amount, median_amount, min_amount, max_amount, and std_amount offer insights into the distribution of transaction amounts.\ntype_count, operation_count, and k_symbol_count: The counts of unique transaction types, operations, and transaction symbols (k_symbol), respectively, indicating the diversity of transaction characteristics.\n\nCumulative Balance Calculation: After aggregating the monthly data, the function computes a cumulative balance (balance) for each account by cumulatively summing the volume (total transaction amount) over time. This step provides insight into how the account balance evolves over the months.\n\nAs we have already explored and verified in the EDA section of the transactional data, each account starts with a transaction where the amount equals the inital balance. This validation ensures the integrity of the aggregated data, as the balance should accurately reflect the total transaction volume over time.\n\ndef aggregate_transactions_monthly(df):\n    \"\"\"\n    Aggregate financial transaction data on a monthly basis per account.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing financial transaction data with 'account_id', 'date', and other relevant columns.\n\n    - validate (bool): If True, validate the aggregated data. Default is True.\n\n    Returns:\n    - pd.DataFrame: Monthly aggregated financial transaction data per account.\n    \"\"\"\n    df_sorted = df.sort_values(by=[\"account_id\", \"date\"])\n    df_sorted[\"month\"] = df_sorted[\"date\"].dt.to_period(\"M\")\n\n    monthly_aggregated_data = (\n        df_sorted.groupby([\"account_id\", \"month\"])\n        .agg(\n            volume=(\"amount\", \"sum\"),\n            total_abs_amount=(\"amount\", lambda x: x.abs().sum()),\n            transaction_count=(\"amount\", \"count\"),\n            positive_transaction_count=(\n                \"amount\",\n                lambda x: (x &gt;= 0).sum(),\n            ),  # TODO: it seems that there are some transactions with 0 amount, how to handle those?\n            negative_transaction_count=(\"amount\", lambda x: (x &lt; 0).sum()),\n            average_amount=(\"amount\", \"mean\"),\n            median_amount=(\"amount\", \"median\"),\n            min_amount=(\"amount\", \"min\"),\n            max_amount=(\"amount\", \"max\"),\n            std_amount=(\"amount\", \"std\"),\n            type_count=(\"transaction_type\", \"nunique\"),\n            operation_count=(\"operation\", \"nunique\"),\n            k_symbol_count=(\"k_symbol\", \"nunique\"),\n        )\n        .reset_index()\n        .sort_values(by=[\"account_id\", \"month\"])\n    )\n\n    monthly_aggregated_data[\"balance\"] = monthly_aggregated_data.groupby(\"account_id\")[\n        \"volume\"\n    ].cumsum()\n    return monthly_aggregated_data\n\n\nagg_transactions_monthly_df = aggregate_transactions_monthly(transactions_df)\nagg_transactions_monthly_df.to_csv(\"./data/agg_transactions_monthly.csv\", index=False)\nagg_transactions_monthly_df.describe()\n\n\n\n\n\n\n\n\naccount_id\nvolume\ntotal_abs_amount\ntransaction_count\npositive_transaction_count\nnegative_transaction_count\naverage_amount\nmedian_amount\nmin_amount\nmax_amount\nstd_amount\ntype_count\noperation_count\nk_symbol_count\nbalance\n\n\n\n\ncount\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n176803.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n\n\nmean\n2799.983292\n1065.354397\n33815.492309\n5.708079\n2.189017\n3.519062\n451.659265\n-372.421445\n-9607.378249\n14756.009580\n9030.305445\n1.921181\n3.568965\n3.719649\n34474.787632\n\n\nstd\n2331.861909\n12509.136299\n37724.985550\n2.417842\n0.726115\n2.173427\n2479.100575\n1933.445907\n10746.883348\n12958.692736\n7402.806514\n0.269457\n0.832363\n1.085701\n19799.443508\n\n\nmin\n1.000000\n-101550.300000\n14.600000\n1.000000\n0.000000\n0.000000\n-37000.000000\n-37000.000000\n-87400.000000\n-37000.000000\n0.000000\n1.000000\n1.000000\n1.000000\n-41125.800000\n\n\n25%\n1172.000000\n-2266.600000\n9659.500000\n4.000000\n2.000000\n2.000000\n-379.566667\n-785.000000\n-13428.000000\n4756.000000\n3283.937059\n2.000000\n3.000000\n3.000000\n20405.600000\n\n\n50%\n2375.000000\n1058.100000\n22933.100000\n5.000000\n2.000000\n3.000000\n220.260000\n-14.600000\n-6177.000000\n10929.000000\n6824.369949\n2.000000\n4.000000\n4.000000\n30000.000000\n\n\n75%\n3576.000000\n4132.200000\n43668.000000\n7.000000\n2.000000\n5.000000\n878.680000\n44.700000\n-2672.000000\n21553.000000\n12622.945077\n2.000000\n4.000000\n4.000000\n44540.500000\n\n\nmax\n11382.000000\n115038.200000\n609736.200000\n23.000000\n9.000000\n16.000000\n44708.000000\n44708.000000\n44708.000000\n74812.000000\n57782.701468\n2.000000\n6.000000\n7.000000\n138317.800000\n\n\n\n\n\n\n\nThe validate_monthly_aggregated_transactions function is invoked to ensure the integrity and correctness of the aggregated data through several assertions:\n\nThe balance should consistently increase or decrease based on whether the total monthly transaction volume is positive or negative, respectively.\nFor each account, the balance in the first month should equal the total transaction volume of that month.\nThe sum of positive and negative transaction counts must equal the total transaction count for each month.\nThe number of unique accounts in the aggregated data should match that in the original dataset.\nThe final balances of accounts in the aggregated data should closely match their last recorded transactions in the original dataset.\n\n\ndef validate_monthly_aggregated_transactions(aggregated_data, original_df):\n    \"\"\"\n    Validate the integrity and correctness of aggregated monthly financial transactions.\n\n    Parameters:\n    - aggregated_data (pd.DataFrame): Aggregated monthly transaction data.\n    - original_df (pd.DataFrame): Original dataset of financial transactions.\n\n    Raises:\n    - AssertionError: If validation conditions are not met.\n    \"\"\"\n\n    assert (aggregated_data[\"volume\"] &gt;= 0).all() == (\n        aggregated_data[\"balance\"].diff() &gt;= 0\n    ).all(), \"If the total amount is positive, the balance should go up.\"\n\n    assert (aggregated_data[\"volume\"] &lt; 0).all() == (\n        aggregated_data[\"balance\"].diff() &lt; 0\n    ).all(), \"If the total amount is negative, the balance should go down.\"\n\n    first_month = aggregated_data.groupby(\"account_id\").nth(0)\n    assert (\n        first_month[\"volume\"] == first_month[\"balance\"]\n    ).all(), \"The balance should equal the volume for the first month.\"\n\n    assert (\n        aggregated_data[\"positive_transaction_count\"]\n        + aggregated_data[\"negative_transaction_count\"]\n        == aggregated_data[\"transaction_count\"]\n    ).all(), \"The sum of positive and negative transaction counts should equal the total transaction count.\"\n\n    assert (\n        aggregated_data[\"account_id\"].nunique() == original_df[\"account_id\"].nunique()\n    ), \"The number of unique account_ids in the aggregated DataFrame should be the same as the original DataFrame.\"\n\n    assert (\n        pd.merge(\n            aggregated_data.groupby(\"account_id\")\n            .last()\n            .reset_index()[[\"account_id\", \"balance\"]],\n            original_df[\n                original_df.groupby(\"account_id\")[\"date\"].transform(\"max\")\n                == original_df[\"date\"]\n            ][[\"account_id\", \"balance\"]],\n            on=\"account_id\",\n            suffixes=(\"_final\", \"_last\"),\n        )\n        .apply(\n            lambda x: np.isclose(x[\"balance_final\"], x[\"balance_last\"], atol=5), axis=1\n        )\n        .any()\n    ), \"Some accounts' final balances do not match their last transactions.\"\n\n\nvalidate_monthly_aggregated_transactions(agg_transactions_monthly_df, transactions_df)"
  },
  {
    "objectID": "main.html#exploratory-data-analysis-aggregated-monthly-transactions",
    "href": "main.html#exploratory-data-analysis-aggregated-monthly-transactions",
    "title": "Data Import & Wrangling",
    "section": "4 Exploratory Data Analysis: Aggregated Monthly Transactions",
    "text": "4 Exploratory Data Analysis: Aggregated Monthly Transactions\n\n4.1 Monthly Balance Difference and Volume\nThis plot gives a clear picture of how money moves in and out of an account each month and how these movements affect the overall balance. It does this by showing two things:\n\nBalance Difference: This line shows whether the account balance went up or down each month. If the line goes up, it means the account gained money that month. If it goes down, the account lost money.\nVolume: This line shows the total amount of money that moved in the account each month, regardless of whether it was coming in or going out.\n\nWhat to Look For: - A direct link between the amount of money moved (volume) and changes in the account balance. High incoming money should lead to an uptick in the balance, and lots of outgoing money should lead to a downturn. - This visual check helps to understand how active the account is and whether it’s generally getting fuller or emptier over time.\n\ndef plot_monthly_balance_diff_and_volume(\n    transactions_monthly, account_id, figsize=(12, 8)\n):\n    account_transactions = transactions_monthly[\n        transactions_monthly[\"account_id\"] == account_id\n    ].sort_values(by=\"month\")\n    account_transactions[\"balance_diff\"] = account_transactions[\"balance\"].diff()\n\n    plt.figure(figsize=figsize)\n\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"balance_diff\"],\n        marker=\"o\",\n        label=\"Balance Difference\",\n    )\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"volume\"],\n        marker=\"x\",\n        linestyle=\"--\",\n        label=\"Volume\",\n    )\n\n    plt.title(f\"Monthly Balance Difference and Volume for Account {account_id}\")\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_monthly_balance_diff_and_volume(agg_transactions_monthly_df, 2)\n\n\n\n\n\n\n\n\n\n\n4.2 Monthly Transactions, Balance, and Volume Plot Explanation\nThis visualization offers a snapshot of an account’s activity over time by comparing money movement each month with the overall account balance. It helps to understand:\n\nVolume: How much money came in or went out of the account each month. Incoming money is shown as up, and outgoing money as down.\nBalance: The total money in the account at the end of each month, showing how it’s changed over time due to the monthly transactions.\n\nWhat to Look For: - How the monthly money movement impacts the account’s growing or shrinking balance. For example, a few months of high income should visibly increase the balance. - This simple visual guide helps spot trends, like if the account is steadily growing, holding steady, or facing issues, giving quick insights into financial well-being and further validates the aggregation made in the previous step.\n\ndef plot_monthly_transactions_balance_and_volume(agg_transactions_monthly, account_id):\n    account_transactions = agg_transactions_monthly[\n        agg_transactions_monthly[\"account_id\"] == account_id\n    ]\n\n    plt.figure(figsize=(15, 10))\n\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"volume\"],\n        marker=\"o\",\n        label=\"Volume\",\n    )\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"balance\"],\n        marker=\"x\",\n        linestyle=\"--\",\n        label=\"Balance\",\n    )\n\n    plt.title(f\"Monthly Transactions and Balance for Account {account_id}\")\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.xticks(rotation=60)\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 2)\n\n\n\n\n\n\n\n\n\n\n4.3 Delieverable: Closer Look at Account 14\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 14)\n\n\n\n\n\n\n\n\nAccount 14 shows a rather conservative transaction history. The spending habits are all withing range of 10k to -10k per month. We can see little volatility, the account shows a slight trend of growing.\n\n\n4.4 Delieverable: Closer Look at Account 18\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 18)\n\n\n\n\n\n\n\n\nAccount 18 paints a different picture in comparison to account 14.\nThe volatility here is a lot higher, indiciating a potential for a business account or high income household. Especially March 1994 to December 1994 show some volatile transaction habits.\nLooking at the balance and volume per month for the accounts 14 and 18 we can notice some interesting patterns.\nTODO: Add analysis"
  },
  {
    "objectID": "main.html#pivot-transactions-rolling-up-to-monthly-aggregates",
    "href": "main.html#pivot-transactions-rolling-up-to-monthly-aggregates",
    "title": "Data Import & Wrangling",
    "section": "5 Pivot Transactions: Rolling Up to Monthly Aggregates",
    "text": "5 Pivot Transactions: Rolling Up to Monthly Aggregates\nWe have condensed transaction data into a monthly aggregated format. This aggregation serves a multifaceted purpose:\n\nMonthly aggregation standardizes the time frame across which we analyze transactions, allowing us to compare transactional behaviors consistently across all accounts.\nAggregating data on a monthly level illuminates patterns that daily data might obscure. It enables us to discern trends over a broader time scale, capturing cyclical behaviors, seasonal effects, and response to macroeconomic events.\nDaily transaction data can be “noisy” with random fluctuations. By considering monthly totals and averages, we reduce this noise, revealing underlying trends more clearly.\nOur primary objective is to understand behaviors leading up to the issuance of a card. Aggregating transactions on a monthly basis helps focus on the crucial period preceding card issuance, enabling us to correlate transactional behaviors with the propensity to become a cardholder.\n\n\ndef pivot_transactions(\n    non_transactional, transactions_monthly, months_before_card_range=(2, 13)\n):\n    \"\"\"\n    Aggregate monthly transaction data and merge it with non-transactional account data,\n    focusing on the time frame leading up to the card issuance.\n\n    This function merges monthly transaction data with non-transactional data to associate each\n    transaction with the respective account and card issued date. It then filters transactions based\n    on a specified range of months before card issuance and aggregates various transaction metrics.\n\n    Parameters:\n    - non_transactional (pd.DataFrame): A DataFrame containing non-transactional account data. This is only used to map card issuance dates to transactions.\n    - transactions_monthly (pd.DataFrame): A DataFrame containing monthly transaction data.\n    - months_before_card_range (tuple): A tuple specifying the inclusive range of months before card\n                                        issuance to filter the transactions for aggregation.\n\n    The aggregation includes the sum of volume and transaction counts, as well as the mean and other\n    statistical measures of transaction amounts, for each account within the specified months before\n    card issuance.\n\n    The resulting DataFrame is pivoted to have 'account_id' as rows and the months before card\n    issuance as columns, with aggregated metrics as values. Column names are constructed to\n    describe the month and the metric represented.\n\n    Returns:\n    - pd.DataFrame: The final aggregated and pivoted dataset ready for analysis, with each row\n                    representing an account and each column a specific metric in the months before\n                    card issuance.\n    \"\"\"\n    merged_df = transactions_monthly.merge(\n        non_transactional[[\"account_id\"]], on=\"account_id\"\n    )\n\n    merged_df[\"card_issued_date\"] = merged_df[\"account_id\"].map(\n        non_transactional.set_index(\"account_id\")[\"card_issued\"]\n    )\n    merged_df[\"months_before_card\"] = merged_df.apply(\n        lambda row: (row[\"card_issued_date\"].to_period(\"M\") - row[\"month\"]).n, axis=1\n    )\n\n    start_month, end_month = months_before_card_range\n    filtered_df = merged_df.query(f\"{start_month} &lt;= months_before_card &lt;= {end_month}\")\n\n    aggregated_data = (\n        filtered_df.groupby([\"account_id\", \"months_before_card\"])\n        .agg(\n            {\n                \"volume\": \"sum\",\n                \"total_abs_amount\": \"sum\",\n                \"transaction_count\": \"sum\",\n                \"positive_transaction_count\": \"sum\",\n                \"negative_transaction_count\": \"sum\",\n                \"average_amount\": \"mean\",\n                \"median_amount\": \"median\",\n                \"min_amount\": \"min\",\n                \"max_amount\": \"max\",\n                \"std_amount\": \"std\",\n                \"type_count\": \"sum\",\n                \"operation_count\": \"sum\",\n                \"k_symbol_count\": \"sum\",\n                \"balance\": \"mean\",\n            }\n        )\n        .reset_index()\n    )\n\n    pivoted_data = aggregated_data.pivot(\n        index=\"account_id\", columns=\"months_before_card\"\n    )\n    pivoted_data.columns = [\n        \"_\".join([\"M\", str(col[1]), col[0]]) for col in pivoted_data.columns.values\n    ]\n\n    final_dataset = pivoted_data.reset_index()\n    return final_dataset\n\n\ntransactions_pivoted_df = pivot_transactions(\n    matched_non_card_holders_w_issue_date_df, agg_transactions_monthly_df\n)\ntransactions_pivoted_df.describe()\n\n\n\n\n\n\n\n\naccount_id\nM_2_volume\nM_3_volume\nM_4_volume\nM_5_volume\nM_6_volume\nM_7_volume\nM_8_volume\nM_9_volume\nM_10_volume\n...\nM_4_balance\nM_5_balance\nM_6_balance\nM_7_balance\nM_8_balance\nM_9_balance\nM_10_balance\nM_11_balance\nM_12_balance\nM_13_balance\n\n\n\n\ncount\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n...\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n656.000000\n655.000000\n656.000000\n656.000000\n\n\nmean\n2824.231707\n-188.081402\n2131.023476\n-83.378354\n1166.012652\n-662.276982\n1418.670732\n1039.067683\n1139.186890\n-633.459299\n...\n43713.226677\n43796.605030\n42630.592378\n43292.869360\n41874.198628\n40835.130945\n39695.944055\n40352.773588\n39736.136585\n39596.693293\n\n\nstd\n2370.634460\n12069.931426\n13667.115448\n13070.792019\n14518.623820\n13857.280237\n14971.381727\n13142.470175\n12976.711696\n15761.581790\n...\n20210.049464\n21178.364095\n20094.254246\n20971.757649\n20486.289111\n19711.803414\n19561.774856\n20190.800543\n19130.114948\n19766.034091\n\n\nmin\n11.000000\n-76779.500000\n-54322.500000\n-69155.200000\n-62718.000000\n-67190.700000\n-62113.900000\n-84970.900000\n-75013.500000\n-66945.600000\n...\n1762.200000\n-4575.900000\n677.800000\n-8789.700000\n-1299.300000\n-7.900000\n-3269.700000\n820.700000\n-9843.200000\n192.000000\n\n\n25%\n1146.750000\n-2860.275000\n-2118.800000\n-2363.675000\n-3044.425000\n-3524.050000\n-2881.525000\n-1730.000000\n-2057.200000\n-3582.750000\n...\n26655.925000\n26337.725000\n26830.275000\n26528.400000\n25485.675000\n25222.525000\n24919.950000\n24950.750000\n25026.475000\n24524.575000\n\n\n50%\n2330.500000\n601.850000\n1345.850000\n1163.600000\n1196.400000\n841.700000\n1198.750000\n1328.200000\n1105.650000\n890.900000\n...\n42680.500000\n42269.700000\n41417.900000\n41245.050000\n39681.050000\n38903.600000\n36803.850000\n37070.200000\n37223.300000\n36958.000000\n\n\n75%\n3666.000000\n3583.525000\n4677.725000\n4250.250000\n4512.375000\n4456.775000\n4391.850000\n4530.200000\n4713.650000\n3944.450000\n...\n54942.825000\n55514.200000\n54229.400000\n56922.150000\n53359.175000\n53476.650000\n51744.600000\n50787.050000\n50391.900000\n49328.125000\n\n\nmax\n11382.000000\n57541.800000\n76136.000000\n60272.100000\n69456.400000\n65912.700000\n88209.200000\n55601.500000\n72059.700000\n98041.500000\n...\n105453.800000\n112335.700000\n106459.000000\n111264.300000\n106472.800000\n108358.900000\n132286.300000\n108202.400000\n112159.300000\n111050.900000\n\n\n\n\n8 rows × 169 columns"
  },
  {
    "objectID": "main.html#merge-everything-together",
    "href": "main.html#merge-everything-together",
    "title": "Data Import & Wrangling",
    "section": "6 Merge everything together",
    "text": "6 Merge everything together\n\ngolden_record_df = matched_non_card_holders_w_issue_date_df.merge(\n    transactions_pivoted_df, on=\"account_id\", how=\"left\"\n)\ngolden_record_df.to_csv(\"data/golden_record.csv\", index=False)\ndata_reduction[\"Final Golden Record\"] = len(golden_record_df)\n\nassert golden_record_df[\n    \"client_id\"\n].is_unique, \"Each client_id should appear exactly once in the final DataFrame.\"\nassert golden_record_df[\n    \"account_id\"\n].is_unique, \"Each account_id should appear exactly once in the final DataFrame.\"\n\ngolden_record_df.head()\n\n\n\n\n\n\n\n\naccount_id\naccount_district_id\naccount_frequency\naccount_created\naccount_district_name\naccount_region\naccount_inhabitants\naccount_small_municipalities\naccount_medium_municipalities\naccount_large_municipalities\n...\nM_4_balance\nM_5_balance\nM_6_balance\nM_7_balance\nM_8_balance\nM_9_balance\nM_10_balance\nM_11_balance\nM_12_balance\nM_13_balance\n\n\n\n\n0\n576\n55\nMONTHLY_ISSUANCE\n1993-01-01\nBrno - venkov\nsouth Moravia\n157042\n49\n70\n18\n...\n35433.9\n32763.3\n30103.6\n27455.2\n39623.2\n41346.8\n40646.7\n37953.8\n35272.1\n34357.8\n\n\n1\n3818\n74\nMONTHLY_ISSUANCE\n1993-01-01\nOstrava - mesto\nnorth Moravia\n323870\n0\n0\n0\n...\n32448.7\n20928.3\n48812.7\n45898.7\n38802.0\n41537.8\n30863.9\n49023.8\n44879.5\n41579.8\n\n\n2\n704\n55\nMONTHLY_ISSUANCE\n1993-01-01\nBrno - venkov\nsouth Moravia\n157042\n49\n70\n18\n...\n49756.6\n35452.9\n44789.8\n33259.4\n19026.1\n49039.4\n31829.9\n35224.0\n46092.2\n31803.3\n\n\n3\n1695\n76\nMONTHLY_ISSUANCE\n1993-01-03\nSumperk\nnorth Moravia\n127369\n31\n32\n13\n...\n98674.9\n97326.2\n105257.0\n68223.8\n101761.7\n62686.1\n55853.1\n81933.7\n79168.4\n99457.9\n\n\n4\n2379\n44\nMONTHLY_ISSUANCE\n1993-01-10\nChrudim\neast Bohemia\n105606\n77\n26\n7\n...\n20854.4\n20774.5\n19017.2\n17267.0\n15524.3\n19539.7\n15254.2\n14955.0\n13221.3\n30056.2\n\n\n\n\n5 rows × 232 columns\n\n\n\nLooking at the first few rows of the final golden record, we can see the aggregated transactional data for each account, with columns representing various metrics for each month leading up to the card issuance date.\n\nplt.figure(figsize=(10, 6))\nplt.title(\"Number of Clients by Card Issuance Status\")\nsns.countplot(x=\"has_card\", data=golden_record_df)\nplt.xlabel(\"Card Issued\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the number of clients with a card issued is equal to the number of clients without a card issued, indicating a successful matching process.\n\nplt.figure(figsize=(10, 6))\nplt.title(\"Distribution of Card Issuance Dates\")\nsns.histplot(\n    golden_record_df, x=\"card_issued\", hue=\"has_card\", kde=True, bins=30, alpha=0.5\n)\nplt.xlabel(\"Card Issuance Date\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nThe distribution of card issuance dates shows that the card issuance process was spread out over time, with an expected identical distribution for clients with and without cards issued."
  },
  {
    "objectID": "main.html#data-reduction-summary",
    "href": "main.html#data-reduction-summary",
    "title": "Data Import & Wrangling",
    "section": "7 Data Reduction Summary",
    "text": "7 Data Reduction Summary\nThe following waterfall chart visualizes the data reduction process, highlighting the number of records retained or lost at each stage.\n\nimport plotly.graph_objects as go\n\ndata_reduction_df = pd.DataFrame(\n    list(data_reduction.items()), columns=[\"Category\", \"Amount\"]\n)\ncolors = [\"skyblue\" if amt &gt;= 0 else \"orange\" for amt in data_reduction_df[\"Amount\"]]\n\nfig = go.Figure(\n    go.Waterfall(\n        name=\"20\",\n        orientation=\"v\",\n        measure=[\"relative\"] * (len(data_reduction_df) - 1) + [\"total\"],\n        x=data_reduction_df[\"Category\"],\n        textposition=\"outside\",\n        text=[f\"{amt:,.0f}\" for amt in data_reduction_df[\"Amount\"]],\n        y=data_reduction_df[\"Amount\"],\n        connector={\"line\": {\"color\": \"black\", \"width\": 2}},\n        decreasing={\"marker\": {\"color\": \"orange\"}},\n        increasing={\"marker\": {\"color\": \"skyblue\"}},\n        totals={\"marker\": {\"color\": \"skyblue\"}},\n    )\n)\n\nfig.update_layout(\n    title=\"Enhanced Data Reduction Waterfall Chart\",\n    xaxis=dict(title=\"Category\"),\n    yaxis=dict(title=\"Amount\", range=[0, 5500]),\n    waterfallgap=0.3,\n)\nfig.show()"
  },
  {
    "objectID": "main.html#exploratory-data-analysis-golden-record",
    "href": "main.html#exploratory-data-analysis-golden-record",
    "title": "Data Import & Wrangling",
    "section": "8 Exploratory Data Analysis: Golden Record",
    "text": "8 Exploratory Data Analysis: Golden Record\n\n8.1 Comparing Cardholders and Non-Cardholders\n\n8.1.1 Trends in Monthly Financial Metrics\n\ngolden_cardholders = golden_record_df[golden_record_df[\"has_card\"]]\ngolden_non_cardholders = golden_record_df[~golden_record_df[\"has_card\"]]\n\n\ndef plot_trends_with_medians(\n    cardholders, non_cardholders, columns, title, median_ranges\n):\n    \"\"\"\n    Plots line graphs for average monthly values and annotates medians for specified ranges,\n    adjusting x-axis indices to match the month sequence from the start.\n\n    Parameters:\n    - cardholders (pd.DataFrame): DataFrame containing data for cardholders.\n    - non_cardholders (pd.DataFrame): DataFrame containing data for non-cardholders.\n    - columns (list of str): List of column names ordered by time.\n    - title (str): Title for the plot.\n    - median_ranges (list of tuples): Each tuple contains start and end indices for calculating medians.\n    \"\"\"\n    cardholder_avgs = cardholders[columns].mean()\n    non_cardholder_avgs = non_cardholders[columns].mean()\n\n    months = list(range(1, 1 + len(columns)))\n    plt.figure(figsize=(14, 7))\n    plt.plot(\n        months,\n        cardholder_avgs.values,\n        marker=\"o\",\n        linestyle=\"-\",\n        color=\"blue\",\n        label=\"Cardholders\",\n    )\n    plt.plot(\n        months,\n        non_cardholder_avgs.values,\n        marker=\"o\",\n        linestyle=\"-\",\n        color=\"orange\",\n        label=\"Non-Cardholders\",\n    )\n\n    for start, end in median_ranges:\n        median_cardholder = cardholders[columns[start : end + 1]].median().median()\n        median_non_cardholder = (\n            non_cardholders[columns[start : end + 1]].median().median()\n        )\n        plt.hlines(\n            median_cardholder,\n            months[start],\n            months[end],\n            colors=\"darkblue\",\n            linestyles=\"--\",\n            label=f\"Median {start+1}-{end+1} (Cardholders): {median_cardholder:.2f}\",\n        )\n        plt.hlines(\n            median_non_cardholder,\n            months[start],\n            months[end],\n            colors=\"red\",\n            linestyles=\"--\",\n            label=f\"Median {start+1}-{end+1} (Non-Cardholders): {median_non_cardholder:.2f}\",\n        )\n\n    plt.title(title)\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.grid(True)\n    plt.xticks(months, labels=[f\"M_{month}\" for month in months])  # Proper month labels\n    plt.show()\n\n\n\n8.1.2 Monthly Balance Trends\n\nmedian_ranges = [\n    (0, 2),\n    (9, 11),\n]  # First 3 months and last 3 months for a 12-month period\nbalance_columns = [f\"M_{i}_balance\" for i in range(2, 14)]\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    balance_columns,\n    \"Monthly Balance Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n8.1.3 Monthly Volume Trends\n\nvolume_columns = [\n    f\"M_{i}_volume\" for i in range(2, 14)\n]  # Simulating monthly volume columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    volume_columns,\n    \"Monthly Volume Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n8.1.4 Monthly Transaction Count Trends\n\ntransaction_count_columns = [\n    f\"M_{i}_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    transaction_count_columns,\n    \"Monthly Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n8.1.5 Monthly Positive and Negative Transaction Count Trends\n\npositive_transaction_count_columns = [\n    f\"M_{i}_positive_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly positive transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    positive_transaction_count_columns,\n    \"Monthly Positive Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n8.1.6 Monthly Negative Transaction Count Trends\n\nnegative_transaction_count_columns = [\n    f\"M_{i}_negative_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly negative transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    negative_transaction_count_columns,\n    \"Monthly Negative Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n8.1.7 Comparison of Average Feature Values\n\ndef plot_grouped_comparison(cardholders, non_cardholders, feature_columns):\n    \"\"\"\n    Plots grouped bar charts for average feature values of cardholders and non-cardholders.\n\n    Parameters:\n    - cardholders (pd.DataFrame): DataFrame containing data for cardholders.\n    - non_cardholders (pd.DataFrame): DataFrame containing data for non-cardholders.\n    - feature_columns (list of str): List of column names whose averages to compare.\n    \"\"\"\n    cardholder_avg = cardholders[feature_columns].mean()\n    non_cardholder_avg = non_cardholders[feature_columns].mean()\n\n    index = range(len(feature_columns))\n    bar_width = 0.35\n\n    fig, ax = plt.subplots(figsize=(14, 8))\n    bars1 = ax.bar(\n        index, cardholder_avg, bar_width, label=\"Cardholders\", color=\"skyblue\"\n    )\n    bars2 = ax.bar(\n        [p + bar_width for p in index],\n        non_cardholder_avg,\n        bar_width,\n        label=\"Non-Cardholders\",\n        color=\"orange\",\n    )\n\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Average Value\")\n    ax.set_title(\"Average Feature Values by Group\")\n    ax.set_xticks([p + bar_width / 2 for p in index])\n    ax.set_xticklabels(feature_columns)\n    ax.legend()\n\n    plt.xticks(rotation=45)  # Rotate feature names for better visibility\n    plt.show()\n\n\nplot_grouped_comparison(\n    golden_cardholders,\n    golden_non_cardholders,\n    [col for col in golden_record_df.columns if \"balance\" in col],\n)\nplot_grouped_comparison(golden_cardholders, golden_non_cardholders, [\"loan_amount\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### DEPENDENCIES TODO REMOVE FOR MERGE\n\n# save golden record to temp\ngolden_record_df.to_parquet(\"temp/golden_record.parquet\")\n\n\n### DEPENDENCY #TODO REMOVE FOR MERGE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ngolden_record_df = pd.read_parquet('temp/golden_record.parquet')"
  },
  {
    "objectID": "main.html#data-partitioning",
    "href": "main.html#data-partitioning",
    "title": "Data Import & Wrangling",
    "section": "9 Data Partitioning",
    "text": "9 Data Partitioning\nThe data is split in a 80/20 ratio for training and testing purposes. The stratification ensures that the distribution of the target variable is maintained in both sets. When actually training the models, we will additionally use cross-validation to ensure robust evaluation.\n\nfrom sklearn.model_selection import train_test_split\n\n\nclass DataModule:\n    def __init__(self, X_train, X_test, y_train, y_test, feature_columns=None):\n        self.feature_columns = feature_columns if feature_columns is not None else X_train.columns\n\n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train\n        self.y_test = y_test\n\n\ndef create_data_module(df, feature_cols, target_col=\"has_card\", test_size=0.2):\n    X = df.drop(columns=[target_col])[feature_cols]\n    y = df[target_col]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, stratify=y, shuffle=True\n    )\n\n    return DataModule(X_train, X_test, y_train, y_test)\n\n\ndata_module = create_data_module(golden_record_df, golden_record_df.drop(columns=[\"has_card\"]).columns)\n\nprint(f\"Train set size: {len(data_module.X_train)}\")\nprint(f\"Test set size: {len(data_module.X_test)}\")\n\nprint(f\"Train set distribution:\\n{data_module.y_train.value_counts(normalize=True)}\")\nprint(f\"Test set distribution:\\n{data_module.y_test.value_counts(normalize=True)}\")\n\nTrain set size: 524\nTest set size: 132\nTrain set distribution:\nhas_card\nTrue     0.5\nFalse    0.5\nName: proportion, dtype: float64\nTest set distribution:\nhas_card\nTrue     0.5\nFalse    0.5\nName: proportion, dtype: float64\n\n\nAs we can see the distribution of the target variable is maintained in both sets after the split."
  },
  {
    "objectID": "main.html#model-construction",
    "href": "main.html#model-construction",
    "title": "Data Import & Wrangling",
    "section": "10 Model Construction",
    "text": "10 Model Construction\n\n10.1 Pipeline for Training and Evaluation\nThe train_evaluate_model function is designed to streamline the process of training and evaluating machine learning models. It performs the following steps:\n\nPreprocessing: The function automatically handles numerical and categorical features, imputing missing values, scaling numerical features, and one-hot encoding categorical features.\nModel Training: The specified model is trained on the training data.\nCross-Validation: The model is evaluated using cross-validation with specified evaluation metrics.\nModel Evaluation: The model is evaluated on the test set using various metrics, including accuracy, F1 score, AUC-ROC, precision, and recall.\n\nThe pipeline is flexible and can accommodate various models and feature sets, making it a versatile tool for model development and evaluation. It returns a summary of evaluation metrics for both training and test sets, as well as the true labels and predicted probabilities for the test set.\n\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score\nimport scikitplot as skplt\nimport dalex as dx\n\n\nclass Trainer:\n    def __init__(self, data_module, model, cv=10, verbose=False):\n        self.data_module = data_module\n        self.model = model\n        self.cv = cv\n        self.verbose = verbose\n        self.preprocessor = self._create_preprocessor()\n        self.pipeline = None\n        self.train_metrics_report = None\n        self.test_metrics_report = None\n\n    def _create_preprocessor(self):\n        numerical_features = [col for col in self.data_module.X_train.columns if\n                              self.data_module.X_train[col].dtype in [\"int64\", \"float64\"]]\n        categorical_features = [col for col in self.data_module.X_train.columns if col not in numerical_features]\n\n        other_features = [col for col in self.data_module.X_train.columns if\n                          col not in numerical_features + categorical_features]\n        if len(other_features) &gt; 0:\n            raise ValueError(f\"Columns with unsupported data types found: {other_features}\")\n\n        numerical_pipeline = Pipeline(\n            [(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n        )\n\n        categorical_pipeline = Pipeline(\n            [\n                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n            ]\n        )\n\n        return ColumnTransformer(\n            transformers=[\n                (\"num\", numerical_pipeline, numerical_features),\n                (\"cat\", categorical_pipeline, categorical_features),\n            ]\n        )\n\n    def fit(self):\n        model_pipeline = Pipeline([(\"model\", self.model)])\n        self.pipeline = Pipeline([\n            (\"preprocessor\", self.preprocessor),\n            (\"model_pipeline\", model_pipeline)\n        ])\n        self.pipeline.fit(self.data_module.X_train, self.data_module.y_train)\n        return self\n\n    def eval_train(self):\n        scoring = {\n            \"accuracy\": \"accuracy\",\n            \"f1_macro\": make_scorer(f1_score),\n            \"roc_auc\": \"roc_auc\",\n            \"precision\": make_scorer(precision_score),\n            \"recall\": make_scorer(recall_score),\n        }\n        train_metrics_summary = cross_validate(\n            self.pipeline, self.data_module.X_train, self.data_module.y_train, scoring=scoring, cv=self.cv,\n            return_train_score=False, n_jobs=-1, verbose=3 if self.verbose else 0,\n            error_score=\"raise\"\n        )\n        self.train_metrics_report = {\n            metric: {\n                \"folds\": train_metrics_summary[f\"test_{metric}\"].tolist(),\n                \"mean\": train_metrics_summary[f\"test_{metric}\"].mean(),\n                \"std\": train_metrics_summary[f\"test_{metric}\"].std()\n            } for metric in scoring\n        }\n\n        return self\n\n    def eval_test(self):\n        X_test, y_test = self.data_module.X_test, self.data_module.y_test\n        y_pred_proba = self.pipeline.predict_proba(X_test)[:, 1] if hasattr(self.pipeline, \"predict_proba\") else np.nan\n        test_metrics = {\n            \"accuracy\": self.pipeline.score(X_test, y_test),\n            \"f1_macro\": f1_score(y_test, self.pipeline.predict(X_test), average=\"macro\"),\n            \"roc_auc\": roc_auc_score(y_test, y_pred_proba) if hasattr(self.pipeline, \"predict_proba\") else np.nan,\n            \"precision\": precision_score(y_test, self.pipeline.predict(X_test)),\n            \"recall\": recall_score(y_test, self.pipeline.predict(X_test))\n        }\n        self.test_metrics_report = {metric: test_metrics[metric] for metric in test_metrics}\n\n        return self\n\n    def get_trained_model(self):\n        return self.pipeline\n\n    def get_preprocessor(self):\n        return self.preprocessor\n\n    def get_train_metrics_report(self):\n        return self.train_metrics_report\n\n    def get_test_metrics_report(self):\n        return self.test_metrics_report\n\n\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\n\nclass Visualizer:\n    def __init__(self, trainer, model_name):\n        self.trainer = trainer\n        self.model_name = model_name\n\n        X_train, X_test, y_train, y_test = (\n            self.trainer.data_module.X_train,\n            self.trainer.data_module.X_test,\n            self.trainer.data_module.y_train,\n            self.trainer.data_module.y_test)\n\n        self.explainer = dx.Explainer(trainer.get_trained_model(), X_test, y_test)\n\n        self.X_test = X_test\n        self.y_true = y_test\n        self.y_test_pred_proba = (trainer.get_trained_model()\n                                  .predict_proba(X_test))\n\n    def plot_validation_metrics(self):\n        train_metrics = self.trainer.get_train_metrics_report()\n        cv = len(train_metrics['accuracy']['folds'])\n\n        metrics = list(train_metrics.keys())\n        fold_scores = {metric: train_metrics[metric]['folds'] for metric in metrics}\n\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.boxplot(fold_scores.values(), labels=metrics, notch=True, patch_artist=True)\n        ax.set_title(f'{self.model_name}: Validation Metrics Box Plot (CV={cv})')\n        ax.set_xlabel('Metrics')\n        ax.set_ylabel('Score')\n        ax.set_ylim(0, 1)\n        ax.grid(True)\n        fig.show()\n\n    def plot_test_metrics(self):\n        test_metrics = self.trainer.get_test_metrics_report()\n        test_values = list(test_metrics.values())\n        test_names = list(test_metrics.keys())\n\n        fig, ax = plt.subplots(figsize=(12, 6))\n        sns.barplot(x=test_names, y=test_values, ax=ax)\n        ax.set_title(f'{self.model_name}: Test Metrics')\n        ax.set_xlabel('Metrics')\n        ax.set_ylabel('Score')\n        for i, v in enumerate(test_values):\n            if np.isnan(v):\n                ax.text(i, 0.5, \"N/A\", ha='center', va='bottom')\n            else:\n                ax.text(i, v + 0.01, f\"{v:.2f}\", ha='center', va='bottom')\n        ax.set_ylim(0, 1)\n        ax.grid(True)\n        fig.show()\n\n    def plot_confusion_matrix(self):\n        preds = self.y_test_pred_proba.argmax(axis=1)\n        fig, ax = plt.subplots(figsize=(12, 6))\n        skplt.metrics.plot_confusion_matrix(self.y_true, preds, ax=ax)\n        ax.set_title(f'{self.model_name}: Confusion Matrix')\n\n    def plot_classification_report(self):\n        preds = self.y_test_pred_proba.argmax(axis=1)\n        report = classification_report(self.y_true, preds, output_dict=True)\n\n        report_df = pd.DataFrame(report).transpose()\n        report_df = report_df.round(2)\n\n        fig, ax = plt.subplots(figsize=(12, 6))\n        table = ax.table(cellText=report_df.values, colLabels=report_df.columns, rowLabels=report_df.index,\n                         cellLoc='center', rowLoc='center', loc='center', fontsize=12)\n        table.auto_set_font_size(False)\n        table.set_fontsize(12)\n        table.scale(1.2, 1.2)\n\n        ax.axis('off')\n        ax.set_title(f'{self.model_name}: Classification Report')\n        fig.show()\n\n    def plot_threshold_optimization(self):\n        precision, recall, thresholds = precision_recall_curve(self.y_true, self.y_test_pred_proba[:, 1])\n        f1_scores = 2 * (precision * recall) / (precision + recall)\n        optimal_idx = np.argmax(f1_scores)\n        optimal_threshold = thresholds[optimal_idx]\n\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.plot(thresholds, f1_scores[:-1], label='F1-score')\n        ax.axvline(x=optimal_threshold, color='red', linestyle='--',\n                   label=f'Optimal Threshold: {optimal_threshold:.2f}')\n        ax.set_title(f'{self.model_name}: Threshold Optimization')\n        ax.set_xlabel('Threshold')\n        ax.set_ylabel('F1-score')\n        ax.legend()\n        fig.show()\n\n    def plot_roc_curve(self):\n        fig, ax = plt.subplots(figsize=(12, 6))\n        skplt.metrics.plot_roc(self.y_true, self.y_test_pred_proba, ax=ax)\n        ax.set_title(f'{self.model_name}: ROC Curve on Test Set')\n        fig.show()\n\n    def plot_precision_recall_curve(self):\n        fig, ax = plt.subplots(figsize=(12, 6))\n        skplt.metrics.plot_precision_recall(self.y_true, self.y_test_pred_proba, ax=ax)\n        ax.set_title(f'{self.model_name}: Precision-Recall Curve on Test Set')\n        fig.show()\n\n    def plot_lift_curve(self):\n        fig, ax = plt.subplots(figsize=(12, 6))\n        skplt.metrics.plot_lift_curve(self.y_true, self.y_test_pred_proba, ax=ax)\n        ax.set_title(f'{self.model_name}: Lift Curve on Test Set')\n        fig.show()\n\n    def plot_cumulative_gain_curve(self):\n        fig, ax = plt.subplots(figsize=(12, 6))\n        skplt.metrics.plot_cumulative_gain(self.y_true, self.y_test_pred_proba, ax=ax)\n        ax.set_title(f'{self.model_name}: Cumulative Gain Curve on Test Set')\n        fig.show()\n\n    def plot_partial_dependence(self, feature):\n        pdp = self.explainer.model_profile(type='partial', variables=feature)\n        pdp.plot()\n\n    def plot_accumulated_local_effects(self, feature):\n        ale = self.explainer.model_profile(type='accumulated', variables=feature)\n        ale.plot()\n\n    def plot_breakdown(self, observation):\n        breakdown = self.explainer.predict_parts(observation, type='break_down')\n        breakdown.plot()\n\n    def plot_model_explanations(self):\n        # Feature importance\n        feature_importance = self.explainer.model_parts()\n        feature_importance.plot()\n\n        # Model profiles\n        model_profile = self.explainer.model_profile(type='partial')\n        model_profile.plot()\n\n    def visualize_explanations(self, feature_columns=[]):\n        self.plot_model_explanations()\n\n        if not feature_columns:\n            feature_columns = self.trainer.data_module.feature_columns[0]\n\n        # Partial Dependence Plot\n        self.plot_partial_dependence(feature_columns)\n\n        # Accumulated Local Effects\n        self.plot_accumulated_local_effects(feature_columns)\n\n        # Breakdown Plot\n        observation = self.trainer.data_module.X_test.iloc[0]\n        self.plot_breakdown(observation)\n\n        plt.show()\n\n\ndef create_top_n_customers_list(model, data, n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a Top-N list of customers likely to not have a card.\n\n    Args:\n    model (BaseEstimator): A trained scikit-learn model.\n    data (pd.DataFrame): Dataframe containing the features needed by the model.\n    n (int): Number of top customers to retrieve.\n\n    Returns:\n    pd.DataFrame: Dataframe containing the top N customers likely to not have a card.\n    \"\"\"\n    if not hasattr(model, 'predict_proba'):\n        raise ValueError(\"Model does not support probability predictions\")\n\n    if n &gt; len(data):\n        raise ValueError(\"N cannot be greater than the number of customers\")\n\n    probabilities = model.predict_proba(data)[:, 1]\n\n    results = pd.DataFrame({\n        'Client ID': data['client_id'],\n        'Probability': probabilities\n    })\n\n    results_sorted = results.sort_values(by='Probability', ascending=True).reset_index(drop=True)\n    top_n_results = results_sorted.head(n)\n\n    return top_n_results\n\n\n\n10.2 Baseline Model: Logistic Regression\n\nbaseline_feature_columns = [\n                               'age',\n                               'client_region'\n                           ] + [col for col in golden_record_df.columns if\n                                'M_' in col and ('_balance' in col or '_volume' in col)]\n\nbaseline_data_module = create_data_module(golden_record_df, baseline_feature_columns)\nfor i, col in enumerate(baseline_feature_columns):\n    print(i, col)\n\n0 age\n1 client_region\n2 M_2_volume\n3 M_3_volume\n4 M_4_volume\n5 M_5_volume\n6 M_6_volume\n7 M_7_volume\n8 M_8_volume\n9 M_9_volume\n10 M_10_volume\n11 M_11_volume\n12 M_12_volume\n13 M_13_volume\n14 M_2_balance\n15 M_3_balance\n16 M_4_balance\n17 M_5_balance\n18 M_6_balance\n19 M_7_balance\n20 M_8_balance\n21 M_9_balance\n22 M_10_balance\n23 M_11_balance\n24 M_12_balance\n25 M_13_balance\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nbaseline_trainer = (Trainer(baseline_data_module,\n                            LogisticRegression(max_iter=10000))\n                    .fit().eval_train())\n\nbaseline_visualizer = Visualizer(baseline_trainer, \"Baseline Logistic Regression\")\nbaseline_visualizer.plot_validation_metrics()\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 26 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7f3418701580&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0103, mean = 0.501, max = 0.999\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.968, mean = -0.000885, max = 0.726\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\nIn order to possibly improve the model performance, we will include more features in the training data. We will include all features except for the ones that are not relevant for the model training.\nAfter merging the transactional and non-transactional data, we have many columns that are unnecessary for model training. We will remove all columns containing card-related information, except for the has_card column. This decision stems from the fact that 50% of our dataset consists of cardholders and the other 50% consists of non-cardholders, which we matched with the cardholders. Therefore, the data in the non-target card-related columns come from the actual cardholders.\nAdditionally we will remove all columns that contain time-dependent information, such as dates and IDs, as they are not relevant for the model.\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.loc[:,\n                   ~golden_record_df.columns.str.contains(\"card\") | golden_record_df.columns.str.contains(\"has_card\")]\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} card-related columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(columns=[\"loan_granted_date\", \"birth_date\", \"account_created\"])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} time-dependent columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(\n    columns=[\"loan_account_id\", \"loan_loan_id\", \"order_account_id\", \"client_district_name\", \"disp_id\", \"account_id\",\n             \"account_district_name\"])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} ID columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(columns=[col for col in golden_record_df.columns if \"std\" in col])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} std columns. Now {len(golden_record_df.columns)} columns remain.\")\n\ncols_to_exclude_in_train = [\"client_id\", \"has_card\"]\n\nall_cols_data_module = create_data_module(golden_record_df,\n                                          golden_record_df.drop(columns=cols_to_exclude_in_train).columns)\n\nRemoved 6 card-related columns. Now 226 columns remain.\nRemoved 3 time-dependent columns. Now 223 columns remain.\nRemoved 7 ID columns. Now 216 columns remain.\nRemoved 12 std columns. Now 204 columns remain.\n\n\n\nfor col in golden_record_df.columns:\n    print(col)\n\naccount_district_id\naccount_frequency\naccount_region\naccount_inhabitants\naccount_small_municipalities\naccount_medium_municipalities\naccount_large_municipalities\naccount_huge_municipalities\naccount_cities\naccount_ratio_urban_inhabitants\naccount_average_salary\naccount_unemployment_rate_1995\naccount_unemployment_rate_1996\naccount_entrepreneurs_per_1000_inhabitants\naccount_crimes_committed_1995\naccount_crimes_committed_1996\nclient_id\ntype\nclient_district_id\nsex\nage\nclient_region\nclient_inhabitants\nclient_small_municipalities\nclient_medium_municipalities\nclient_large_municipalities\nclient_huge_municipalities\nclient_cities\nclient_ratio_urban_inhabitants\nclient_average_salary\nclient_unemployment_rate_1995\nclient_unemployment_rate_1996\nclient_entrepreneurs_per_1000_inhabitants\nclient_crimes_committed_1995\nclient_crimes_committed_1996\norder_k_symbol_debited_sum_household\norder_k_symbol_debited_sum_insurance_payment\norder_k_symbol_debited_sum_leasing\norder_k_symbol_debited_sum_loan_payment\norder_k_symbol_debited_sum_na\nloan_amount\nloan_duration\nloan_monthly_payments\nloan_status\nhas_card\nclient_tenure_years_relative\nage_group\nloan_status_simplified\nM_2_volume\nM_3_volume\nM_4_volume\nM_5_volume\nM_6_volume\nM_7_volume\nM_8_volume\nM_9_volume\nM_10_volume\nM_11_volume\nM_12_volume\nM_13_volume\nM_2_total_abs_amount\nM_3_total_abs_amount\nM_4_total_abs_amount\nM_5_total_abs_amount\nM_6_total_abs_amount\nM_7_total_abs_amount\nM_8_total_abs_amount\nM_9_total_abs_amount\nM_10_total_abs_amount\nM_11_total_abs_amount\nM_12_total_abs_amount\nM_13_total_abs_amount\nM_2_transaction_count\nM_3_transaction_count\nM_4_transaction_count\nM_5_transaction_count\nM_6_transaction_count\nM_7_transaction_count\nM_8_transaction_count\nM_9_transaction_count\nM_10_transaction_count\nM_11_transaction_count\nM_12_transaction_count\nM_13_transaction_count\nM_2_positive_transaction_count\nM_3_positive_transaction_count\nM_4_positive_transaction_count\nM_5_positive_transaction_count\nM_6_positive_transaction_count\nM_7_positive_transaction_count\nM_8_positive_transaction_count\nM_9_positive_transaction_count\nM_10_positive_transaction_count\nM_11_positive_transaction_count\nM_12_positive_transaction_count\nM_13_positive_transaction_count\nM_2_negative_transaction_count\nM_3_negative_transaction_count\nM_4_negative_transaction_count\nM_5_negative_transaction_count\nM_6_negative_transaction_count\nM_7_negative_transaction_count\nM_8_negative_transaction_count\nM_9_negative_transaction_count\nM_10_negative_transaction_count\nM_11_negative_transaction_count\nM_12_negative_transaction_count\nM_13_negative_transaction_count\nM_2_average_amount\nM_3_average_amount\nM_4_average_amount\nM_5_average_amount\nM_6_average_amount\nM_7_average_amount\nM_8_average_amount\nM_9_average_amount\nM_10_average_amount\nM_11_average_amount\nM_12_average_amount\nM_13_average_amount\nM_2_median_amount\nM_3_median_amount\nM_4_median_amount\nM_5_median_amount\nM_6_median_amount\nM_7_median_amount\nM_8_median_amount\nM_9_median_amount\nM_10_median_amount\nM_11_median_amount\nM_12_median_amount\nM_13_median_amount\nM_2_min_amount\nM_3_min_amount\nM_4_min_amount\nM_5_min_amount\nM_6_min_amount\nM_7_min_amount\nM_8_min_amount\nM_9_min_amount\nM_10_min_amount\nM_11_min_amount\nM_12_min_amount\nM_13_min_amount\nM_2_max_amount\nM_3_max_amount\nM_4_max_amount\nM_5_max_amount\nM_6_max_amount\nM_7_max_amount\nM_8_max_amount\nM_9_max_amount\nM_10_max_amount\nM_11_max_amount\nM_12_max_amount\nM_13_max_amount\nM_2_type_count\nM_3_type_count\nM_4_type_count\nM_5_type_count\nM_6_type_count\nM_7_type_count\nM_8_type_count\nM_9_type_count\nM_10_type_count\nM_11_type_count\nM_12_type_count\nM_13_type_count\nM_2_operation_count\nM_3_operation_count\nM_4_operation_count\nM_5_operation_count\nM_6_operation_count\nM_7_operation_count\nM_8_operation_count\nM_9_operation_count\nM_10_operation_count\nM_11_operation_count\nM_12_operation_count\nM_13_operation_count\nM_2_k_symbol_count\nM_3_k_symbol_count\nM_4_k_symbol_count\nM_5_k_symbol_count\nM_6_k_symbol_count\nM_7_k_symbol_count\nM_8_k_symbol_count\nM_9_k_symbol_count\nM_10_k_symbol_count\nM_11_k_symbol_count\nM_12_k_symbol_count\nM_13_k_symbol_count\nM_2_balance\nM_3_balance\nM_4_balance\nM_5_balance\nM_6_balance\nM_7_balance\nM_8_balance\nM_9_balance\nM_10_balance\nM_11_balance\nM_12_balance\nM_13_balance\n\n\n\n\n10.3 Candidate Models\n\n10.3.1 Logistic Regression\nWe will train a logistic regression model with the new feature set and evaluate its performance as it already showed promising results in the baseline model.\n\nlog_reg_trainer = (Trainer(all_cols_data_module,\n                           LogisticRegression(max_iter=10000))\n                   .fit().eval_train())\n\nlog_reg_visualizer = Visualizer(log_reg_trainer, \"Logistic Regression\")\nlog_reg_visualizer.plot_validation_metrics()\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7f3418701580&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 1.17e-05, mean = 0.527, max = 1.0\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -1.0, mean = -0.0266, max = 0.991\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Random Forest\nWe will also train a Random Forest model to see if it can outperform the logistic regression model. Random Forest models are known for their robustness and ability to capture complex relationships in the data.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier()\n\nrf_trainer = (Trainer(all_cols_data_module, rf_model)\n              .fit()\n              .eval_train())\n\nrf_visualizer = Visualizer(rf_trainer, \"Random Forest\")\nrf_visualizer.plot_validation_metrics()\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7f3418701580&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0, mean = 0.493, max = 0.98\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.94, mean = 0.0072, max = 0.72\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Decision Tree\nWe will also train a Decision Tree model to see how it performs compared to the other models. Decision Trees are known for their interpretability and simplicity.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_model = DecisionTreeClassifier(max_depth=5)\n\ndecision_tree_trainer = (Trainer(all_cols_data_module, decision_tree_model)\n                         .fit()\n                         .eval_train())\n\ndecision_tree_visualizer = Visualizer(decision_tree_trainer, \"Decision Tree\")\ndecision_tree_visualizer.plot_validation_metrics()\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7f3418701580&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0, mean = 0.529, max = 1.0\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -1.0, mean = -0.0295, max = 1.0\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost_model = GradientBoostingClassifier(\n    n_estimators=100, learning_rate=0.1\n)\n\ngradient_boost_trainer = (Trainer(all_cols_data_module, gradient_boost_model)\n                          .fit()\n                          .eval_train())\n\ngradient_boost_visualizer = Visualizer(gradient_boost_trainer, \"Gradient Boosting\")\ngradient_boost_visualizer.plot_validation_metrics()\n\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7f3418701580&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.00394, mean = 0.497, max = 0.988\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.988, mean = 0.00324, max = 0.914\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!"
  },
  {
    "objectID": "main.html#model-comparison-selection",
    "href": "main.html#model-comparison-selection",
    "title": "Data Import & Wrangling",
    "section": "11 Model Comparison & Selection",
    "text": "11 Model Comparison & Selection"
  },
  {
    "objectID": "main.html#model-optimization",
    "href": "main.html#model-optimization",
    "title": "Data Import & Wrangling",
    "section": "12 Model Optimization",
    "text": "12 Model Optimization"
  },
  {
    "objectID": "main.html#model-explanation-reduction",
    "href": "main.html#model-explanation-reduction",
    "title": "Data Import & Wrangling",
    "section": "13 Model Explanation & Reduction",
    "text": "13 Model Explanation & Reduction"
  },
  {
    "objectID": "main.html#conclusion",
    "href": "main.html#conclusion",
    "title": "Data Import & Wrangling",
    "section": "14 Conclusion",
    "text": "14 Conclusion"
  }
]
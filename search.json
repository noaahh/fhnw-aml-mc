[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "",
    "text": "import random\nfrom collections import OrderedDict\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_theme()\n# plt.style.use('seaborn-white')\n# plt.style.use('ggplot')\n\ndata_reduction = OrderedDict()\n\nSEED = 1337\n\ndef seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything(SEED)"
  },
  {
    "objectID": "main.html#helper-functions",
    "href": "main.html#helper-functions",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "1.1 Helper Functions",
    "text": "1.1 Helper Functions\n\ndef remap_values(df, column, mapping):\n    # assert that all values in the column are in the mapping except for NaN\n    assert df[column].dropna().isin(mapping.keys()).all()\n\n    df[column] = df[column].map(mapping, na_action=\"ignore\")\n    return df\n\n\ndef map_empty_to_nan(df, column):\n    if df[column].dtype != \"object\":\n        return df\n\n    df[column] = df[column].replace(r\"^\\s*$\", np.nan, regex=True)\n    return df\n\n\ndef read_csv(file_path, sep=\";\", dtypes=None):\n    df = pd.read_csv(file_path, sep=sep, dtype=dtypes)\n\n    for col in df.columns:\n        df = map_empty_to_nan(df, col)\n\n    return df\n\n\ndef plot_categorical_variables(df, categorical_columns, fill_na_value=\"NA\"):\n    \"\"\"\n    Plots count plots for categorical variables in a DataFrame, filling NA values with a specified string.\n\n    Parameters:\n    - df: pandas.DataFrame containing the data.\n    - categorical_vars: list of strings, names of the categorical variables in df to plot.\n    - fill_na_value: string, the value to use for filling NA values in the categorical variables.\n    \"\"\"\n    # Fill NA values in the specified categorical variables\n    for var in categorical_columns:\n        if df[var].isna().any():\n            df[var] = df[var].fillna(fill_na_value)\n\n    total = float(len(df))\n    fig, axes = plt.subplots(\n        nrows=len(categorical_columns), figsize=(8, len(categorical_columns) * 5)\n    )\n\n    if len(categorical_columns) == 1:  # If there's only one categorical variable, wrap axes in a list\n        axes = [axes]\n\n    for i, var in enumerate(categorical_columns):\n        ax = sns.countplot(\n            x=var, data=df, ax=axes[i], order=df[var].value_counts().index\n        )\n\n        axes[i].set_title(f\"Distribution of {var}\")\n        axes[i].set_ylabel(\"Count\")\n        axes[i].set_xlabel(var)\n        # if the number is more thatn 6 rotate the x labels\n        if len(df[var].value_counts()) &gt; 6:\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(\n                p.get_x() + p.get_width() / 2.0,\n                height + 3,\n                \"{:1.2f}%\".format((height / total) * 100),\n                ha=\"center\",\n            )\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_numerical_distributions(df, numerical_columns, kde=True, bins=30):\n    \"\"\"\n    Plots the distribution of all numerical variables in a DataFrame.\n\n    Parameters:\n    - df: pandas.DataFrame containing the data.\n    \"\"\"\n\n    # Determine the number of rows needed for subplots based on the number of numerical variables\n    nrows = len(numerical_columns)\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(8, 5 * nrows))\n\n    if nrows == 1:  # If there's only one numerical variable, wrap axes in a list\n        axes = [axes]\n\n    for i, var in enumerate(numerical_columns):\n        sns.histplot(df[var], ax=axes[i], kde=kde, bins=bins)\n        axes[i].set_title(f\"Distribution of {var}\")\n        axes[i].set_xlabel(var)\n        axes[i].set_ylabel(\"Frequency\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_date_monthly_counts(df, date_column, title):\n    \"\"\"\n    Plots the monthly counts of a date column in a DataFrame.\n\n    Parameters:\n    - df: pandas.DataFrame containing the data.\n    - date_column: string, name of the date column in df to plot.\n    - title: string, title of the plot.\n    \"\"\"\n    df[date_column] = pd.to_datetime(df[date_column])\n    df[\"month\"] = df[date_column].dt.to_period(\"M\")\n\n    monthly_counts = df[\"month\"].value_counts().sort_index()\n    monthly_counts.plot(kind=\"bar\")\n    plt.title(title)\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Count\")\n    plt.show()\n\n\ndef add_percentage_labels(ax, hue_order):\n    for p in ax.patches:\n        height = p.get_height()\n        width = p.get_width()\n        x = p.get_x()\n        y = p.get_y()\n        label_text = f\"{height:.1f}%\"\n        label_x = x + width / 2\n        label_y = y + height / 2\n        ax.text(\n            label_x,\n            label_y,\n            label_text,\n            ha=\"center\",\n            va=\"center\",\n            fontsize=9,\n            color=\"white\",\n            weight=\"bold\"\n        )"
  },
  {
    "objectID": "main.html#entities",
    "href": "main.html#entities",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "1.2 Entities",
    "text": "1.2 Entities\n\n1.2.1 Accounts\n\naccounts_df = read_csv(\"data/account.csv\")\n\n# Translated frequency from Czech to English\n# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\naccounts_df = remap_values(\n    accounts_df,\n    \"frequency\",\n    {\n        \"POPLATEK MESICNE\": \"MONTHLY_ISSUANCE\",\n        \"POPLATEK TYDNE\": \"WEEKLY_ISSUANCE\",\n        \"POPLATEK PO OBRATU\": \"ISSUANCE_AFTER_TRANSACTION\",\n    },\n)\n\naccounts_df[\"date\"] = pd.to_datetime(accounts_df[\"date\"], format=\"%y%m%d\")\n\naccounts_df.rename(\n    columns={\"date\": \"account_created\", \"frequency\": \"account_frequency\"}, inplace=True\n)\n\ndata_reduction[\"Total number of accounts\"] = len(accounts_df)\naccounts_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4500 entries, 0 to 4499\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   account_id         4500 non-null   int64         \n 1   district_id        4500 non-null   int64         \n 2   account_frequency  4500 non-null   object        \n 3   account_created    4500 non-null   datetime64[ns]\ndtypes: datetime64[ns](1), int64(2), object(1)\nmemory usage: 140.8+ KB\n\n\n\n# todo add some basic eda here\naccounts_df.head()\n\n\n\n\n\n\n\n\naccount_id\ndistrict_id\naccount_frequency\naccount_created\n\n\n\n\n0\n576\n55\nMONTHLY_ISSUANCE\n1993-01-01\n\n\n1\n3818\n74\nMONTHLY_ISSUANCE\n1993-01-01\n\n\n2\n704\n55\nMONTHLY_ISSUANCE\n1993-01-01\n\n\n3\n2378\n16\nMONTHLY_ISSUANCE\n1993-01-01\n\n\n4\n2632\n24\nMONTHLY_ISSUANCE\n1993-01-02\n\n\n\n\n\n\n\n\naccounts_df.nunique()\n\naccount_id           4500\ndistrict_id            77\naccount_frequency       3\naccount_created      1535\ndtype: int64\n\n\n\nplot_categorical_variables(accounts_df, [\"account_frequency\"])\n\n\n\n\n\n\n\n\n\nplot_numerical_distributions(accounts_df, [\"account_created\"])\n\n\n\n\n\n\n\n\n\n\n1.2.2 Clients\n\nclients_df = read_csv(\"data/client.csv\")\n\n\ndef parse_birth_number(birth_number):\n    birth_number_str = str(birth_number)\n\n    # Extract year, month, and day from birth number from string\n    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n    year = int(birth_number_str[:2])\n    month = int(birth_number_str[2:4])\n    day = int(birth_number_str[4:6])\n\n    # Determine sex based on month and adjust month for female clients\n    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n    if month &gt; 50:\n        sex = \"Female\"\n        month -= 50\n    else:\n        sex = \"Male\"\n\n    # Validate date\n    assert 1 &lt;= month &lt;= 12\n    assert 1 &lt;= day &lt;= 31\n    assert 0 &lt;= year &lt;= 99\n\n    if month in [4, 6, 9, 11]:\n        assert 1 &lt;= day &lt;= 30\n    elif month == 2:\n        assert 1 &lt;= day &lt;= 29\n    else:\n        assert 1 &lt;= day &lt;= 31\n\n    # Assuming all dates are in the 1900s\n    birth_date = datetime(1900 + year, month, day)\n    return pd.Series([sex, birth_date])\n\n\nclients_df[[\"sex\", \"birth_date\"]] = clients_df[\"birth_number\"].apply(parse_birth_number)\n\n# Calculate 'age' assuming the reference year is 1999\nclients_df[\"age\"] = clients_df[\"birth_date\"].apply(lambda x: 1999 - x.year)\n\n# Drop 'birth_number' column as it is no longer needed\nclients_df = clients_df.drop(columns=[\"birth_number\"])\n\nclients_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5369 entries, 0 to 5368\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   client_id    5369 non-null   int64         \n 1   district_id  5369 non-null   int64         \n 2   sex          5369 non-null   object        \n 3   birth_date   5369 non-null   datetime64[ns]\n 4   age          5369 non-null   int64         \ndtypes: datetime64[ns](1), int64(3), object(1)\nmemory usage: 209.9+ KB\n\n\n\n# todo add some basic eda here\nclients_df.head()\n\n\n\n\n\n\n\n\nclient_id\ndistrict_id\nsex\nbirth_date\nage\n\n\n\n\n0\n1\n18\nFemale\n1970-12-13\n29\n\n\n1\n2\n1\nMale\n1945-02-04\n54\n\n\n2\n3\n1\nFemale\n1940-10-09\n59\n\n\n3\n4\n5\nMale\n1956-12-01\n43\n\n\n4\n5\n5\nFemale\n1960-07-03\n39\n\n\n\n\n\n\n\n\nclients_df.describe()\n\n\n\n\n\n\n\n\nclient_id\ndistrict_id\nbirth_date\nage\n\n\n\n\ncount\n5369.000000\n5369.000000\n5369\n5369.000000\n\n\nmean\n3359.011920\n37.310114\n1953-09-12 09:32:21.143602176\n45.801639\n\n\nmin\n1.000000\n1.000000\n1911-08-20 00:00:00\n12.000000\n\n\n25%\n1418.000000\n14.000000\n1940-11-25 00:00:00\n31.000000\n\n\n50%\n2839.000000\n38.000000\n1954-05-06 00:00:00\n45.000000\n\n\n75%\n4257.000000\n60.000000\n1968-06-09 00:00:00\n59.000000\n\n\nmax\n13998.000000\n77.000000\n1987-09-27 00:00:00\n88.000000\n\n\nstd\n2832.911984\n25.043690\nNaN\n17.282283\n\n\n\n\n\n\n\n\nplot_numerical_distributions(clients_df, [\"birth_date\", \"age\"])\n\n\n\n\n\n\n\n\n\n\n1.2.3 Dispositions\n\ndispositions_df = read_csv(\"data/disp.csv\")\ndispositions_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5369 entries, 0 to 5368\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   disp_id     5369 non-null   int64 \n 1   client_id   5369 non-null   int64 \n 2   account_id  5369 non-null   int64 \n 3   type        5369 non-null   object\ndtypes: int64(3), object(1)\nmemory usage: 167.9+ KB\n\n\n\ndispositions_df.head()\n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\ntype\n\n\n\n\n0\n1\n1\n1\nOWNER\n\n\n1\n2\n2\n2\nOWNER\n\n\n2\n3\n3\n2\nDISPONENT\n\n\n3\n4\n4\n3\nOWNER\n\n\n4\n5\n5\n3\nDISPONENT\n\n\n\n\n\n\n\n\ndispositions_df.describe()\n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\n\n\n\n\ncount\n5369.000000\n5369.000000\n5369.000000\n\n\nmean\n3337.097970\n3359.011920\n2767.496927\n\n\nstd\n2770.418826\n2832.911984\n2307.843630\n\n\nmin\n1.000000\n1.000000\n1.000000\n\n\n25%\n1418.000000\n1418.000000\n1178.000000\n\n\n50%\n2839.000000\n2839.000000\n2349.000000\n\n\n75%\n4257.000000\n4257.000000\n3526.000000\n\n\nmax\n13690.000000\n13998.000000\n11382.000000\n\n\n\n\n\n\n\n\nplot_categorical_variables(dispositions_df, [\"type\"])\n\n\n\n\n\n\n\n\nAs the goal of this model is to address accounts and not client directly we will focus on the clients which own an account and focus solely on them.\n\ndispositions_df = dispositions_df[dispositions_df[\"type\"] == \"OWNER\"]\n\n\n\n1.2.4 Orders\n\norders_df = read_csv(\"data/order.csv\")\n\n# Translated from Czech to English\n# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\norders_df = remap_values(\n    orders_df,\n    \"k_symbol\",\n    {\n        \"POJISTNE\": \"Insurance_Payment\",\n        \"SIPO\": \"Household\",\n        \"LEASING\": \"Leasing\",\n        \"UVER\": \"Loan_Payment\",\n    },\n)\n\norders_df[\"account_to\"] = orders_df[\"account_to\"].astype(\"category\")\n\norders_df = orders_df.rename(columns={\"amount\": \"debited_amount\"})\n\norders_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6471 entries, 0 to 6470\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   order_id        6471 non-null   int64   \n 1   account_id      6471 non-null   int64   \n 2   bank_to         6471 non-null   object  \n 3   account_to      6471 non-null   category\n 4   debited_amount  6471 non-null   float64 \n 5   k_symbol        5092 non-null   object  \ndtypes: category(1), float64(1), int64(2), object(2)\nmemory usage: 573.9+ KB\n\n\n\norders_df.head()\n\n\n\n\n\n\n\n\norder_id\naccount_id\nbank_to\naccount_to\ndebited_amount\nk_symbol\n\n\n\n\n0\n29401\n1\nYZ\n87144583\n2452.0\nHousehold\n\n\n1\n29402\n2\nST\n89597016\n3372.7\nLoan_Payment\n\n\n2\n29403\n2\nQR\n13943797\n7266.0\nHousehold\n\n\n3\n29404\n3\nWX\n83084338\n1135.0\nHousehold\n\n\n4\n29405\n3\nCD\n24485939\n327.0\nNaN\n\n\n\n\n\n\n\n\norders_df.describe()\n\n\n\n\n\n\n\n\norder_id\naccount_id\ndebited_amount\n\n\n\n\ncount\n6471.000000\n6471.000000\n6471.000000\n\n\nmean\n33778.197497\n2962.302890\n3280.635698\n\n\nstd\n3737.681949\n2518.503228\n2714.475335\n\n\nmin\n29401.000000\n1.000000\n1.000000\n\n\n25%\n31187.500000\n1223.000000\n1241.500000\n\n\n50%\n32988.000000\n2433.000000\n2596.000000\n\n\n75%\n34785.500000\n3645.500000\n4613.500000\n\n\nmax\n46338.000000\n11362.000000\n14882.000000\n\n\n\n\n\n\n\n\norders_df.nunique()\n\norder_id          6471\naccount_id        3758\nbank_to             13\naccount_to        6446\ndebited_amount    4412\nk_symbol             4\ndtype: int64\n\n\nThere appear to be as many order ids as there are rows.\n\nplot_categorical_variables(orders_df, [\"k_symbol\", \"bank_to\"])\n\n/tmp/ipykernel_1839/945940023.py:33: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n\n\n\n\n\n\n\n\n\n\nplot_numerical_distributions(orders_df, [\"debited_amount\"])\n\n\n\n\n\n\n\n\n\n\n1.2.5 Transactions\n\n# column 8 is the 'bank' column which contains NaNs and must be read as string\ntransactions_df = read_csv(\"data/trans.csv\", dtypes={8: str})\n\ntransactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"], format=\"%y%m%d\")\n\n# Translated type, operations and characteristics from Czech to English\n# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\ntransactions_df = remap_values(\n    transactions_df,\n    \"type\",\n    {\n        \"VYBER\": \"Withdrawal\",  # Also withdrawal as it is against the documentation present in the dataset\n        \"PRIJEM\": \"Credit\",\n        \"VYDAJ\": \"Withdrawal\",\n    },\n)\n\ntransactions_df = remap_values(\n    transactions_df,\n    \"operation\",\n    {\n        \"VYBER KARTOU\": \"Credit Card Withdrawal\",\n        \"VKLAD\": \"Credit in Cash\",\n        \"PREVOD Z UCTU\": \"Collection from Another Bank\",\n        \"VYBER\": \"Withdrawal in Cash\",\n        \"PREVOD NA UCET\": \"Remittance to Another Bank\",\n    },\n)\n\ntransactions_df = remap_values(\n    transactions_df,\n    \"k_symbol\",\n    {\n        \"POJISTNE\": \"Insurance Payment\",\n        \"SLUZBY\": \"Payment on Statement\",\n        \"UROK\": \"Interest Credited\",\n        \"SANKC. UROK\": \"Sanction Interest\",\n        \"SIPO\": \"Household\",\n        \"DUCHOD\": \"Old-age Pension\",\n        \"UVER\": \"Loan Payment\",\n    },\n)\n\n# Set the amount to negative for withdrawals and positive for credits\ntransactions_df[\"amount\"] = np.where(\n    transactions_df[\"type\"] == \"Credit\",\n    transactions_df[\"amount\"],\n    -transactions_df[\"amount\"],\n)\n\ntransactions_df.rename(columns={\"type\": \"transaction_type\"}, inplace=True)\n\ntransactions_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1056320 entries, 0 to 1056319\nData columns (total 10 columns):\n #   Column            Non-Null Count    Dtype         \n---  ------            --------------    -----         \n 0   trans_id          1056320 non-null  int64         \n 1   account_id        1056320 non-null  int64         \n 2   date              1056320 non-null  datetime64[ns]\n 3   transaction_type  1056320 non-null  object        \n 4   operation         873206 non-null   object        \n 5   amount            1056320 non-null  float64       \n 6   balance           1056320 non-null  float64       \n 7   k_symbol          521006 non-null   object        \n 8   bank              273508 non-null   object        \n 9   account           295389 non-null   float64       \ndtypes: datetime64[ns](1), float64(3), int64(2), object(4)\nmemory usage: 80.6+ MB\n\n\n\ntransactions_df.head()\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntransaction_type\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\n\n\n0\n695247\n2378\n1993-01-01\nCredit\nCredit in Cash\n700.0\n700.0\nNaN\nNaN\nNaN\n\n\n1\n171812\n576\n1993-01-01\nCredit\nCredit in Cash\n900.0\n900.0\nNaN\nNaN\nNaN\n\n\n2\n207264\n704\n1993-01-01\nCredit\nCredit in Cash\n1000.0\n1000.0\nNaN\nNaN\nNaN\n\n\n3\n1117247\n3818\n1993-01-01\nCredit\nCredit in Cash\n600.0\n600.0\nNaN\nNaN\nNaN\n\n\n4\n579373\n1972\n1993-01-02\nCredit\nCredit in Cash\n400.0\n400.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ntransactions_df.describe()\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\namount\nbalance\naccount\n\n\n\n\ncount\n1.056320e+06\n1.056320e+06\n1056320\n1.056320e+06\n1.056320e+06\n2.953890e+05\n\n\nmean\n1.335311e+06\n2.936867e+03\n1997-01-04 07:29:27.037261952\n1.866397e+02\n3.851833e+04\n4.567092e+07\n\n\nmin\n1.000000e+00\n1.000000e+00\n1993-01-01 00:00:00\n-8.740000e+04\n-4.112570e+04\n0.000000e+00\n\n\n25%\n4.302628e+05\n1.204000e+03\n1996-01-16 00:00:00\n-3.019000e+03\n2.240250e+04\n1.782858e+07\n\n\n50%\n8.585065e+05\n2.434000e+03\n1997-04-10 00:00:00\n-1.460000e+01\n3.314340e+04\n4.575095e+07\n\n\n75%\n2.060979e+06\n3.660000e+03\n1998-02-28 00:00:00\n2.000000e+02\n4.960362e+04\n7.201341e+07\n\n\nmax\n3.682987e+06\n1.138200e+04\n1998-12-31 00:00:00\n7.481200e+04\n2.096370e+05\n9.999420e+07\n\n\nstd\n1.227487e+06\n2.477345e+03\nNaN\n1.121353e+04\n2.211787e+04\n3.066340e+07\n\n\n\n\n\n\n\n\nplot_categorical_variables(\n    transactions_df, [\"transaction_type\", \"operation\", \"k_symbol\"]\n)\n\n/tmp/ipykernel_1839/945940023.py:33: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n\n\n\n\n\n\n\n\n\n\nplot_numerical_distributions(transactions_df, [\"date\", \"amount\", \"balance\"])\n\n\n\n\n\n\n\n\nLooking at the distributions of the transaction table we can see that the count of transactions per year increase over time. So we can conclude that the bank has a growing client base.\nHowever, the other plots are not very useful. For one the transaction amount seems to be very sparse, ranging from values between -80000 and 80000.\nThe balance distribution also showcases that there are accounts with a negative balance after a transaction, which would only make sense if debt is also included in this value.\nAccording to description of the field balance: “balance after transaction”\n\n1.2.5.1 Transaction Amounts and Counts by Month\n\n# Getting a list of unique years from the dataset\ntransactions_df[\"year\"] = transactions_df[\"date\"].dt.year\ntransactions_df[\"month\"] = transactions_df[\"date\"].dt.month\n\nmonths = [\n    \"Jan\",\n    \"Feb\",\n    \"Mar\",\n    \"Apr\",\n    \"May\",\n    \"Jun\",\n    \"Jul\",\n    \"Aug\",\n    \"Sep\",\n    \"Oct\",\n    \"Nov\",\n    \"Dec\",\n]\nyears = sorted(transactions_df[\"year\"].unique())\n\nfig, axs = plt.subplots(\n    len(years) * 2,\n    1,\n    figsize=(8, 6 * len(years)),\n    sharex=True,\n    gridspec_kw={\"height_ratios\": [3, 1] * len(years)},\n)\n\nfor i, year in enumerate(years):\n    # Filter transactions for the current year\n    yearly_transactions = transactions_df[transactions_df[\"year\"] == year]\n\n    # Preparing data for the box plot: a list of amounts for each month for the current year\n    amounts_per_month_yearly = [\n        yearly_transactions[yearly_transactions[\"month\"] == month][\"amount\"]\n        for month in range(1, 13)\n    ]\n\n    # Preparing data for the bar chart for the current year\n    monthly_summary_yearly = (\n        yearly_transactions.groupby(\"month\")\n        .agg(TotalAmount=(\"amount\", \"sum\"), TransactionCount=(\"amount\", \"count\"))\n        .reset_index()\n    )\n\n    # Box plot for transaction amounts by month for the current year\n    axs[i * 2].boxplot(amounts_per_month_yearly, patch_artist=True)\n    # now with seaborn\n    # sns.boxplot(data=yearly_transactions, x='month', y='amount', ax=axs[i*2])\n    axs[i * 2].set_title(f\"Transaction Amounts Per Month in {year} (Box Plot)\")\n    axs[i * 2].set_yscale(\"symlog\")\n    axs[i * 2].set_ylabel(\"Transaction Amounts (log scale)\")\n    axs[i * 2].grid(True, which=\"both\")\n\n    # Bar chart for transaction count by month for the current year\n    axs[i * 2 + 1].bar(\n        monthly_summary_yearly[\"month\"],\n        monthly_summary_yearly[\"TransactionCount\"],\n        color=\"tab:red\",\n        alpha=0.6,\n    )\n    axs[i * 2 + 1].set_ylabel(\"Transaction Count\")\n    axs[i * 2 + 1].grid(True, which=\"both\")\n\n# Setting x-ticks and labels for the last bar chart (shared x-axis for all)\naxs[-1].set_xticks(range(1, 13))\naxs[-1].set_xticklabels(months)\naxs[-1].set_xlabel(\"Month\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(\n    2,\n    len(years),\n    figsize=(8 * len(years) / 2, 7),\n    sharey=\"row\",\n    gridspec_kw={\"height_ratios\": [3, 1]},\n)\n\nfor i, year in enumerate(years):\n    # Filter transactions for the current year\n    yearly_transactions = transactions_df[transactions_df[\"year\"] == year]\n\n    # Preparing data for the box plot: a list of amounts for each month for the current year\n    amounts_per_month_yearly = [\n        yearly_transactions[yearly_transactions[\"month\"] == month][\"amount\"]\n        for month in range(1, 13)\n    ]\n\n    # Preparing data for the bar chart for the current year\n    monthly_summary_yearly = (\n        yearly_transactions.groupby(\"month\")\n        .agg(TotalAmount=(\"amount\", \"sum\"), TransactionCount=(\"amount\", \"count\"))\n        .reset_index()\n    )\n\n    # Selecting the appropriate axes for multiple or single year scenarios\n    ax_box = axs[0, i] if len(years) &gt; 1 else axs[0]\n    ax_bar = axs[1, i] if len(years) &gt; 1 else axs[1]\n\n    ax_box.boxplot(amounts_per_month_yearly, patch_artist=True)\n    ax_box.set_title(f\"{year} (Box Plot)\")\n    ax_box.set_yscale(\"symlog\")\n    ax_box.set_ylabel(\"Transaction Amounts (log scale)\")\n    ax_box.grid(True, which=\"both\")\n\n    ax_bar.bar(\n        monthly_summary_yearly[\"month\"],\n        monthly_summary_yearly[\"TransactionCount\"],\n        color=\"tab:red\",\n        alpha=0.6,\n    )\n    ax_bar.set_ylabel(\"Transaction Count\")\n    ax_bar.grid(True, which=\"both\")\n\n    # Setting common x-ticks and labels for all axes\n    ax_bar.set_xticks(range(1, 13))\n    ax_bar.set_xticklabels(months)\n\nfig.text(0.5, 0.04, \"Month\", ha=\"center\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.2.5.2 Negative Balances\n\nnegative_balances = transactions_df[transactions_df[\"balance\"] &lt; 0]\nplot_numerical_distributions(negative_balances, [\"balance\", \"amount\"])\nprint(f\"Number of transactions with negative balance: {len(negative_balances)}\")\n\n\n\n\n\n\n\n\nNumber of transactions with negative balance: 2999\n\n\nThere appear to be 2999 transactions which have a negative balance, therefore after the transaction the account balance was negative. This implies that these accounts are in some kind of debt.\n\n\n\n1.2.6 Loans\n\nloans_df = read_csv(\"data/loan.csv\")\n\nloans_df[\"date\"] = pd.to_datetime(loans_df[\"date\"], format=\"%y%m%d\")\n\nloans_df[\"status\"] = loans_df[\"status\"].map(\n    {\n        \"A\": \"Contract finished, no problems\",\n        \"B\": \"Contract finished, loan not paid\",\n        \"C\": \"Contract running, OK thus-far\",\n        \"D\": \"Contract running, client in debt\",\n    }\n)\n\nloans_df.rename(\n    columns={\n        \"date\": \"granted_date\",\n        \"amount\": \"amount\",\n        \"duration\": \"duration\",\n        \"payments\": \"monthly_payments\",\n        \"status\": \"status\",\n    },\n    inplace=True,\n)\n\nloans_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 682 entries, 0 to 681\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   loan_id           682 non-null    int64         \n 1   account_id        682 non-null    int64         \n 2   granted_date      682 non-null    datetime64[ns]\n 3   amount            682 non-null    int64         \n 4   duration          682 non-null    int64         \n 5   monthly_payments  682 non-null    float64       \n 6   status            682 non-null    object        \ndtypes: datetime64[ns](1), float64(1), int64(4), object(1)\nmemory usage: 37.4+ KB\n\n\n\n# todo add some basic eda here\nloans_df.head()\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ngranted_date\namount\nduration\nmonthly_payments\nstatus\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nContract finished, loan not paid\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nContract finished, no problems\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nContract finished, no problems\n\n\n3\n5325\n1843\n1993-08-03\n105804\n36\n2939.0\nContract finished, no problems\n\n\n4\n7240\n11013\n1993-09-06\n274740\n60\n4579.0\nContract finished, no problems\n\n\n\n\n\n\n\n\nloans_df.describe()\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ngranted_date\namount\nduration\nmonthly_payments\n\n\n\n\ncount\n682.000000\n682.000000\n682\n682.000000\n682.000000\n682.000000\n\n\nmean\n6172.466276\n5824.162757\n1996-09-29 05:35:43.108504448\n151410.175953\n36.492669\n4190.664223\n\n\nmin\n4959.000000\n2.000000\n1993-07-05 00:00:00\n4980.000000\n12.000000\n304.000000\n\n\n25%\n5577.500000\n2967.000000\n1995-07-04 12:00:00\n66732.000000\n24.000000\n2477.000000\n\n\n50%\n6176.500000\n5738.500000\n1997-02-06 12:00:00\n116928.000000\n36.000000\n3934.000000\n\n\n75%\n6752.500000\n8686.000000\n1997-12-12 12:00:00\n210654.000000\n48.000000\n5813.500000\n\n\nmax\n7308.000000\n11362.000000\n1998-12-08 00:00:00\n590820.000000\n60.000000\n9910.000000\n\n\nstd\n682.579279\n3283.512681\nNaN\n113372.406310\n17.075219\n2215.830344\n\n\n\n\n\n\n\n\nloans_df.nunique()\n\nloan_id             682\naccount_id          682\ngranted_date        559\namount              645\nduration              5\nmonthly_payments    577\nstatus                4\ndtype: int64\n\n\nIt seems as if one account can have at max one loan.\n\nplot_categorical_variables(loans_df, [\"duration\", \"status\"])\n\n\n\n\n\n\n\n\nThe distribution of durations seems to be even.\n\nplot_numerical_distributions(loans_df, [\"granted_date\"])\n\n\n\n\n\n\n\n\n\n\n1.2.7 Credit Cards\n\ncards_df = read_csv(\"data/card.csv\")\n\ncards_df[\"issued\"] = pd.to_datetime(\n    cards_df[\"issued\"], format=\"%y%m%d %H:%M:%S\"\n).dt.date\n\ncards_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 892 entries, 0 to 891\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   card_id  892 non-null    int64 \n 1   disp_id  892 non-null    int64 \n 2   type     892 non-null    object\n 3   issued   892 non-null    object\ndtypes: int64(2), object(2)\nmemory usage: 28.0+ KB\n\n\n\ncards_df.head()\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype\nissued\n\n\n\n\n0\n1005\n9285\nclassic\n1993-11-07\n\n\n1\n104\n588\nclassic\n1994-01-19\n\n\n2\n747\n4915\nclassic\n1994-02-05\n\n\n3\n70\n439\nclassic\n1994-02-08\n\n\n4\n577\n3687\nclassic\n1994-02-15\n\n\n\n\n\n\n\n\ncards_df.describe()\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\n\n\n\n\ncount\n892.000000\n892.000000\n\n\nmean\n480.855381\n3511.862108\n\n\nstd\n306.933982\n2984.373626\n\n\nmin\n1.000000\n9.000000\n\n\n25%\n229.750000\n1387.000000\n\n\n50%\n456.500000\n2938.500000\n\n\n75%\n684.250000\n4459.500000\n\n\nmax\n1247.000000\n13660.000000\n\n\n\n\n\n\n\n\nplot_categorical_variables(cards_df, [\"type\"])\n\n\n\n\n\n\n\n\n\nplot_numerical_distributions(cards_df, [\"issued\"])\n\n\n\n\n\n\n\n\n\n\n1.2.8 Demographic data\n\ndistricts_df = read_csv(\"data/district.csv\")\n\n# Rename columns\n# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\ndistricts_df.rename(\n    columns={\n        \"A1\": \"district_id\",\n        \"A2\": \"district_name\",\n        \"A3\": \"region\",\n        \"A4\": \"inhabitants\",\n        \"A5\": \"small_municipalities\",\n        \"A6\": \"medium_municipalities\",\n        \"A7\": \"large_municipalities\",\n        \"A8\": \"huge_municipalities\",\n        \"A9\": \"cities\",\n        \"A10\": \"ratio_urban_inhabitants\",\n        \"A11\": \"average_salary\",\n        \"A12\": \"unemployment_rate_1995\",\n        \"A13\": \"unemployment_rate_1996\",\n        \"A14\": \"entrepreneurs_per_1000_inhabitants\",\n        \"A15\": \"crimes_committed_1995\",\n        \"A16\": \"crimes_committed_1996\",\n    },\n    inplace=True,\n)\n\nfor col in [\n    \"unemployment_rate_1995\",\n    \"unemployment_rate_1996\",\n    \"crimes_committed_1995\",\n    \"crimes_committed_1996\",\n]:\n    districts_df[col] = pd.to_numeric(districts_df[col], errors=\"coerce\")\n\ndistricts_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 77 entries, 0 to 76\nData columns (total 16 columns):\n #   Column                              Non-Null Count  Dtype  \n---  ------                              --------------  -----  \n 0   district_id                         77 non-null     int64  \n 1   district_name                       77 non-null     object \n 2   region                              77 non-null     object \n 3   inhabitants                         77 non-null     int64  \n 4   small_municipalities                77 non-null     int64  \n 5   medium_municipalities               77 non-null     int64  \n 6   large_municipalities                77 non-null     int64  \n 7   huge_municipalities                 77 non-null     int64  \n 8   cities                              77 non-null     int64  \n 9   ratio_urban_inhabitants             77 non-null     float64\n 10  average_salary                      77 non-null     int64  \n 11  unemployment_rate_1995              76 non-null     float64\n 12  unemployment_rate_1996              77 non-null     float64\n 13  entrepreneurs_per_1000_inhabitants  77 non-null     int64  \n 14  crimes_committed_1995               76 non-null     float64\n 15  crimes_committed_1996               77 non-null     int64  \ndtypes: float64(4), int64(10), object(2)\nmemory usage: 9.8+ KB\n\n\nIt appears as if there is 1 null value for unemployment rate in 1995 and crimes committed in 1995.\n\n# todo add some basic eda here\ndistricts_df.head()\n\n\n\n\n\n\n\n\ndistrict_id\ndistrict_name\nregion\ninhabitants\nsmall_municipalities\nmedium_municipalities\nlarge_municipalities\nhuge_municipalities\ncities\nratio_urban_inhabitants\naverage_salary\nunemployment_rate_1995\nunemployment_rate_1996\nentrepreneurs_per_1000_inhabitants\ncrimes_committed_1995\ncrimes_committed_1996\n\n\n\n\n0\n1\nHl.m. Praha\nPrague\n1204953\n0\n0\n0\n1\n1\n100.0\n12541\n0.29\n0.43\n167\n85677.0\n99107\n\n\n1\n2\nBenesov\ncentral Bohemia\n88884\n80\n26\n6\n2\n5\n46.7\n8507\n1.67\n1.85\n132\n2159.0\n2674\n\n\n2\n3\nBeroun\ncentral Bohemia\n75232\n55\n26\n4\n1\n5\n41.7\n8980\n1.95\n2.21\n111\n2824.0\n2813\n\n\n3\n4\nKladno\ncentral Bohemia\n149893\n63\n29\n6\n2\n6\n67.4\n9753\n4.64\n5.05\n109\n5244.0\n5892\n\n\n4\n5\nKolin\ncentral Bohemia\n95616\n65\n30\n4\n1\n6\n51.4\n9307\n3.85\n4.43\n118\n2616.0\n3040\n\n\n\n\n\n\n\n\ndistricts_df.describe()\n\n\n\n\n\n\n\n\ndistrict_id\ninhabitants\nsmall_municipalities\nmedium_municipalities\nlarge_municipalities\nhuge_municipalities\ncities\nratio_urban_inhabitants\naverage_salary\nunemployment_rate_1995\nunemployment_rate_1996\nentrepreneurs_per_1000_inhabitants\ncrimes_committed_1995\ncrimes_committed_1996\n\n\n\n\ncount\n77.000000\n7.700000e+01\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n76.000000\n77.000000\n77.000000\n76.000000\n77.000000\n\n\nmean\n39.000000\n1.338849e+05\n48.623377\n24.324675\n6.272727\n1.727273\n6.259740\n63.035065\n9031.675325\n3.119342\n3.787013\n116.129870\n4850.315789\n5030.831169\n\n\nstd\n22.371857\n1.369135e+05\n32.741829\n12.780991\n4.015222\n1.008338\n2.435497\n16.221727\n790.202347\n1.665568\n1.908480\n16.608773\n9888.951933\n11270.796786\n\n\nmin\n1.000000\n4.282100e+04\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n33.900000\n8110.000000\n0.290000\n0.430000\n81.000000\n818.000000\n888.000000\n\n\n25%\n20.000000\n8.585200e+04\n22.000000\n16.000000\n4.000000\n1.000000\n5.000000\n51.900000\n8512.000000\n1.787500\n2.310000\n105.000000\n2029.750000\n2122.000000\n\n\n50%\n39.000000\n1.088710e+05\n49.000000\n25.000000\n6.000000\n2.000000\n6.000000\n59.800000\n8814.000000\n2.825000\n3.600000\n113.000000\n2932.000000\n3040.000000\n\n\n75%\n58.000000\n1.390120e+05\n71.000000\n32.000000\n8.000000\n2.000000\n8.000000\n73.500000\n9317.000000\n3.890000\n4.790000\n126.000000\n4525.500000\n4595.000000\n\n\nmax\n77.000000\n1.204953e+06\n151.000000\n70.000000\n20.000000\n5.000000\n11.000000\n100.000000\n12541.000000\n7.340000\n9.400000\n167.000000\n85677.000000\n99107.000000\n\n\n\n\n\n\n\n\ndistricts_df.nunique()\n\ndistrict_id                           77\ndistrict_name                         77\nregion                                 8\ninhabitants                           77\nsmall_municipalities                  53\nmedium_municipalities                 36\nlarge_municipalities                  17\nhuge_municipalities                    6\ncities                                11\nratio_urban_inhabitants               70\naverage_salary                        76\nunemployment_rate_1995                70\nunemployment_rate_1996                73\nentrepreneurs_per_1000_inhabitants    44\ncrimes_committed_1995                 75\ncrimes_committed_1996                 76\ndtype: int64\n\n\n\nplot_numerical_distributions(districts_df, [\"crimes_committed_1995\"])\n\n\n\n\n\n\n\n\n\nplot_categorical_variables(districts_df, [\"region\"])\n\n/tmp/ipykernel_1839/945940023.py:33: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n\n\n\n\n\n\n\n\n\nWe need to differentiate between the domicile of the client and account, as they can be different."
  },
  {
    "objectID": "main.html#data-relationships",
    "href": "main.html#data-relationships",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "1.3 Data Relationships",
    "text": "1.3 Data Relationships\nFollowing the documentation of the dataset, there are multiple relationships that need to be validated. https://sorry.vse.cz/~berka/challenge/PAST/index.html\nThe ERD according to the descriptions on https://sorry.vse.cz/~berka/challenge/PAST/index.html\n\nThis ERD shows how the data appears in the dataset:\n\nIn order to also validate the relationships from a algorithmic perspective, we can use the following code:\n\n# Verify 1:1 relationships between CLIENT, LOAN and DISPOSITION\nassert dispositions_df[\n    \"client_id\"\n].is_unique, \"Each client_id should appear exactly once in the DISPOSITION DataFrame.\"\nassert loans_df[\n    \"account_id\"\n].is_unique, \"Each account_id should appear exactly once in the LOAN DataFrame.\"\n\n# Verify 1:M relationships between ACCOUNT and DISPOSITION\n# assert dispositions['account_id'].is_unique == False, \"An account_id should appear more than once in the DISPOSITION DataFrame.\"\nassert (\n    dispositions_df[\"account_id\"].is_unique == True\n), \"An account_id should appear once in the DISPOSITION DataFrame.\"\n# TODO check if in accordance to decision to remove disponents from dispositions\n\n# Verify each district_id in ACCOUNT and CLIENT exists in DISTRICT\nassert set(accounts_df[\"district_id\"]).issubset(\n    set(districts_df[\"district_id\"])\n), \"All district_ids in ACCOUNT should exist in DISTRICT.\"\nassert set(clients_df[\"district_id\"]).issubset(\n    set(districts_df[\"district_id\"])\n), \"All district_ids in CLIENT should exist in DISTRICT.\"\n\n# Verify each account_id in DISPOSITION, ORDER, TRANSACTION, and LOAN exists in ACCOUNT\nassert set(dispositions_df[\"account_id\"]).issubset(\n    set(accounts_df[\"account_id\"])\n), \"All account_ids in DISPOSITION should exist in ACCOUNT.\"\nassert set(orders_df[\"account_id\"]).issubset(\n    set(accounts_df[\"account_id\"])\n), \"All account_ids in ORDER should exist in ACCOUNT.\"\nassert set(transactions_df[\"account_id\"]).issubset(\n    set(accounts_df[\"account_id\"])\n), \"All account_ids in TRANSACTION should exist in ACCOUNT.\"\nassert set(loans_df[\"account_id\"]).issubset(\n    set(accounts_df[\"account_id\"])\n), \"All account_ids in LOAN should exist in ACCOUNT.\"\n\n# Verify each client_id in DISPOSITION exists in CLIENT\nassert set(dispositions_df[\"client_id\"]).issubset(\n    set(clients_df[\"client_id\"])\n), \"All client_ids in DISPOSITION should exist in CLIENT.\"\n\n# Verify each disp_id in CARD exists in DISPOSITION\nassert set(cards_df[\"disp_id\"]).issubset(\n    set(dispositions_df[\"disp_id\"])\n), \"All disp_ids in CARD should exist in DISPOSITION.\""
  },
  {
    "objectID": "main.html#non-transactional-data",
    "href": "main.html#non-transactional-data",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "3.1 Non-transactional Data",
    "text": "3.1 Non-transactional Data\n\n3.1.1 Card Holders\n\nplt.figure()\nplt.title(\"Number of Clients by Card Type\")\nsns.barplot(\n    x=[\"No Card\", \"Classic/Gold Card Holders\", \"Junior Card Holders\"],\n    y=[\n        non_transactional_df[\"card_type\"].isna().sum(),\n        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n    ],\n)\n# ensure that the number of clients is shown on the bars\nfor i, v in enumerate(\n    [\n        non_transactional_df[\"card_type\"].isna().sum(),\n        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n    ]\n):\n    plt.text(i, v + 10, str(v), ha=\"center\", va=\"bottom\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the distribution of card holders in general we can see that the most clients are not in a possession of a credit card.\n\nplt.figure()\nplt.title(\n    f'Distribution of Age for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\"age\"],\n    kde=True,\n    bins=30,\n)\nplt.xlabel(\"Age of Client (presumably in 1999)\")\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the age distribution of Junior Card holders paints a picture on this group, however only looking at the current age may be misleading as we need to understand how old they were when the card was issued to determine if they could have been eligble for a Classic/Gold card (at least 18 when the card was issued).\n\nnon_transactional_df[\"card_issued\"] = pd.to_datetime(\n    non_transactional_df[\"card_issued\"]\n)\n\nnon_transactional_df[\"age_at_card_issuance\"] = (\n    non_transactional_df[\"card_issued\"] - non_transactional_df[\"birth_date\"]\n)\nnon_transactional_df[\"age_at_card_issuance\"] = (\n    non_transactional_df[\"age_at_card_issuance\"].dt.days // 365\n)\n\nplt.figure()\nplt.title(\n    f'Distribution of Age at Card Issuance for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=30,\n)\nplt.xlabel(\"Age at Card Issuance\")\nplt.show()\n\n\n\n\n\n\n\n\nHere we can see that roughly 1/3 of the Junior Card holders were not of legal age (assuming legal age is 18) when receiving their Junior Card.\n\nplt.figure()\nplt.title(\n    f\"Distribution of Age at Card Issuance for All Card Types\\n total count = {len(non_transactional_df)}\"\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=10,\n    color=\"blue\",\n    label=\"Junior Card Holders\",\n)\nsns.histplot(\n    non_transactional_df[non_transactional_df[\"card_type\"] != \"junior\"][\n        \"age_at_card_issuance\"\n    ],\n    kde=True,\n    bins=30,\n    color=\"red\",\n    label=\"Non-Junior Card Holders\",\n)\nplt.legend()\nplt.xlabel(\"Age at Card Issuance\")\nplt.show()\n\n\n\n\n\n\n\n\nComparing the age at issue date between Junior and non-Junior (Classic/Gold) card holders shows that there is no overlap between the two groups, which makes intutively sense.\nTherefore removing the subset of Junior Cards seems as valid as there is no reason to believe that there are Junior Cards issued wrongly, the subset being relatively small compared to the remaining issued cards and the fact that our target is specifically Classic/Gold Card owners.\n\nbefore_len = len(non_transactional_df)\nnon_transactional_df = non_transactional_df[\n    non_transactional_df[\"card_type\"] != \"junior\"\n]\ndata_reduction[\"Junior Card Holders\"] = -(before_len - len(non_transactional_df))\ndel before_len\n\nLooking at the age distribution of Junior card holders and their occurence in comparison it seems valid to remove them as they are not the target group and make up a small subset of the complete dataset.\n\n\n3.1.2 Time factors on Card Status\nThe time between creating an account and issuing a card may also be important when filtering customers based on their history. We should avoid filtering out potentially interesting periods and understand how the timespans between account creation and card issuance are distributed.\n\nnon_transactional_w_cards_df = non_transactional_df[\n    non_transactional_df[\"card_issued\"].notna()\n    & non_transactional_df[\"account_created\"].notna()\n]\nnon_transactional_w_cards_df[\"duration_days\"] = (\n    non_transactional_w_cards_df[\"card_issued\"]\n    - non_transactional_w_cards_df[\"account_created\"]\n).dt.days\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    non_transactional_w_cards_df[\"duration_days\"], bins=50, edgecolor=\"black\", kde=True\n)\nplt.title(\"Distribution of Duration Between Account Creation and Card Issuance\")\nplt.xlabel(\"Duration in Days\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1839/17211290.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nThe histogram displays a distribution with multiple peaks, indicating that there are several typical time frames for card issuance after account creation. The highest peak occurs within the first 250 days, suggesting that a significant number of cards are issued during this period. The frequency decreases as duration increases, with noticeable peaks that may correspond to specific processing batch cycles or policy changes over time. The distribution also has a long tail, suggesting that in some cases, card issuance can take a very long time.\nAnalyzing the length of time a client has been with the bank in relation to their account creation date and card ownership can provide valuable insights for a bank’s customer relationship management and product targeting strategies. Long-standing clients may exhibit different banking behaviors, such as product adoption and loyalty patterns, compared to newer clients.\n\nmax_account_creation_date = non_transactional_df[\"card_issued\"].max()\n\nnon_transactional_df[\"client_tenure_years_relative\"] = (\n    max_account_creation_date - non_transactional_df[\"account_created\"]\n).dt.days / 365.25\n\nplt.figure()\nax = sns.histplot(\n    data=non_transactional_df,\n    x=\"client_tenure_years_relative\",\n    hue=\"has_card\",\n    multiple=\"stack\",\n    binwidth=1,\n    stat=\"percent\",\n)\n\n# Call the function to add labels\nadd_percentage_labels(ax, non_transactional_df[\"has_card\"].unique())\n\n# Additional plot formatting\nplt.title(\"Client Tenure Relative to Latest Card Issued Date and Card Ownership\")\nplt.xlabel(\"Client Tenure (Years, Relative to Latest Card Issuance)\")\nplt.ylabel(\"Percentage of Clients\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart shows the tenure of clients in years, categorized by whether they own a credit card (True) or not (False). Each bar represents the percentage of clients within a specific tenure range, allowing for comparison of the distribution of card ownership among clients with different lengths of association with the bank.\n\n\n3.1.3 Demographics\nUsing the available demographic data, we can investigate the potential correlation between demographic data and card status. The average salary may indicate a difference between cardholders and non-cardholders, as it is reasonable to assume that cardholders have a higher average salary than non-cardholders.\n\nplt.figure()\nsns.boxplot(x=\"has_card\", y=\"client_average_salary\", data=non_transactional_df)\nplt.title(\"Average Salary in Client's Region by Card Ownership\")\nplt.xlabel(\"Has Card\")\nplt.ylabel(\"Average Salary\")\nplt.xticks([0, 1], [\"No Card Owner\", \"Card Owner\"])\n\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot compares the average salaries of clients who own a credit card with those who do not. Both groups have a substantial overlap in salary ranges, suggesting that while there might be a trend for card owners to have higher salaries, the difference is not significant. The median salary for card owners is slightly higher than that for non-card owners, as indicated by the median line within the respective boxes.\nBoth distributions have outliers on the higher end, indicating that some individuals have salaries significantly above the average in both groups. However, these outliers do not dominate the general trend.\nIt should also be noted that this plot assumes that the average salary of the region’s clients remained constant over the years, which is unlikely to be true.\nThe group of bar charts represents the distribution of credit card ownership across various demographics, showing the percentage of clients with and without cards within different age groups, sexes, and regions.\n\nnon_transactional_df[\"age_group\"] = pd.cut(\n    non_transactional_df[\"age\"],\n    bins=[0, 25, 40, 55, 70, 100],\n    labels=[\"&lt;25\", \"25-40\", \"40-55\", \"55-70\", \"&gt;70\"],\n)\n\nplt.figure(figsize=(8, 12))\n\n# Age Group\nplt.subplot(3, 1, 1)\nage_group_counts = (\n    non_transactional_df.groupby([\"age_group\", \"has_card\"]).size().unstack(fill_value=0)\n)\nage_group_percentages = (age_group_counts.T / age_group_counts.sum(axis=1)).T * 100\nage_group_plot = age_group_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nage_group_plot.set_title(\"Card Ownership by Age Group\")\nage_group_plot.set_ylabel(\"Percentage\")\nadd_percentage_labels(age_group_plot, non_transactional_df[\"has_card\"].unique())\n\n# Sex\nplt.subplot(3, 1, 2)\nsex_counts = (\n    non_transactional_df.groupby([\"sex\", \"has_card\"]).size().unstack(fill_value=0)\n)\nsex_percentages = (sex_counts.T / sex_counts.sum(axis=1)).T * 100\nsex_plot = sex_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nsex_plot.set_title(\"Card Ownership by Sex\")\nsex_plot.set_ylabel(\"Percentage\")\nadd_percentage_labels(sex_plot, non_transactional_df[\"has_card\"].unique())\n\n# Client Region\nplt.subplot(3, 1, 3)\nregion_counts = (\n    non_transactional_df.groupby([\"client_region\", \"has_card\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_percentages = (region_counts.T / region_counts.sum(axis=1)).T * 100\nregion_plot = region_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\nregion_plot.set_title(\"Card Ownership by Client Region\")\nregion_plot.set_ylabel(\"Percentage\")\nregion_plot.tick_params(axis=\"x\", rotation=45)\nadd_percentage_labels(region_plot, non_transactional_df[\"has_card\"].unique())\n\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1839/271218705.py:12: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nCard Ownership by Age Group: The bar chart displays the proportion of cardholders in different age groups. The percentage of cardholders is lowest in the age group of over 70, followed by the age group of 55-70, indicating that card ownership is more prevalent among younger demographics.\nCard Ownership by Sex: The bar chart shows the breakdown of card ownership by sex. The data reveals that the percentage of cardholders is comparable between both sexes, and no significant difference is present.\nCard Ownership by Region The bar chart at the bottom illustrates card ownership across different regions, showing a relatively consistent pattern among most regions.\n\n\n3.1.4 Impact of Loans / Debt\n\nsimplified_loan_status_mapping = {\n    \"Contract finished, no problems\": \"Finished\",\n    \"Contract finished, loan not paid\": \"Not Paid\",\n    \"Contract running, OK thus-far\": \"Running\",\n    \"Contract running, client in debt\": \"In Debt\",\n    \"No Loan\": \"No Loan\",\n}\n\nnon_transactional_df[\"loan_status_simplified\"] = non_transactional_df[\n    \"loan_status\"\n].map(simplified_loan_status_mapping)\n\n# this variable wants to kill itself\nloan_status_simplified_card_ownership_counts = (\n    non_transactional_df.groupby([\"loan_status_simplified\", \"has_card\"])\n    .size()\n    .unstack(fill_value=0)\n)\nloan_status_simplified_card_ownership_percentages = (\n    loan_status_simplified_card_ownership_counts.T\n    / loan_status_simplified_card_ownership_counts.sum(axis=1)\n).T * 100\n\nloan_status_simplified_card_ownership_percentages.plot(\n    kind=\"bar\", stacked=True, figsize=(8, 6)\n)\nplt.title(\"Interaction Between Simplified Loan Status and Card Ownership\")\nplt.xlabel(\"Simplified Loan Status\")\nplt.ylabel(\"Percentage of Clients\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Has Card\", labels=[\"No Card\", \"Has Card\"])\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "main.html#transactional-data",
    "href": "main.html#transactional-data",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "3.2 Transactional Data",
    "text": "3.2 Transactional Data\nTODO: Add more EDA for transactional data\n\nzero_amount_transactions_df = transactions_df[transactions_df[\"amount\"] == 0]\n\nzero_amount_transactions_info = {\n    \"total_zero_amount_transactions\": len(zero_amount_transactions_df),\n    \"unique_accounts_with_zero_amount\": zero_amount_transactions_df[\n        \"account_id\"\n    ].nunique(),\n    \"transaction_type_distribution\": zero_amount_transactions_df[\n        \"transaction_type\"\n    ].value_counts(normalize=True),\n    \"operation_distribution\": zero_amount_transactions_df[\"operation\"].value_counts(\n        normalize=True\n    ),\n    \"k_symbol_distribution\": zero_amount_transactions_df[\"k_symbol\"].value_counts(\n        normalize=True\n    ),\n}\n\nzero_amount_transactions_info, len(zero_amount_transactions_info)\n\n({'total_zero_amount_transactions': 14,\n  'unique_accounts_with_zero_amount': 12,\n  'transaction_type_distribution': transaction_type\n  Withdrawal    0.714286\n  Credit        0.285714\n  Name: proportion, dtype: float64,\n  'operation_distribution': operation\n  Withdrawal in Cash    0.714286\n  NA                    0.285714\n  Name: proportion, dtype: float64,\n  'k_symbol_distribution': k_symbol\n  Sanction Interest    0.714286\n  Interest Credited    0.285714\n  Name: proportion, dtype: float64},\n 5)\n\n\n\naccounts_with_zero_amount_transactions = accounts_df[\n    accounts_df[\"account_id\"].isin(zero_amount_transactions_df[\"account_id\"].unique())\n]\naccounts_with_zero_amount_transactions\n\n\n\n\n\n\n\n\naccount_id\ndistrict_id\naccount_frequency\naccount_created\n\n\n\n\n178\n5369\n54\nMONTHLY_ISSUANCE\n1993-02-25\n\n\n289\n5483\n13\nMONTHLY_ISSUANCE\n1993-03-28\n\n\n496\n5129\n68\nMONTHLY_ISSUANCE\n1993-06-08\n\n\n513\n1475\n1\nWEEKLY_ISSUANCE\n1993-06-14\n\n\n799\n9337\n30\nMONTHLY_ISSUANCE\n1993-09-13\n\n\n896\n102\n11\nMONTHLY_ISSUANCE\n1993-10-16\n\n\n986\n8957\n1\nMONTHLY_ISSUANCE\n1993-11-13\n\n\n2033\n5125\n1\nMONTHLY_ISSUANCE\n1995-09-14\n\n\n2300\n9051\n5\nWEEKLY_ISSUANCE\n1996-01-17\n\n\n2651\n3859\n53\nMONTHLY_ISSUANCE\n1996-04-23\n\n\n3212\n6083\n6\nWEEKLY_ISSUANCE\n1996-09-19\n\n\n3342\n1330\n68\nMONTHLY_ISSUANCE\n1996-10-22\n\n\n\n\n\n\n\n\n# Clean up unnecessary variables\ndel accounts_with_zero_amount_transactions\ndel zero_amount_transactions_df\ndel zero_amount_transactions_info\n\nValidating first transactions where the amount equals the balance is essential for the integrity of our aggregated data analysis. This specific assertion underpins the reliability of our subsequent aggregation operations by ensuring each account’s financial history starts from a verifiable point.\n\ndef validate_first_transactions(transactions):\n    \"\"\"\n    Validates that for each account in the transactions DataFrame, there is at least\n    one transaction where the amount equals the balance on the account's first transaction date.\n\n    Parameters:\n    - transactions (pd.DataFrame): DataFrame containing transaction data with columns\n      'account_id', 'date', 'amount', and 'balance'.\n\n    Raises:\n    - AssertionError: If not every account has a first transaction where the amount equals the balance.\n    \"\"\"\n\n    first_dates = (\n        transactions.groupby(\"account_id\")[\"date\"].min().reset_index(name=\"first_date\")\n    )\n\n    first_trans = pd.merge(transactions, first_dates, how=\"left\", on=[\"account_id\"])\n\n    first_trans_filtered = first_trans[\n        (first_trans[\"date\"] == first_trans[\"first_date\"])\n        & (first_trans[\"amount\"] == first_trans[\"balance\"])\n    ]\n\n    first_trans_filtered = first_trans_filtered.drop_duplicates(subset=[\"account_id\"])\n\n    unique_accounts = transactions[\"account_id\"].nunique()\n    assert (\n        unique_accounts == first_trans_filtered[\"account_id\"].nunique()\n    ), \"Not every account has a first transaction where the amount equals the balance.\"\n\n    return \"Validation successful: Each account has a first transaction where the amount equals the balance.\"\n\n\nvalidate_first_transactions(transactions_df)\n\n'Validation successful: Each account has a first transaction where the amount equals the balance.'\n\n\nWe can confirm the truth of the assertions made. It is certain that there is a transaction with an amount equal to the balance in the transaction history of any account on the first date.\n\n## DEPENDENCY 1 TODO REMOVE FOR MERGE \nimport json\n# save transactions_df to temp as parquet\n\ntransactions_df.to_parquet(\"temp/transactions.parquet\")\naccounts_df.to_parquet(\"temp/accounts.parquet\")\nnon_transactional_df.to_parquet(\"temp/non_transactional.parquet\")\n\n# save data reduction\nwith open(\"temp/data_reduction.json\", \"w\") as f:\n    json.dump(data_reduction, f)\n\n\n## DEPENDENCY #TODO REMOVE FOR MERGE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\n\ntransactions_df = pd.read_parquet(\"temp/transactions.parquet\")\naccounts_df = pd.read_parquet(\"temp/accounts.parquet\")\nnon_transactional_df = pd.read_parquet(\"temp/non_transactional.parquet\")\n# read data_reduction from temp/data_reduction.json\nwith open(\"temp/data_reduction.json\", \"r\") as f:\n    data_reduction = json.load(f)"
  },
  {
    "objectID": "main.html#set-artificial-issue-date-for-non-card-holders",
    "href": "main.html#set-artificial-issue-date-for-non-card-holders",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "4.1 Set artificial issue date for non-card holders",
    "text": "4.1 Set artificial issue date for non-card holders\n\ndef add_months_since_account_to_card(df):\n    df[\"months_since_account_to_card\"] = df.apply(\n        lambda row: (\n            (\n                row[\"card_issued\"].to_period(\"M\")\n                - row[\"account_created\"].to_period(\"M\")\n            ).n\n            if pd.notnull(row[\"card_issued\"]) and pd.notnull(row[\"account_created\"])\n            else np.nan\n        ),\n        axis=1,\n    )\n    return df\n\n\ndef filter_clients_without_sufficient_history(\n    non_transactional_df, min_history_months=25\n):\n    if \"months_since_account_to_card\" not in non_transactional_df.columns:\n        print(\n            \"Warning: months_since_account_to_card column not found. Calculating history length.\"\n        )\n        non_transactional_df = add_months_since_account_to_card(non_transactional_df)\n\n    count_before = len(non_transactional_df)\n    filtered_df = non_transactional_df[\n        non_transactional_df[\"months_since_account_to_card\"].isnull()\n        | (non_transactional_df[\"months_since_account_to_card\"] &gt;= min_history_months)\n    ]\n    print(\n        f\"Filtered out {count_before - len(filtered_df)} records with less than {min_history_months} months of history. Percentage: {(count_before - len(filtered_df)) / count_before * 100:.2f}%.\"\n    )\n    return filtered_df\n\n\nbefore_len = len(non_transactional_df)\nnon_transactional_w_sufficient_history_df = filter_clients_without_sufficient_history(\n    non_transactional_df\n)\ndata_reduction[\"Clients without sufficient history\"] = -(\n    before_len - len(non_transactional_w_sufficient_history_df)\n)\ndel before_len\n\nWarning: months_since_account_to_card column not found. Calculating history length.\nFiltered out 419 records with less than 25 months of history. Percentage: 9.62%.\n\n\n\nnon_transactional_w_card_df = non_transactional_w_sufficient_history_df.dropna(\n    subset=[\"card_issued\"]\n).copy()\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    non_transactional_w_card_df[\"months_since_account_to_card\"], kde=True, bins=30\n)\nplt.title(\n    \"Distribution of Months from Account Creation to Card Issuance (for Card Holders)\"\n)\nplt.xlabel(\"Months\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "main.html#match-by-similar-transaction-activity",
    "href": "main.html#match-by-similar-transaction-activity",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "4.2 Match by similar transaction activity",
    "text": "4.2 Match by similar transaction activity\nThe following approaches were considered to match non-card holders with card holders:\n\nLooking at the distributions above extract the amount of history a buyer most likely has at the issue data of the card\nFor each non buyer, find a buyer which was active in a similar time window (Jaccard similarity on the Year-Month sets). Instead of looking at the full activity of a buyer, we only look at the pre-purchase activity as there is reason to believe that clients may change their patterns after purchasing date and therefore add unwanted bias.\n\nThe second approach is chosen as it is provides an intuitive way to match clients based on their activity which is not only explainable but also provides a way to match clients based on their behavior. It strikes a balance of not finding a perfect match but a good enough match to focus on the discriminative features of the data.\nThe following image serves as an technical overview of the matching process: \nThe process emphasizes matching based on the timing of activity, rather than a wide array of characteristics. By identifying when both existing cardholders and non-cardholders interacted with the bank, we can infer a level of behavioral alignment that extends beyond mere transactional data. This alignment suggests a shared response to external conditions.\nThe resolution of the activity matrix is a binary matrix where each row represents a client and each column represents a month. A value of 1 indicates activity in a given month, while 0 indicates inactivity. Therefore we concentrate on the periods during which clients engage with the bank in the form of transactions\nAssumption: This assumes that clients active during similar periods might be influenced by the same economic and societal conditions, providing a more nuanced foundation for establishing connections between current cardholders and potential new ones.\n\n4.2.1 Construction of the Activity Matrix\nThe activity matrix serves as the foundation of our matching process, mapping out the engagement of clients with our services over time. It is constructed from transaction data, organizing client interactions into a structured format that highlights periods of activity.\n\nData Aggregation: We start with transaction data, which records each client’s interactions across various months. This data includes every transaction made by both current cardholders and potential non-cardholders.\nTemporal Transformation: Each transaction is associated with a specific date. These dates are then transformed into monthly periods, consolidating daily transactions into a monthly view of activity. This step simplifies the data, focusing on the presence of activity within each month rather than the specific dates or frequencies of transactions.\nMatrix Structure: The transformed data is arranged into a matrix format. Rows represent individual clients, identified by their account IDs. Columns correspond to monthly periods, spanning the entire range of months covered by the transaction data.\nActivity Indication: In the matrix, a cell value is set to indicate the presence of activity for a given client in a given month. If a client made one or more transactions in a month, the corresponding cell is marked to reflect this activity. The absence of transactions for a client in a month leaves the cell unmarked.\nBinary Representation: The final step involves converting the activity indicators into a binary format. Active months are represented by a ‘1’, indicating the presence of transactions, while inactive months are denoted by a ‘0’, indicating no transactions.\n\nThe heatmap provided offers a visual representation of the activity matrix for clients, depicting the levels of engagement over various periods.\n\nDiagonal Trend: There is a distinct diagonal pattern, indicating that newer accounts (those created more recently) have fewer periods of activity. This makes sense as these accounts have not had the opportunity to transact over the earlier periods displayed on the heatmap.\nDarker Areas (Purple): These represent periods of inactivity where clients did not engage. The darker the shade, the less activity occurred in that particular period for the corresponding set of accounts.\nBrighter Areas (Yellow): In contrast, the brighter areas denote periods of activity. A brighter shade implies more clients were active during that period.\nAccount Creation Date: Clients are sorted by their account creation date. Those who joined earlier are at the top, while more recent clients appear toward the bottom of the heatmap.\n\n\ndef prepare_activity_matrix(transactions):\n    \"\"\"\n    Create an activity matrix from transaction data.\n\n    The function transforms transaction data into a binary matrix that indicates\n    whether an account was active in a given month.\n\n    Parameters:\n    - transactions (pd.DataFrame): A DataFrame containing the transaction data.\n\n    Returns:\n    - pd.DataFrame: An activity matrix with accounts as rows and months as columns.\n    \"\"\"\n    transactions[\"month_year\"] = transactions[\"date\"].dt.to_period(\"M\")\n    transactions[\"active\"] = 1\n\n    activity_matrix = transactions.pivot_table(\n        index=\"account_id\", columns=\"month_year\", values=\"active\", fill_value=0\n    )\n\n    activity_matrix.columns = [f\"active_{str(col)}\" for col in activity_matrix.columns]\n    return activity_matrix\n\n\ndef plot_activity_matrix(activity_matrix):\n    sparse_matrix = activity_matrix.astype(bool)\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(sparse_matrix, cmap=\"viridis\", cbar=True, yticklabels=False)\n    plt.title(f\"Activity Matrix across all clients sorted by account creation date\")\n    plt.xlabel(\"Period\")\n    plt.ylabel(\"Accounts\")\n    plt.tight_layout()\n    plt.show()\n\n\nactivity_matrix = prepare_activity_matrix(transactions_df)\nplot_activity_matrix(activity_matrix)\n\n\n\n\n\n\n\n\n\n\n4.2.2 Eligibility Criteria\nAfter constructing the activity matrix, we check for eligibility of non-cardholders to be matched with cardholders. This ensures alignment for later model construction. The eligibility criteria are as follows:\n\nAccount History: Non-cardholders must have an established history of interaction, with at least 25 months of history between account creation and card issuance (12 months (= New customer period) + 13 months (= one year of history) + 1 month (Lag period)).\nAccount Creation Date: The account creation date of a non-cardholder must precede the card issuance date of the cardholder as this is a prerequisite for the matching process to work correctly when we set the issue date for non-card holders.\n\n\nfrom sklearn.metrics import pairwise_distances\nfrom tqdm import tqdm\n\nELIGIBILITY_THRESHOLD_HIST_MONTHS = 25\n\n\ndef check_eligibility_for_matching(non_cardholder, cardholder, verbose=False):\n    \"\"\"\n    Determine if a non-cardholder is eligible for matching with a cardholder.\n\n    This function checks whether the card issuance to a cardholder occurred at least\n    25 months after the non-cardholder's account was created.\n\n    Parameters:\n    - non_cardholder (pd.Series): A data series containing the non-cardholder's details.\n    - cardholder (pd.Series): A data series containing the cardholder's details.\n    - verbose (bool): If True, print detailed eligibility information. Default is False.\n\n    Returns:\n    - bool: True if the non-cardholder is eligible for matching, False otherwise.\n    \"\"\"\n    if cardholder[\"card_issued\"] &lt;= non_cardholder[\"account_created\"]:\n        return False\n\n    period_diff = (\n        cardholder[\"card_issued\"].to_period(\"M\")\n        - non_cardholder[\"account_created\"].to_period(\"M\")\n    ).n\n\n    if verbose:\n        print(\n            f\"Card issued: {cardholder['card_issued']}, Account created: {non_cardholder['account_created']}, Period diff: {period_diff}, Eligible: {period_diff &gt;= ELIGIBILITY_THRESHOLD_HIST_MONTHS}\"\n        )\n\n    return period_diff &gt;= ELIGIBILITY_THRESHOLD_HIST_MONTHS\n\n\n\n4.2.3 Matching Process\nNext up we will implement the matching process. Our matching utilizes the Jaccard similarity index to compare activity patterns: We compare a vector representing an existing cardholder’s monthly activity against a matrix of non-cardholders’ activity patterns. Here we only consider the activity from the first transaction period across all customers to the card issue date.\nThe Jaccard similarity index is calculated as the intersection of active months divided by the union of active months between the two clients. This index ranges from 0 to 1, with higher values indicating greater similarity in activity patterns.\n\\[J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\nThe function match_cardholders_with_non_cardholders will perform the following steps:\n\nData Preparation: The function prepares the activity matrix and splits the non-cardholders into two groups: those with and without cards.\nMatching Process: For each cardholder, the function calculates the Jaccard similarity between their activity pattern and those of eligible non-cardholders. It then selects the top N similar non-cardholders and randomly assigns one match per cardholder.\nMatch Selection: The function selects a non-cardholder match for each cardholder based on the Jaccard similarity scores. It ensures that each non-cardholder is matched only once and that the top N similar non-cardholders are considered for matching.\n\nThe selection among the top N similar non-cardholders is done randomly to avoid bias. This process is defined in the select_non_cardholders function.\nThe function also checks for the eligibility as defined above.\nIf no eligible non-cardholders are found, the function prints a warning message.\n\nOutput: The function returns a list of tuples containing the matched cardholder and non-cardholder client IDs along with their similarity scores.\n\n\ndef select_non_cardholders(\n    distances,\n    eligible_non_cardholders,\n    matches,\n    matched_applicants,\n    cardholder,\n    without_card_activity,\n    top_n,\n):\n    \"\"\"\n    Randomly select a non-cardholder match for a cardholder from the top N eligible candidates.\n\n    Parameters:\n    - distances (np.array): An array of Jaccard distances between a cardholder and non-cardholders.\n    - eligible_non_cardholders (list): A list of indices for non-cardholders who are eligible for matching.\n    - matches (list): A list to which the match will be appended.\n    - matched_applicants (set): A set of indices for non-cardholders who have already been matched.\n    - cardholder (pd.Series): The data series of the current cardholder.\n    - without_card_activity (pd.DataFrame): A DataFrame of non-cardholders without card issuance.\n    - top_n (int): The number of top similar non-cardholders to consider for matching.\n\n    Returns:\n    - None: The matches list is updated in place with the selected match.\n    \"\"\"\n    eligible_distances = distances[eligible_non_cardholders]\n    sorted_indices = np.argsort(eligible_distances)[:top_n]\n\n    if sorted_indices.size &gt; 0:\n        selected_index = np.random.choice(sorted_indices)\n        actual_selected_index = eligible_non_cardholders[selected_index]\n\n        if actual_selected_index not in matched_applicants:\n            matched_applicants.add(actual_selected_index)\n            applicant = without_card_activity.iloc[actual_selected_index]\n            similarity = 1 - eligible_distances[selected_index]\n\n            matches.append(\n                (cardholder[\"client_id\"], applicant[\"client_id\"], similarity)\n            )\n\n\ndef match_cardholders_with_non_cardholders(non_transactional, transactions, top_n=5):\n    \"\"\"\n    Match cardholders with non-cardholders based on the similarity of their activity patterns.\n\n    The function creates an activity matrix, identifies eligible non-cardholders, calculates\n    the Jaccard similarity to find matches, and randomly selects one match per cardholder\n    from the top N similar non-cardholders.\n\n    Parameters:\n    - non_transactional (pd.DataFrame): A DataFrame containing non-cardholders.\n    - transactions (pd.DataFrame): A DataFrame containing transactional data.\n    - top_n (int): The number of top similar non-cardholders to consider for matching.\n\n    Returns:\n    - list: A list of tuples with the cardholder and matched non-cardholder client IDs and similarity scores.\n    \"\"\"\n    with_card = non_transactional[non_transactional[\"card_issued\"].notna()]\n    without_card = non_transactional[non_transactional[\"card_issued\"].isna()]\n\n    activity_matrix = prepare_activity_matrix(transactions)\n\n    with_card_activity = with_card.join(activity_matrix, on=\"account_id\", how=\"left\")\n    without_card_activity = without_card.join(\n        activity_matrix, on=\"account_id\", how=\"left\"\n    )\n\n    matched_non_cardholders = set()\n    matches = []\n\n    for idx, cardholder in tqdm(\n        with_card_activity.iterrows(),\n        total=len(with_card_activity),\n        desc=\"Matching cardholders\",\n    ):\n        issue_period = cardholder[\"card_issued\"].to_period(\"M\")\n        eligible_cols = [\n            col\n            for col in activity_matrix\n            if col.startswith(\"active\") and pd.Period(col.split(\"_\")[1]) &lt;= issue_period\n        ]\n\n        if not eligible_cols:\n            print(\n                f\"No eligible months found for cardholder client_id {cardholder['client_id']}.\"\n            )\n            continue\n        \n        cardholder_vector = cardholder[eligible_cols].values.reshape(1, -1)\n        non_cardholder_matrix = without_card_activity[eligible_cols].values\n        \n        cardholder_vector = np.where(cardholder_vector &gt; 0, 1, 0).astype(bool)\n        non_cardholder_matrix = np.where(non_cardholder_matrix &gt; 0, 1, 0).astype(bool)\n\n        assert (\n            cardholder_vector.shape[1] == non_cardholder_matrix.shape[1]\n        ), \"Dimension mismatch between cardholder and applicant activity matrix.\"\n\n        distances = pairwise_distances(\n            cardholder_vector, non_cardholder_matrix, \n            metric=\"jaccard\", n_jobs=-1 \n        ).flatten()\n        eligible_non_cardholders = [\n            i\n            for i, applicant in without_card_activity.iterrows()\n            if check_eligibility_for_matching(applicant, cardholder)\n            and i not in matched_non_cardholders\n        ]\n\n        if eligible_non_cardholders:\n            select_non_cardholders(\n                distances,\n                eligible_non_cardholders,\n                matches,\n                matched_non_cardholders,\n                cardholder,\n                without_card_activity,\n                top_n,\n            )\n        else:\n            print(\n                f\"No eligible non-cardholders found for cardholder client_id {cardholder['client_id']}.\"\n            )\n\n    return matches\n\nTODO: Visualise the matching process\nThe matching process is executed, and the results are stored in the matched_non_card_holders_df DataFrame. The percentage of clients with a card issued before and after matching is calculated to assess the impact of the matching process. We expect the percentage of clients with a card issued to increase by 100% after matching, as each non-cardholder should be matched with a cardholder.\nLast but not least we set the artificial card issue date for each non-cardholder based on the matching results.\n\ndef set_artificial_issue_dates(non_transactional_df, matches):\n    \"\"\"\n    Augment the non-transactional DataFrame with artificial card issue dates based on matching results.\n\n    Each matched non-cardholder is assigned a card issue date corresponding to their matched\n    cardholder. The 'has_card' flag for each non-cardholder is updated accordingly.\n\n    Parameters:\n    - non_transactional_df (pd.DataFrame): The DataFrame of non-cardholders to augment.\n    - matches (list): A list of tuples containing the matched cardholder and non-cardholder IDs and similarity scores.\n\n    Returns:\n    - pd.DataFrame: The augmented DataFrame with artificial card issue dates.\n    \"\"\"\n    augmented_df = non_transactional_df.copy()\n    augmented_df[\"has_card\"] = True\n\n    for cardholder_id, non_cardholder_id, _ in matches:\n        card_issue_date = augmented_df.loc[\n            augmented_df[\"client_id\"] == cardholder_id, \"card_issued\"\n        ].values[0]\n        augmented_df.loc[\n            augmented_df[\"client_id\"] == non_cardholder_id, [\"card_issued\", \"has_card\"]\n        ] = [card_issue_date, False]\n\n    return augmented_df\n\n\nmatched_non_card_holders_df = match_cardholders_with_non_cardholders(\n    non_transactional_w_sufficient_history_df, transactions_df\n)\n\nprint(\n    f\"Percentage of clients with card issued: {non_transactional_w_sufficient_history_df['card_issued'].notna().mean() * 100:.2f}%\"\n)\nmatched_non_card_holders_w_issue_date_df = set_artificial_issue_dates(\n    non_transactional_w_sufficient_history_df, matched_non_card_holders_df\n)\nprint(\n    f\"Percentage of clients with card issued after matching: {matched_non_card_holders_w_issue_date_df['card_issued'].notna().mean() * 100:.2f}%\"\n)\n\nMatching cardholders:   0%|          | 0/328 [00:00&lt;?, ?it/s]Matching cardholders:   0%|          | 1/328 [00:00&lt;01:51,  2.92it/s]Matching cardholders:   1%|          | 2/328 [00:00&lt;01:50,  2.96it/s]Matching cardholders:   1%|          | 3/328 [00:01&lt;01:50,  2.95it/s]Matching cardholders:   1%|          | 4/328 [00:01&lt;01:43,  3.14it/s]Matching cardholders:   2%|▏         | 5/328 [00:01&lt;01:39,  3.26it/s]Matching cardholders:   2%|▏         | 6/328 [00:01&lt;01:42,  3.15it/s]Matching cardholders:   2%|▏         | 7/328 [00:02&lt;01:42,  3.12it/s]Matching cardholders:   2%|▏         | 8/328 [00:02&lt;01:43,  3.10it/s]Matching cardholders:   3%|▎         | 9/328 [00:02&lt;01:32,  3.45it/s]Matching cardholders:   3%|▎         | 10/328 [00:03&lt;01:26,  3.70it/s]Matching cardholders:   3%|▎         | 11/328 [00:03&lt;01:31,  3.45it/s]Matching cardholders:   4%|▎         | 12/328 [00:03&lt;01:35,  3.30it/s]Matching cardholders:   4%|▍         | 13/328 [00:04&lt;01:59,  2.64it/s]Matching cardholders:   4%|▍         | 14/328 [00:04&lt;01:49,  2.86it/s]Matching cardholders:   5%|▍         | 15/328 [00:04&lt;01:47,  2.91it/s]Matching cardholders:   5%|▍         | 16/328 [00:05&lt;01:35,  3.28it/s]Matching cardholders:   5%|▌         | 17/328 [00:05&lt;01:26,  3.60it/s]Matching cardholders:   5%|▌         | 18/328 [00:05&lt;01:20,  3.84it/s]Matching cardholders:   6%|▌         | 19/328 [00:05&lt;01:26,  3.58it/s]Matching cardholders:   6%|▌         | 20/328 [00:06&lt;01:29,  3.42it/s]Matching cardholders:   6%|▋         | 21/328 [00:06&lt;01:33,  3.30it/s]Matching cardholders:   7%|▋         | 22/328 [00:06&lt;01:35,  3.21it/s]Matching cardholders:   7%|▋         | 23/328 [00:07&lt;01:39,  3.05it/s]Matching cardholders:   7%|▋         | 24/328 [00:07&lt;01:43,  2.93it/s]Matching cardholders:   8%|▊         | 25/328 [00:07&lt;01:42,  2.95it/s]Matching cardholders:   8%|▊         | 26/328 [00:08&lt;01:31,  3.29it/s]Matching cardholders:   8%|▊         | 27/328 [00:08&lt;01:25,  3.51it/s]Matching cardholders:   9%|▊         | 28/328 [00:08&lt;01:28,  3.39it/s]Matching cardholders:   9%|▉         | 29/328 [00:08&lt;01:28,  3.38it/s]Matching cardholders:   9%|▉         | 30/328 [00:09&lt;01:31,  3.25it/s]Matching cardholders:   9%|▉         | 31/328 [00:09&lt;01:29,  3.31it/s]Matching cardholders:  10%|▉         | 32/328 [00:09&lt;01:22,  3.60it/s]Matching cardholders:  10%|█         | 33/328 [00:10&lt;01:24,  3.49it/s]Matching cardholders:  10%|█         | 34/328 [00:10&lt;01:34,  3.12it/s]Matching cardholders:  11%|█         | 35/328 [00:10&lt;01:34,  3.12it/s]Matching cardholders:  11%|█         | 36/328 [00:11&lt;01:26,  3.39it/s]Matching cardholders:  11%|█▏        | 37/328 [00:11&lt;01:28,  3.28it/s]Matching cardholders:  12%|█▏        | 38/328 [00:11&lt;01:30,  3.21it/s]Matching cardholders:  12%|█▏        | 39/328 [00:12&lt;01:26,  3.33it/s]Matching cardholders:  12%|█▏        | 40/328 [00:12&lt;01:27,  3.29it/s]Matching cardholders:  12%|█▎        | 41/328 [00:12&lt;01:28,  3.24it/s]Matching cardholders:  13%|█▎        | 42/328 [00:12&lt;01:29,  3.20it/s]Matching cardholders:  13%|█▎        | 43/328 [00:13&lt;01:30,  3.14it/s]Matching cardholders:  13%|█▎        | 44/328 [00:13&lt;01:24,  3.36it/s]Matching cardholders:  14%|█▎        | 45/328 [00:13&lt;01:20,  3.51it/s]Matching cardholders:  14%|█▍        | 46/328 [00:14&lt;01:17,  3.63it/s]Matching cardholders:  14%|█▍        | 47/328 [00:14&lt;01:21,  3.43it/s]Matching cardholders:  15%|█▍        | 48/328 [00:14&lt;01:24,  3.31it/s]Matching cardholders:  15%|█▍        | 49/328 [00:15&lt;01:26,  3.21it/s]Matching cardholders:  15%|█▌        | 50/328 [00:15&lt;01:26,  3.21it/s]Matching cardholders:  16%|█▌        | 51/328 [00:15&lt;01:24,  3.26it/s]Matching cardholders:  16%|█▌        | 52/328 [00:15&lt;01:19,  3.49it/s]Matching cardholders:  16%|█▌        | 53/328 [00:16&lt;01:30,  3.03it/s]Matching cardholders:  16%|█▋        | 54/328 [00:16&lt;01:22,  3.31it/s]Matching cardholders:  17%|█▋        | 55/328 [00:16&lt;01:23,  3.27it/s]Matching cardholders:  17%|█▋        | 56/328 [00:17&lt;01:25,  3.19it/s]Matching cardholders:  17%|█▋        | 57/328 [00:17&lt;01:26,  3.14it/s]Matching cardholders:  18%|█▊        | 58/328 [00:17&lt;01:23,  3.24it/s]Matching cardholders:  18%|█▊        | 59/328 [00:18&lt;01:16,  3.50it/s]Matching cardholders:  18%|█▊        | 60/328 [00:18&lt;01:16,  3.52it/s]Matching cardholders:  19%|█▊        | 61/328 [00:18&lt;01:12,  3.69it/s]Matching cardholders:  19%|█▉        | 62/328 [00:18&lt;01:16,  3.47it/s]Matching cardholders:  19%|█▉        | 63/328 [00:19&lt;01:19,  3.33it/s]Matching cardholders:  20%|█▉        | 64/328 [00:19&lt;01:20,  3.27it/s]Matching cardholders:  20%|█▉        | 65/328 [00:19&lt;01:21,  3.21it/s]Matching cardholders:  20%|██        | 66/328 [00:20&lt;01:20,  3.25it/s]Matching cardholders:  20%|██        | 67/328 [00:20&lt;01:22,  3.18it/s]Matching cardholders:  21%|██        | 68/328 [00:20&lt;01:23,  3.11it/s]Matching cardholders:  21%|██        | 69/328 [00:21&lt;01:20,  3.20it/s]Matching cardholders:  21%|██▏       | 70/328 [00:21&lt;01:19,  3.25it/s]Matching cardholders:  22%|██▏       | 71/328 [00:21&lt;01:20,  3.19it/s]Matching cardholders:  22%|██▏       | 72/328 [00:22&lt;01:15,  3.38it/s]Matching cardholders:  22%|██▏       | 73/328 [00:22&lt;01:18,  3.25it/s]Matching cardholders:  23%|██▎       | 74/328 [00:22&lt;01:24,  3.01it/s]Matching cardholders:  23%|██▎       | 75/328 [00:23&lt;01:23,  3.04it/s]Matching cardholders:  23%|██▎       | 76/328 [00:23&lt;01:22,  3.04it/s]Matching cardholders:  23%|██▎       | 77/328 [00:23&lt;01:21,  3.09it/s]Matching cardholders:  24%|██▍       | 78/328 [00:23&lt;01:19,  3.14it/s]Matching cardholders:  24%|██▍       | 79/328 [00:24&lt;01:20,  3.10it/s]Matching cardholders:  24%|██▍       | 80/328 [00:24&lt;01:19,  3.12it/s]Matching cardholders:  25%|██▍       | 81/328 [00:24&lt;01:14,  3.30it/s]Matching cardholders:  25%|██▌       | 82/328 [00:25&lt;01:14,  3.28it/s]Matching cardholders:  25%|██▌       | 83/328 [00:25&lt;01:16,  3.20it/s]Matching cardholders:  26%|██▌       | 84/328 [00:25&lt;01:17,  3.14it/s]Matching cardholders:  26%|██▌       | 85/328 [00:26&lt;01:15,  3.21it/s]Matching cardholders:  26%|██▌       | 86/328 [00:26&lt;01:11,  3.39it/s]Matching cardholders:  27%|██▋       | 87/328 [00:26&lt;01:09,  3.47it/s]Matching cardholders:  27%|██▋       | 88/328 [00:26&lt;01:05,  3.69it/s]Matching cardholders:  27%|██▋       | 89/328 [00:27&lt;01:03,  3.78it/s]Matching cardholders:  27%|██▋       | 90/328 [00:27&lt;01:06,  3.56it/s]Matching cardholders:  28%|██▊       | 91/328 [00:27&lt;01:10,  3.38it/s]Matching cardholders:  28%|██▊       | 92/328 [00:28&lt;01:11,  3.30it/s]Matching cardholders:  28%|██▊       | 93/328 [00:28&lt;01:13,  3.20it/s]Matching cardholders:  29%|██▊       | 94/328 [00:28&lt;01:14,  3.15it/s]Matching cardholders:  29%|██▉       | 95/328 [00:29&lt;01:15,  3.09it/s]Matching cardholders:  29%|██▉       | 96/328 [00:29&lt;01:15,  3.06it/s]Matching cardholders:  30%|██▉       | 97/328 [00:29&lt;01:22,  2.81it/s]Matching cardholders:  30%|██▉       | 98/328 [00:30&lt;01:20,  2.86it/s]Matching cardholders:  30%|███       | 99/328 [00:30&lt;01:18,  2.93it/s]Matching cardholders:  30%|███       | 100/328 [00:30&lt;01:10,  3.24it/s]Matching cardholders:  31%|███       | 101/328 [00:31&lt;01:07,  3.34it/s]Matching cardholders:  31%|███       | 102/328 [00:31&lt;01:10,  3.23it/s]Matching cardholders:  31%|███▏      | 103/328 [00:31&lt;01:11,  3.16it/s]Matching cardholders:  32%|███▏      | 104/328 [00:32&lt;01:11,  3.12it/s]Matching cardholders:  32%|███▏      | 105/328 [00:32&lt;01:08,  3.24it/s]Matching cardholders:  32%|███▏      | 106/328 [00:32&lt;01:09,  3.18it/s]Matching cardholders:  33%|███▎      | 107/328 [00:32&lt;01:08,  3.22it/s]Matching cardholders:  33%|███▎      | 108/328 [00:33&lt;01:03,  3.46it/s]Matching cardholders:  33%|███▎      | 109/328 [00:33&lt;00:59,  3.66it/s]Matching cardholders:  34%|███▎      | 110/328 [00:33&lt;01:03,  3.44it/s]Matching cardholders:  34%|███▍      | 111/328 [00:34&lt;01:05,  3.31it/s]Matching cardholders:  34%|███▍      | 112/328 [00:34&lt;01:05,  3.28it/s]Matching cardholders:  34%|███▍      | 113/328 [00:34&lt;01:03,  3.40it/s]Matching cardholders:  35%|███▍      | 114/328 [00:35&lt;01:05,  3.28it/s]Matching cardholders:  35%|███▌      | 115/328 [00:35&lt;01:06,  3.19it/s]Matching cardholders:  35%|███▌      | 116/328 [00:35&lt;01:05,  3.25it/s]Matching cardholders:  36%|███▌      | 117/328 [00:35&lt;01:05,  3.21it/s]Matching cardholders:  36%|███▌      | 118/328 [00:36&lt;01:00,  3.46it/s]Matching cardholders:  36%|███▋      | 119/328 [00:36&lt;01:03,  3.31it/s]Matching cardholders:  37%|███▋      | 120/328 [00:36&lt;01:08,  3.02it/s]Matching cardholders:  37%|███▋      | 121/328 [00:37&lt;01:09,  2.98it/s]Matching cardholders:  37%|███▋      | 122/328 [00:37&lt;01:04,  3.21it/s]Matching cardholders:  38%|███▊      | 123/328 [00:37&lt;01:03,  3.25it/s]Matching cardholders:  38%|███▊      | 124/328 [00:38&lt;01:01,  3.30it/s]Matching cardholders:  38%|███▊      | 125/328 [00:38&lt;01:02,  3.24it/s]Matching cardholders:  38%|███▊      | 126/328 [00:38&lt;01:03,  3.17it/s]Matching cardholders:  39%|███▊      | 127/328 [00:39&lt;00:59,  3.40it/s]Matching cardholders:  39%|███▉      | 128/328 [00:39&lt;00:56,  3.53it/s]Matching cardholders:  39%|███▉      | 129/328 [00:39&lt;00:59,  3.35it/s]Matching cardholders:  40%|███▉      | 130/328 [00:39&lt;00:59,  3.32it/s]Matching cardholders:  40%|███▉      | 131/328 [00:40&lt;01:01,  3.21it/s]Matching cardholders:  40%|████      | 132/328 [00:40&lt;00:56,  3.45it/s]Matching cardholders:  41%|████      | 133/328 [00:40&lt;00:59,  3.30it/s]Matching cardholders:  41%|████      | 134/328 [00:41&lt;01:00,  3.21it/s]Matching cardholders:  41%|████      | 135/328 [00:41&lt;00:59,  3.24it/s]Matching cardholders:  41%|████▏     | 136/328 [00:41&lt;01:00,  3.17it/s]Matching cardholders:  42%|████▏     | 137/328 [00:42&lt;01:01,  3.12it/s]Matching cardholders:  42%|████▏     | 138/328 [00:42&lt;01:01,  3.08it/s]Matching cardholders:  42%|████▏     | 139/328 [00:42&lt;01:01,  3.07it/s]Matching cardholders:  43%|████▎     | 140/328 [00:43&lt;01:00,  3.11it/s]Matching cardholders:  43%|████▎     | 141/328 [00:43&lt;00:56,  3.32it/s]Matching cardholders:  43%|████▎     | 142/328 [00:43&lt;00:55,  3.35it/s]Matching cardholders:  44%|████▎     | 143/328 [00:44&lt;01:04,  2.89it/s]Matching cardholders:  44%|████▍     | 144/328 [00:44&lt;01:03,  2.92it/s]Matching cardholders:  44%|████▍     | 145/328 [00:44&lt;00:58,  3.10it/s]Matching cardholders:  45%|████▍     | 146/328 [00:45&lt;00:58,  3.13it/s]Matching cardholders:  45%|████▍     | 147/328 [00:45&lt;00:58,  3.09it/s]Matching cardholders:  45%|████▌     | 148/328 [00:45&lt;00:55,  3.24it/s]Matching cardholders:  45%|████▌     | 149/328 [00:45&lt;00:54,  3.31it/s]Matching cardholders:  46%|████▌     | 150/328 [00:46&lt;00:55,  3.22it/s]Matching cardholders:  46%|████▌     | 151/328 [00:46&lt;00:52,  3.37it/s]Matching cardholders:  46%|████▋     | 152/328 [00:46&lt;00:54,  3.26it/s]Matching cardholders:  47%|████▋     | 153/328 [00:47&lt;00:51,  3.41it/s]Matching cardholders:  47%|████▋     | 154/328 [00:47&lt;00:52,  3.34it/s]Matching cardholders:  47%|████▋     | 155/328 [00:47&lt;00:53,  3.24it/s]Matching cardholders:  48%|████▊     | 156/328 [00:48&lt;00:54,  3.17it/s]Matching cardholders:  48%|████▊     | 157/328 [00:48&lt;00:54,  3.12it/s]Matching cardholders:  48%|████▊     | 158/328 [00:48&lt;00:53,  3.17it/s]Matching cardholders:  48%|████▊     | 159/328 [00:49&lt;00:51,  3.27it/s]Matching cardholders:  49%|████▉     | 160/328 [00:49&lt;00:50,  3.33it/s]Matching cardholders:  49%|████▉     | 161/328 [00:49&lt;00:48,  3.41it/s]Matching cardholders:  49%|████▉     | 162/328 [00:49&lt;00:48,  3.40it/s]Matching cardholders:  50%|████▉     | 163/328 [00:50&lt;00:47,  3.50it/s]Matching cardholders:  50%|█████     | 164/328 [00:50&lt;00:49,  3.34it/s]Matching cardholders:  50%|█████     | 165/328 [00:50&lt;00:50,  3.24it/s]Matching cardholders:  51%|█████     | 166/328 [00:51&lt;00:56,  2.86it/s]Matching cardholders:  51%|█████     | 167/328 [00:51&lt;00:54,  2.95it/s]Matching cardholders:  51%|█████     | 168/328 [00:51&lt;00:53,  2.98it/s]Matching cardholders:  52%|█████▏    | 169/328 [00:52&lt;00:53,  2.99it/s]Matching cardholders:  52%|█████▏    | 170/328 [00:52&lt;00:52,  3.01it/s]Matching cardholders:  52%|█████▏    | 171/328 [00:52&lt;00:51,  3.03it/s]Matching cardholders:  52%|█████▏    | 172/328 [00:53&lt;00:52,  2.96it/s]Matching cardholders:  53%|█████▎    | 173/328 [00:53&lt;00:49,  3.14it/s]Matching cardholders:  53%|█████▎    | 174/328 [00:53&lt;00:46,  3.28it/s]Matching cardholders:  53%|█████▎    | 175/328 [00:54&lt;00:47,  3.20it/s]Matching cardholders:  54%|█████▎    | 176/328 [00:54&lt;00:48,  3.13it/s]Matching cardholders:  54%|█████▍    | 177/328 [00:54&lt;00:48,  3.10it/s]Matching cardholders:  54%|█████▍    | 178/328 [00:55&lt;00:48,  3.08it/s]Matching cardholders:  55%|█████▍    | 179/328 [00:55&lt;00:48,  3.07it/s]Matching cardholders:  55%|█████▍    | 180/328 [00:55&lt;00:48,  3.05it/s]Matching cardholders:  55%|█████▌    | 181/328 [00:56&lt;00:46,  3.18it/s]Matching cardholders:  55%|█████▌    | 182/328 [00:56&lt;00:45,  3.24it/s]Matching cardholders:  56%|█████▌    | 183/328 [00:56&lt;00:46,  3.15it/s]Matching cardholders:  56%|█████▌    | 184/328 [00:57&lt;00:46,  3.10it/s]Matching cardholders:  56%|█████▋    | 185/328 [00:57&lt;00:45,  3.11it/s]Matching cardholders:  57%|█████▋    | 186/328 [00:57&lt;00:45,  3.14it/s]Matching cardholders:  57%|█████▋    | 187/328 [00:57&lt;00:45,  3.09it/s]Matching cardholders:  57%|█████▋    | 188/328 [00:58&lt;00:45,  3.06it/s]Matching cardholders:  58%|█████▊    | 189/328 [00:58&lt;00:48,  2.87it/s]Matching cardholders:  58%|█████▊    | 190/328 [00:59&lt;00:47,  2.91it/s]Matching cardholders:  58%|█████▊    | 191/328 [00:59&lt;00:46,  2.94it/s]Matching cardholders:  59%|█████▊    | 192/328 [00:59&lt;00:44,  3.05it/s]Matching cardholders:  59%|█████▉    | 193/328 [00:59&lt;00:43,  3.13it/s]Matching cardholders:  59%|█████▉    | 194/328 [01:00&lt;00:42,  3.19it/s]Matching cardholders:  59%|█████▉    | 195/328 [01:00&lt;00:42,  3.12it/s]Matching cardholders:  60%|█████▉    | 196/328 [01:00&lt;00:41,  3.15it/s]Matching cardholders:  60%|██████    | 197/328 [01:01&lt;00:41,  3.18it/s]Matching cardholders:  60%|██████    | 198/328 [01:01&lt;00:41,  3.12it/s]Matching cardholders:  61%|██████    | 199/328 [01:01&lt;00:40,  3.15it/s]Matching cardholders:  61%|██████    | 200/328 [01:02&lt;00:41,  3.10it/s]Matching cardholders:  61%|██████▏   | 201/328 [01:02&lt;00:41,  3.06it/s]Matching cardholders:  62%|██████▏   | 202/328 [01:02&lt;00:40,  3.11it/s]Matching cardholders:  62%|██████▏   | 203/328 [01:03&lt;00:40,  3.07it/s]Matching cardholders:  62%|██████▏   | 204/328 [01:03&lt;00:40,  3.04it/s]Matching cardholders:  62%|██████▎   | 205/328 [01:03&lt;00:40,  3.03it/s]Matching cardholders:  63%|██████▎   | 206/328 [01:04&lt;00:40,  3.02it/s]Matching cardholders:  63%|██████▎   | 207/328 [01:04&lt;00:40,  3.01it/s]Matching cardholders:  63%|██████▎   | 208/328 [01:04&lt;00:38,  3.11it/s]Matching cardholders:  64%|██████▎   | 209/328 [01:05&lt;00:38,  3.08it/s]Matching cardholders:  64%|██████▍   | 210/328 [01:05&lt;00:38,  3.06it/s]Matching cardholders:  64%|██████▍   | 211/328 [01:05&lt;00:38,  3.04it/s]Matching cardholders:  65%|██████▍   | 212/328 [01:06&lt;00:41,  2.81it/s]Matching cardholders:  65%|██████▍   | 213/328 [01:06&lt;00:40,  2.86it/s]Matching cardholders:  65%|██████▌   | 214/328 [01:06&lt;00:39,  2.90it/s]Matching cardholders:  66%|██████▌   | 215/328 [01:07&lt;00:38,  2.93it/s]Matching cardholders:  66%|██████▌   | 216/328 [01:07&lt;00:38,  2.95it/s]Matching cardholders:  66%|██████▌   | 217/328 [01:07&lt;00:37,  2.97it/s]Matching cardholders:  66%|██████▋   | 218/328 [01:08&lt;00:36,  3.01it/s]Matching cardholders:  67%|██████▋   | 219/328 [01:08&lt;00:35,  3.06it/s]Matching cardholders:  67%|██████▋   | 220/328 [01:08&lt;00:35,  3.05it/s]Matching cardholders:  67%|██████▋   | 221/328 [01:09&lt;00:35,  3.02it/s]Matching cardholders:  68%|██████▊   | 222/328 [01:09&lt;00:34,  3.07it/s]Matching cardholders:  68%|██████▊   | 223/328 [01:09&lt;00:33,  3.11it/s]Matching cardholders:  68%|██████▊   | 224/328 [01:10&lt;00:33,  3.07it/s]Matching cardholders:  69%|██████▊   | 225/328 [01:10&lt;00:33,  3.05it/s]Matching cardholders:  69%|██████▉   | 226/328 [01:10&lt;00:33,  3.03it/s]Matching cardholders:  69%|██████▉   | 227/328 [01:11&lt;00:33,  3.01it/s]Matching cardholders:  70%|██████▉   | 228/328 [01:11&lt;00:33,  3.01it/s]Matching cardholders:  70%|██████▉   | 229/328 [01:11&lt;00:32,  3.03it/s]Matching cardholders:  70%|███████   | 230/328 [01:12&lt;00:32,  3.02it/s]Matching cardholders:  70%|███████   | 231/328 [01:12&lt;00:32,  3.02it/s]Matching cardholders:  71%|███████   | 232/328 [01:12&lt;00:31,  3.01it/s]Matching cardholders:  71%|███████   | 233/328 [01:13&lt;00:31,  3.04it/s]Matching cardholders:  71%|███████▏  | 234/328 [01:13&lt;00:30,  3.04it/s]Matching cardholders:  72%|███████▏  | 235/328 [01:13&lt;00:33,  2.77it/s]Matching cardholders:  72%|███████▏  | 236/328 [01:14&lt;00:32,  2.84it/s]Matching cardholders:  72%|███████▏  | 237/328 [01:14&lt;00:31,  2.88it/s]Matching cardholders:  73%|███████▎  | 238/328 [01:14&lt;00:30,  2.92it/s]Matching cardholders:  73%|███████▎  | 239/328 [01:15&lt;00:30,  2.94it/s]Matching cardholders:  73%|███████▎  | 240/328 [01:15&lt;00:29,  2.96it/s]Matching cardholders:  73%|███████▎  | 241/328 [01:15&lt;00:29,  2.98it/s]Matching cardholders:  74%|███████▍  | 242/328 [01:16&lt;00:28,  3.00it/s]Matching cardholders:  74%|███████▍  | 243/328 [01:16&lt;00:28,  2.99it/s]Matching cardholders:  74%|███████▍  | 244/328 [01:16&lt;00:27,  3.00it/s]Matching cardholders:  75%|███████▍  | 245/328 [01:17&lt;00:27,  3.00it/s]Matching cardholders:  75%|███████▌  | 246/328 [01:17&lt;00:27,  2.99it/s]Matching cardholders:  75%|███████▌  | 247/328 [01:17&lt;00:27,  2.99it/s]Matching cardholders:  76%|███████▌  | 248/328 [01:18&lt;00:26,  2.98it/s]Matching cardholders:  76%|███████▌  | 249/328 [01:18&lt;00:26,  3.00it/s]Matching cardholders:  76%|███████▌  | 250/328 [01:18&lt;00:26,  3.00it/s]Matching cardholders:  77%|███████▋  | 251/328 [01:19&lt;00:25,  3.00it/s]Matching cardholders:  77%|███████▋  | 252/328 [01:19&lt;00:25,  3.01it/s]Matching cardholders:  77%|███████▋  | 253/328 [01:19&lt;00:24,  3.00it/s]Matching cardholders:  77%|███████▋  | 254/328 [01:20&lt;00:24,  3.00it/s]Matching cardholders:  78%|███████▊  | 255/328 [01:20&lt;00:24,  3.00it/s]Matching cardholders:  78%|███████▊  | 256/328 [01:20&lt;00:24,  2.97it/s]Matching cardholders:  78%|███████▊  | 257/328 [01:21&lt;00:23,  2.97it/s]Matching cardholders:  79%|███████▊  | 258/328 [01:21&lt;00:25,  2.72it/s]Matching cardholders:  79%|███████▉  | 259/328 [01:22&lt;00:24,  2.80it/s]Matching cardholders:  79%|███████▉  | 260/328 [01:22&lt;00:23,  2.85it/s]Matching cardholders:  80%|███████▉  | 261/328 [01:22&lt;00:23,  2.88it/s]Matching cardholders:  80%|███████▉  | 262/328 [01:23&lt;00:22,  2.91it/s]Matching cardholders:  80%|████████  | 263/328 [01:23&lt;00:22,  2.94it/s]Matching cardholders:  80%|████████  | 264/328 [01:23&lt;00:21,  2.95it/s]Matching cardholders:  81%|████████  | 265/328 [01:24&lt;00:21,  2.96it/s]Matching cardholders:  81%|████████  | 266/328 [01:24&lt;00:20,  2.97it/s]Matching cardholders:  81%|████████▏ | 267/328 [01:24&lt;00:20,  2.96it/s]Matching cardholders:  82%|████████▏ | 268/328 [01:25&lt;00:20,  2.97it/s]Matching cardholders:  82%|████████▏ | 269/328 [01:25&lt;00:19,  2.99it/s]Matching cardholders:  82%|████████▏ | 270/328 [01:25&lt;00:19,  2.98it/s]Matching cardholders:  83%|████████▎ | 271/328 [01:26&lt;00:19,  2.99it/s]Matching cardholders:  83%|████████▎ | 272/328 [01:26&lt;00:18,  2.99it/s]Matching cardholders:  83%|████████▎ | 273/328 [01:26&lt;00:18,  3.00it/s]Matching cardholders:  84%|████████▎ | 274/328 [01:27&lt;00:18,  3.00it/s]Matching cardholders:  84%|████████▍ | 275/328 [01:27&lt;00:18,  2.82it/s]Matching cardholders:  84%|████████▍ | 276/328 [01:27&lt;00:18,  2.87it/s]Matching cardholders:  84%|████████▍ | 277/328 [01:28&lt;00:17,  2.91it/s]Matching cardholders:  85%|████████▍ | 278/328 [01:28&lt;00:17,  2.93it/s]Matching cardholders:  85%|████████▌ | 279/328 [01:28&lt;00:16,  2.95it/s]Matching cardholders:  85%|████████▌ | 280/328 [01:29&lt;00:16,  2.96it/s]Matching cardholders:  86%|████████▌ | 281/328 [01:29&lt;00:17,  2.70it/s]Matching cardholders:  86%|████████▌ | 282/328 [01:29&lt;00:16,  2.77it/s]Matching cardholders:  86%|████████▋ | 283/328 [01:30&lt;00:15,  2.83it/s]Matching cardholders:  87%|████████▋ | 284/328 [01:30&lt;00:15,  2.88it/s]Matching cardholders:  87%|████████▋ | 285/328 [01:30&lt;00:14,  2.90it/s]Matching cardholders:  87%|████████▋ | 286/328 [01:31&lt;00:14,  2.92it/s]Matching cardholders:  88%|████████▊ | 287/328 [01:31&lt;00:14,  2.93it/s]Matching cardholders:  88%|████████▊ | 288/328 [01:31&lt;00:13,  2.93it/s]Matching cardholders:  88%|████████▊ | 289/328 [01:32&lt;00:13,  2.95it/s]Matching cardholders:  88%|████████▊ | 290/328 [01:32&lt;00:12,  2.96it/s]Matching cardholders:  89%|████████▊ | 291/328 [01:32&lt;00:12,  2.97it/s]Matching cardholders:  89%|████████▉ | 292/328 [01:33&lt;00:12,  2.97it/s]Matching cardholders:  89%|████████▉ | 293/328 [01:33&lt;00:11,  2.97it/s]Matching cardholders:  90%|████████▉ | 294/328 [01:33&lt;00:11,  2.98it/s]Matching cardholders:  90%|████████▉ | 295/328 [01:34&lt;00:11,  2.99it/s]Matching cardholders:  90%|█████████ | 296/328 [01:34&lt;00:11,  2.77it/s]Matching cardholders:  91%|█████████ | 297/328 [01:35&lt;00:10,  2.84it/s]Matching cardholders:  91%|█████████ | 298/328 [01:35&lt;00:10,  2.86it/s]Matching cardholders:  91%|█████████ | 299/328 [01:35&lt;00:09,  2.91it/s]Matching cardholders:  91%|█████████▏| 300/328 [01:36&lt;00:09,  2.92it/s]Matching cardholders:  92%|█████████▏| 301/328 [01:36&lt;00:09,  2.95it/s]Matching cardholders:  92%|█████████▏| 302/328 [01:36&lt;00:08,  2.97it/s]Matching cardholders:  92%|█████████▏| 303/328 [01:37&lt;00:08,  2.97it/s]Matching cardholders:  93%|█████████▎| 304/328 [01:37&lt;00:08,  2.72it/s]Matching cardholders:  93%|█████████▎| 305/328 [01:37&lt;00:08,  2.81it/s]Matching cardholders:  93%|█████████▎| 306/328 [01:38&lt;00:07,  2.85it/s]Matching cardholders:  94%|█████████▎| 307/328 [01:38&lt;00:07,  2.90it/s]Matching cardholders:  94%|█████████▍| 308/328 [01:38&lt;00:06,  2.92it/s]Matching cardholders:  94%|█████████▍| 309/328 [01:39&lt;00:06,  2.93it/s]Matching cardholders:  95%|█████████▍| 310/328 [01:39&lt;00:06,  2.96it/s]Matching cardholders:  95%|█████████▍| 311/328 [01:39&lt;00:05,  2.98it/s]Matching cardholders:  95%|█████████▌| 312/328 [01:40&lt;00:05,  2.97it/s]Matching cardholders:  95%|█████████▌| 313/328 [01:40&lt;00:05,  2.98it/s]Matching cardholders:  96%|█████████▌| 314/328 [01:40&lt;00:04,  2.99it/s]Matching cardholders:  96%|█████████▌| 315/328 [01:41&lt;00:04,  2.98it/s]Matching cardholders:  96%|█████████▋| 316/328 [01:41&lt;00:04,  2.99it/s]Matching cardholders:  97%|█████████▋| 317/328 [01:41&lt;00:03,  3.00it/s]Matching cardholders:  97%|█████████▋| 318/328 [01:42&lt;00:03,  2.99it/s]Matching cardholders:  97%|█████████▋| 319/328 [01:42&lt;00:03,  3.00it/s]Matching cardholders:  98%|█████████▊| 320/328 [01:42&lt;00:02,  3.00it/s]Matching cardholders:  98%|█████████▊| 321/328 [01:43&lt;00:02,  3.00it/s]Matching cardholders:  98%|█████████▊| 322/328 [01:43&lt;00:01,  3.00it/s]Matching cardholders:  98%|█████████▊| 323/328 [01:43&lt;00:01,  3.01it/s]Matching cardholders:  99%|█████████▉| 324/328 [01:44&lt;00:01,  2.99it/s]Matching cardholders:  99%|█████████▉| 325/328 [01:44&lt;00:01,  3.00it/s]Matching cardholders:  99%|█████████▉| 326/328 [01:44&lt;00:00,  3.00it/s]Matching cardholders: 100%|█████████▉| 327/328 [01:45&lt;00:00,  2.73it/s]Matching cardholders: 100%|██████████| 328/328 [01:45&lt;00:00,  2.80it/s]Matching cardholders: 100%|██████████| 328/328 [01:45&lt;00:00,  3.11it/s]\n\n\nPercentage of clients with card issued: 8.33%\nPercentage of clients with card issued after matching: 16.67%\n\n\nAfter each non-cardholder got the artifical card issued date assigned we drop the remaining non-cardholders without a match.\n\nbefore_len = len(matched_non_card_holders_w_issue_date_df)\nprint(-(before_len - len(matched_non_card_holders_w_issue_date_df)))\nmatched_non_card_holders_w_issue_date_df = (\n    matched_non_card_holders_w_issue_date_df.dropna(subset=[\"card_issued\"])\n)\ndata_reduction[\"Non-cardholders without match\"] = -(\n    before_len - len(matched_non_card_holders_w_issue_date_df)\n)\ndel before_len\n\n0"
  },
  {
    "objectID": "main.html#aggregate-on-a-monthly-basis",
    "href": "main.html#aggregate-on-a-monthly-basis",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "4.3 Aggregate on a Monthly Basis",
    "text": "4.3 Aggregate on a Monthly Basis\nAfter matching cardholders with non-cardholders and setting artificial card issue dates, we aggregate the transactional data on a monthly basis. This aggregation provides a comprehensive overview of financial activities for each account, facilitating further model development providing us with a fixed of features to work with.\nThe function aggregate_transactions_monthly is designed to process and summarize financial transactions on a monthly basis for each account within a dataset. The explanation of its workings, step by step, is as follows:\n\nSorting Transactions: Initially, the function sorts the transactions in the provided DataFrame transactions_df based on account_id and the transaction date. This ensures that all transactions for a given account are ordered chronologically, which is crucial for accurate monthly aggregation and cumulative balance calculation.\nMonthly Grouping: Each transaction’s date is then converted to a monthly period using dt.to_period(\"M\"). This step categorizes each transaction by the month and year it occurred, facilitating the aggregation of transactions on a monthly basis.\nAggregation of Monthly Data: The function groups the sorted transactions by account_id and the newly created month column. For each group, it calculates several metrics:\n\nvolume: The sum of all transactions’ amounts for the month, representing the total money flow.\ntotal_abs_amount: The sum of the absolute values of the transactions’ amounts, indicating the total amount of money moved, disregarding the direction.\ntransaction_count: The count of transactions, providing a sense of activity level.\npositive_transaction_count and negative_transaction_count: The counts of positive (inflows) and negative (outflows) transactions, respectively. This distinction can help identify the balance between income and expenses.\nStatistical measures like average_amount, median_amount, min_amount, max_amount, and std_amount offer insights into the distribution of transaction amounts.\ntype_count, operation_count, and k_symbol_count: The counts of unique transaction types, operations, and transaction symbols (k_symbol), respectively, indicating the diversity of transaction characteristics.\n\nCumulative Balance Calculation: After aggregating the monthly data, the function computes a cumulative balance (balance) for each account by cumulatively summing the volume (total transaction amount) over time. This step provides insight into how the account balance evolves over the months.\n\nAs we have already explored and verified in the EDA section of the transactional data, each account starts with a transaction where the amount equals the inital balance. This validation ensures the integrity of the aggregated data, as the balance should accurately reflect the total transaction volume over time.\n\ndef aggregate_transactions_monthly(df):\n    \"\"\"\n    Aggregate financial transaction data on a monthly basis per account.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing financial transaction data with 'account_id', 'date', and other relevant columns.\n\n    - validate (bool): If True, validate the aggregated data. Default is True.\n\n    Returns:\n    - pd.DataFrame: Monthly aggregated financial transaction data per account.\n    \"\"\"\n    df_sorted = df.sort_values(by=[\"account_id\", \"date\"])\n    df_sorted[\"month\"] = df_sorted[\"date\"].dt.to_period(\"M\")\n\n    monthly_aggregated_data = (\n        df_sorted.groupby([\"account_id\", \"month\"])\n        .agg(\n            volume=(\"amount\", \"sum\"),\n            total_abs_amount=(\"amount\", lambda x: x.abs().sum()),\n            transaction_count=(\"amount\", \"count\"),\n            positive_transaction_count=(\n                \"amount\",\n                lambda x: (x &gt;= 0).sum(),\n            ),  # TODO: it seems that there are some transactions with 0 amount, how to handle those?\n            negative_transaction_count=(\"amount\", lambda x: (x &lt; 0).sum()),\n            average_amount=(\"amount\", \"mean\"),\n            median_amount=(\"amount\", \"median\"),\n            min_amount=(\"amount\", \"min\"),\n            max_amount=(\"amount\", \"max\"),\n            std_amount=(\"amount\", \"std\"),\n            type_count=(\"transaction_type\", \"nunique\"),\n            operation_count=(\"operation\", \"nunique\"),\n            k_symbol_count=(\"k_symbol\", \"nunique\"),\n        )\n        .reset_index()\n        .sort_values(by=[\"account_id\", \"month\"])\n    )\n\n    monthly_aggregated_data[\"balance\"] = monthly_aggregated_data.groupby(\"account_id\")[\n        \"volume\"\n    ].cumsum()\n    return monthly_aggregated_data\n\n\nagg_transactions_monthly_df = aggregate_transactions_monthly(transactions_df)\nagg_transactions_monthly_df.to_csv(\"./data/agg_transactions_monthly.csv\", index=False)\nagg_transactions_monthly_df.describe()\n\n\n\n\n\n\n\n\naccount_id\nvolume\ntotal_abs_amount\ntransaction_count\npositive_transaction_count\nnegative_transaction_count\naverage_amount\nmedian_amount\nmin_amount\nmax_amount\nstd_amount\ntype_count\noperation_count\nk_symbol_count\nbalance\n\n\n\n\ncount\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n176803.000000\n185057.000000\n185057.000000\n185057.000000\n185057.000000\n\n\nmean\n2799.983292\n1065.354397\n33815.492309\n5.708079\n2.189017\n3.519062\n451.659265\n-372.421445\n-9607.378249\n14756.009580\n9030.305445\n1.921181\n3.568965\n3.719649\n34474.787632\n\n\nstd\n2331.861909\n12509.136299\n37724.985550\n2.417842\n0.726115\n2.173427\n2479.100575\n1933.445907\n10746.883348\n12958.692736\n7402.806514\n0.269457\n0.832363\n1.085701\n19799.443508\n\n\nmin\n1.000000\n-101550.300000\n14.600000\n1.000000\n0.000000\n0.000000\n-37000.000000\n-37000.000000\n-87400.000000\n-37000.000000\n0.000000\n1.000000\n1.000000\n1.000000\n-41125.800000\n\n\n25%\n1172.000000\n-2266.600000\n9659.500000\n4.000000\n2.000000\n2.000000\n-379.566667\n-785.000000\n-13428.000000\n4756.000000\n3283.937059\n2.000000\n3.000000\n3.000000\n20405.600000\n\n\n50%\n2375.000000\n1058.100000\n22933.100000\n5.000000\n2.000000\n3.000000\n220.260000\n-14.600000\n-6177.000000\n10929.000000\n6824.369949\n2.000000\n4.000000\n4.000000\n30000.000000\n\n\n75%\n3576.000000\n4132.200000\n43668.000000\n7.000000\n2.000000\n5.000000\n878.680000\n44.700000\n-2672.000000\n21553.000000\n12622.945077\n2.000000\n4.000000\n4.000000\n44540.500000\n\n\nmax\n11382.000000\n115038.200000\n609736.200000\n23.000000\n9.000000\n16.000000\n44708.000000\n44708.000000\n44708.000000\n74812.000000\n57782.701468\n2.000000\n6.000000\n7.000000\n138317.800000\n\n\n\n\n\n\n\nThe validate_monthly_aggregated_transactions function is invoked to ensure the integrity and correctness of the aggregated data through several assertions:\n\nThe balance should consistently increase or decrease based on whether the total monthly transaction volume is positive or negative, respectively.\nFor each account, the balance in the first month should equal the total transaction volume of that month.\nThe sum of positive and negative transaction counts must equal the total transaction count for each month.\nThe number of unique accounts in the aggregated data should match that in the original dataset.\nThe final balances of accounts in the aggregated data should closely match their last recorded transactions in the original dataset.\n\n\ndef validate_monthly_aggregated_transactions(aggregated_data, original_df):\n    \"\"\"\n    Validate the integrity and correctness of aggregated monthly financial transactions.\n\n    Parameters:\n    - aggregated_data (pd.DataFrame): Aggregated monthly transaction data.\n    - original_df (pd.DataFrame): Original dataset of financial transactions.\n\n    Raises:\n    - AssertionError: If validation conditions are not met.\n    \"\"\"\n\n    assert (aggregated_data[\"volume\"] &gt;= 0).all() == (\n        aggregated_data[\"balance\"].diff() &gt;= 0\n    ).all(), \"If the total amount is positive, the balance should go up.\"\n\n    assert (aggregated_data[\"volume\"] &lt; 0).all() == (\n        aggregated_data[\"balance\"].diff() &lt; 0\n    ).all(), \"If the total amount is negative, the balance should go down.\"\n\n    first_month = aggregated_data.groupby(\"account_id\").nth(0)\n    assert (\n        first_month[\"volume\"] == first_month[\"balance\"]\n    ).all(), \"The balance should equal the volume for the first month.\"\n\n    assert (\n        aggregated_data[\"positive_transaction_count\"]\n        + aggregated_data[\"negative_transaction_count\"]\n        == aggregated_data[\"transaction_count\"]\n    ).all(), \"The sum of positive and negative transaction counts should equal the total transaction count.\"\n\n    assert (\n        aggregated_data[\"account_id\"].nunique() == original_df[\"account_id\"].nunique()\n    ), \"The number of unique account_ids in the aggregated DataFrame should be the same as the original DataFrame.\"\n\n    assert (\n        pd.merge(\n            aggregated_data.groupby(\"account_id\")\n            .last()\n            .reset_index()[[\"account_id\", \"balance\"]],\n            original_df[\n                original_df.groupby(\"account_id\")[\"date\"].transform(\"max\")\n                == original_df[\"date\"]\n            ][[\"account_id\", \"balance\"]],\n            on=\"account_id\",\n            suffixes=(\"_final\", \"_last\"),\n        )\n        .apply(\n            lambda x: np.isclose(x[\"balance_final\"], x[\"balance_last\"], atol=5), axis=1\n        )\n        .any()\n    ), \"Some accounts' final balances do not match their last transactions.\"\n\n\nvalidate_monthly_aggregated_transactions(agg_transactions_monthly_df, transactions_df)"
  },
  {
    "objectID": "main.html#monthly-balance-difference-and-volume",
    "href": "main.html#monthly-balance-difference-and-volume",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "5.1 Monthly Balance Difference and Volume",
    "text": "5.1 Monthly Balance Difference and Volume\nThis plot gives a clear picture of how money moves in and out of an account each month and how these movements affect the overall balance. It does this by showing two things:\n\nBalance Difference: This line shows whether the account balance went up or down each month. If the line goes up, it means the account gained money that month. If it goes down, the account lost money.\nVolume: This line shows the total amount of money that moved in the account each month, regardless of whether it was coming in or going out.\n\nWhat to Look For: - A direct link between the amount of money moved (volume) and changes in the account balance. High incoming money should lead to an uptick in the balance, and lots of outgoing money should lead to a downturn. - This visual check helps to understand how active the account is and whether it’s generally getting fuller or emptier over time.\n\ndef plot_monthly_balance_diff_and_volume(\n    transactions_monthly, account_id \n):\n    account_transactions = transactions_monthly[\n        transactions_monthly[\"account_id\"] == account_id\n    ].sort_values(by=\"month\")\n    account_transactions[\"balance_diff\"] = account_transactions[\"balance\"].diff()\n\n    plt.figure(figsize=(9.5, 6))\n\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"balance_diff\"],\n        marker=\"o\",\n        label=\"Balance Difference\",\n    )\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"volume\"],\n        marker=\"x\",\n        linestyle=\"--\",\n        label=\"Volume\",\n    )\n\n    plt.title(f\"Monthly Balance Difference and Volume for Account {account_id}\")\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.xticks(rotation=90, fontsize=7)\n    plt.yticks(fontsize=8)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nplot_monthly_balance_diff_and_volume(agg_transactions_monthly_df, 2)"
  },
  {
    "objectID": "main.html#monthly-transactions-balance-and-volume-plot-explanation",
    "href": "main.html#monthly-transactions-balance-and-volume-plot-explanation",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "5.2 Monthly Transactions, Balance, and Volume Plot Explanation",
    "text": "5.2 Monthly Transactions, Balance, and Volume Plot Explanation\nThis visualization offers a snapshot of an account’s activity over time by comparing money movement each month with the overall account balance. It helps to understand:\n\nVolume: How much money came in or went out of the account each month. Incoming money is shown as up, and outgoing money as down.\nBalance: The total money in the account at the end of each month, showing how it’s changed over time due to the monthly transactions.\n\nWhat to Look For: - How the monthly money movement impacts the account’s growing or shrinking balance. For example, a few months of high income should visibly increase the balance. - This simple visual guide helps spot trends, like if the account is steadily growing, holding steady, or facing issues, giving quick insights into financial well-being and further validates the aggregation made in the previous step.\n\ndef plot_monthly_transactions_balance_and_volume(agg_transactions_monthly, account_id):\n    account_transactions = agg_transactions_monthly[\n        agg_transactions_monthly[\"account_id\"] == account_id\n    ]\n\n    plt.figure(figsize=(9.5, 6))\n\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"volume\"],\n        marker=\"o\",\n        label=\"Volume\",\n    )\n    plt.plot(\n        account_transactions[\"month\"].astype(str),\n        account_transactions[\"balance\"],\n        marker=\"x\",\n        linestyle=\"--\",\n        label=\"Balance\",\n    )\n\n    plt.title(f\"Monthly Transactions and Balance for Account {account_id}\")\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.xticks(rotation=90, fontsize=7)\n    plt.yticks(fontsize=8)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 2)"
  },
  {
    "objectID": "main.html#delieverable-closer-look-at-account-14",
    "href": "main.html#delieverable-closer-look-at-account-14",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "5.3 Delieverable: Closer Look at Account 14",
    "text": "5.3 Delieverable: Closer Look at Account 14\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 14)\n\n\n\n\n\n\n\n\nAccount 14 shows a rather conservative transaction history. The spending habits are all withing range of 10k to -10k per month. We can see little volatility, the account shows a slight trend of growing."
  },
  {
    "objectID": "main.html#delieverable-closer-look-at-account-18",
    "href": "main.html#delieverable-closer-look-at-account-18",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "5.4 Delieverable: Closer Look at Account 18",
    "text": "5.4 Delieverable: Closer Look at Account 18\n\nplot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 18)\n\n\n\n\n\n\n\n\nAccount 18 paints a different picture in comparison to account 14.\nThe volatility here is a lot higher, indiciating a potential for a business account or high income household. Especially March 1994 to December 1994 show some volatile transaction habits.\nLooking at the balance and volume per month for the accounts 14 and 18 we can notice some interesting patterns.\nTODO: Add analysis"
  },
  {
    "objectID": "main.html#comparing-cardholders-and-non-cardholders",
    "href": "main.html#comparing-cardholders-and-non-cardholders",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "9.1 Comparing Cardholders and Non-Cardholders",
    "text": "9.1 Comparing Cardholders and Non-Cardholders\n\n9.1.1 Trends in Monthly Financial Metrics\n\ngolden_cardholders = golden_record_df[golden_record_df[\"has_card\"]]\ngolden_non_cardholders = golden_record_df[~golden_record_df[\"has_card\"]]\n\n\ndef plot_trends_with_medians(\n    cardholders, non_cardholders, columns, title, median_ranges\n):\n    \"\"\"\n    Plots line graphs for average monthly values and annotates medians for specified ranges,\n    adjusting x-axis indices to match the month sequence from the start.\n\n    Parameters:\n    - cardholders (pd.DataFrame): DataFrame containing data for cardholders.\n    - non_cardholders (pd.DataFrame): DataFrame containing data for non-cardholders.\n    - columns (list of str): List of column names ordered by time.\n    - title (str): Title for the plot.\n    - median_ranges (list of tuples): Each tuple contains start and end indices for calculating medians.\n    \"\"\"\n    cardholder_avgs = cardholders[columns].mean()\n    non_cardholder_avgs = non_cardholders[columns].mean()\n\n    months = list(range(1, 1 + len(columns)))\n    plt.figure()\n    plt.plot(\n        months,\n        cardholder_avgs.values,\n        marker=\"o\",\n        linestyle=\"-\",\n        color=\"blue\",\n        label=\"Cardholders\",\n    )\n    plt.plot(\n        months,\n        non_cardholder_avgs.values,\n        marker=\"o\",\n        linestyle=\"-\",\n        color=\"orange\",\n        label=\"Non-Cardholders\",\n    )\n\n    for start, end in median_ranges:\n        median_cardholder = cardholders[columns[start : end + 1]].median().median()\n        median_non_cardholder = (\n            non_cardholders[columns[start : end + 1]].median().median()\n        )\n        plt.hlines(\n            median_cardholder,\n            months[start],\n            months[end],\n            colors=\"darkblue\",\n            linestyles=\"--\",\n            label=f\"Median {start+1}-{end+1} (Cardholders): {median_cardholder:.2f}\",\n        )\n        plt.hlines(\n            median_non_cardholder,\n            months[start],\n            months[end],\n            colors=\"red\",\n            linestyles=\"--\",\n            label=f\"Median {start+1}-{end+1} (Non-Cardholders): {median_non_cardholder:.2f}\",\n        )\n\n    plt.title(title)\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.grid(True)\n    plt.xticks(months, labels=[f\"M_{month}\" for month in months])  # Proper month labels\n    plt.show()\n\n\n\n9.1.2 Monthly Balance Trends\n\nmedian_ranges = [\n    (0, 2),\n    (9, 11),\n]  # First 3 months and last 3 months for a 12-month period\nbalance_columns = [f\"M_{i}_balance\" for i in range(2, 14)]\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    balance_columns,\n    \"Monthly Balance Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n9.1.3 Monthly Volume Trends\n\nvolume_columns = [\n    f\"M_{i}_volume\" for i in range(2, 14)\n]  # Simulating monthly volume columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    volume_columns,\n    \"Monthly Volume Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n9.1.4 Monthly Transaction Count Trends\n\ntransaction_count_columns = [\n    f\"M_{i}_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    transaction_count_columns,\n    \"Monthly Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n9.1.5 Monthly Positive and Negative Transaction Count Trends\n\npositive_transaction_count_columns = [\n    f\"M_{i}_positive_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly positive transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    positive_transaction_count_columns,\n    \"Monthly Positive Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n9.1.6 Monthly Negative Transaction Count Trends\n\nnegative_transaction_count_columns = [\n    f\"M_{i}_negative_transaction_count\" for i in range(2, 14)\n]  # Simulating monthly negative transaction count columns\nplot_trends_with_medians(\n    golden_cardholders,\n    golden_non_cardholders,\n    negative_transaction_count_columns,\n    \"Monthly Negative Transaction Count Trends\",\n    median_ranges,\n)\n\n\n\n\n\n\n\n\n\n\n9.1.7 Comparison of Average Feature Values\n\ndef plot_grouped_comparison(cardholders, non_cardholders, feature_columns):\n    \"\"\"\n    Plots grouped bar charts for average feature values of cardholders and non-cardholders.\n\n    Parameters:\n    - cardholders (pd.DataFrame): DataFrame containing data for cardholders.\n    - non_cardholders (pd.DataFrame): DataFrame containing data for non-cardholders.\n    - feature_columns (list of str): List of column names whose averages to compare.\n    \"\"\"\n    cardholder_avg = cardholders[feature_columns].mean()\n    non_cardholder_avg = non_cardholders[feature_columns].mean()\n\n    index = range(len(feature_columns))\n    bar_width = 0.35\n\n    fig, ax = plt.subplots()\n    bars1 = ax.bar(\n        index, cardholder_avg, bar_width, label=\"Cardholders\", color=\"skyblue\"\n    )\n    bars2 = ax.bar(\n        [p + bar_width for p in index],\n        non_cardholder_avg,\n        bar_width,\n        label=\"Non-Cardholders\",\n        color=\"orange\",\n    )\n\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Average Value\")\n    ax.set_title(\"Average Feature Values by Group\")\n    ax.set_xticks([p + bar_width / 2 for p in index])\n    ax.set_xticklabels(feature_columns)\n    ax.legend()\n\n    plt.xticks(rotation=45)  # Rotate feature names for better visibility\n    plt.show()\n\n\nplot_grouped_comparison(\n    golden_cardholders,\n    golden_non_cardholders,\n    [col for col in golden_record_df.columns if \"balance\" in col],\n)\nplot_grouped_comparison(golden_cardholders, golden_non_cardholders, [\"loan_amount\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## DEPENDENCIES TODO REMOVE FOR MERGE\n\n# save golden record to temp\ngolden_record_df.to_parquet(\"temp/golden_record.parquet\")\n\n\n## DEPENDENCY #TODO REMOVE FOR MERGE\n\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ngolden_record_df = pd.read_parquet('temp/golden_record.parquet')\n\nnp.random.seed(1337)\nrandom.seed(1337)"
  },
  {
    "objectID": "main.html#pipeline-for-training-and-evaluation",
    "href": "main.html#pipeline-for-training-and-evaluation",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "11.1 Pipeline for Training and Evaluation",
    "text": "11.1 Pipeline for Training and Evaluation\nThe train_evaluate_model function is designed to streamline the process of training and evaluating machine learning models. It performs the following steps:\n\nPreprocessing: The function automatically handles numerical and categorical features, imputing missing values, scaling numerical features, and one-hot encoding categorical features.\nModel Training: The specified model is trained on the training data.\nCross-Validation: The model is evaluated using cross-validation with specified evaluation metrics.\nModel Evaluation: The model is evaluated on the test set using various metrics, including accuracy, F1 score, AUC-ROC, precision, and recall.\n\nThe pipeline is flexible and can accommodate various models and feature sets, making it a versatile tool for model development and evaluation. It returns a summary of evaluation metrics for both training and test sets, as well as the true labels and predicted probabilities for the test set.\n\nfrom sklearn.feature_selection import RFE\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score\nimport scikitplot as skplt\nimport dalex as dx\n\n\nclass Trainer:\n    def __init__(self, data_module, model, cv=10, verbose=False):\n        self.data_module = data_module\n        self.model = model\n        self.cv = cv\n        self.verbose = verbose\n        self.preprocessor = self._create_preprocessor()\n        self.model_pipeline = None\n        self.train_metrics_report = None\n        self.test_metrics_report = None\n\n    def _create_preprocessor(self):\n        numerical_features = [col for col in self.data_module.X_train.columns if\n                              self.data_module.X_train[col].dtype in [\"int64\", \"float64\"]]\n        categorical_features = [col for col in self.data_module.X_train.columns if col not in numerical_features]\n\n        other_features = [col for col in self.data_module.X_train.columns if\n                          col not in numerical_features + categorical_features]\n        if len(other_features) &gt; 0:\n            raise ValueError(f\"Columns with unsupported data types found: {other_features}\")\n\n        numerical_pipeline = Pipeline(\n            [(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n        )\n\n        categorical_pipeline = Pipeline(\n            [\n                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n            ]\n        )\n\n        return ColumnTransformer(\n            transformers=[\n                (\"num\", numerical_pipeline, numerical_features),\n                (\"cat\", categorical_pipeline, categorical_features),\n            ]\n        )\n\n    def fit(self):\n        model_pipeline = Pipeline([(\"model\", self.model)])\n        self.model_pipeline = Pipeline([\n            (\"preprocessor\", self.preprocessor),\n            (\"model_pipeline\", model_pipeline)\n        ])\n        self.model_pipeline.fit(self.data_module.X_train, self.data_module.y_train)\n        return self\n    \n    @staticmethod\n    def get_scoring_metrics():\n        return [\"accuracy\", \"f1_macro\", \"roc_auc\", \"precision\", \"recall\"]\n\n    def eval_train(self):\n        scoring = {\n            \"accuracy\": \"accuracy\",\n            \"f1_macro\": make_scorer(f1_score),\n            \"roc_auc\": \"roc_auc\",\n            \"precision\": make_scorer(precision_score),\n            \"recall\": make_scorer(recall_score),\n        }\n        \n        cv_results = cross_validate(\n            self.model_pipeline, self.data_module.X_train, self.data_module.y_train, scoring=scoring, cv=self.cv,\n            return_train_score=False, n_jobs=-1, verbose=3 if self.verbose else 0,\n            return_estimator=True, return_indices=True,\n            error_score=\"raise\"\n        )\n        \n        self.train_metrics_report = {\n            metric: {\n                \"folds\": cv_results[f\"test_{metric}\"].tolist(),\n                \"mean\": cv_results[f\"test_{metric}\"].mean(),\n                \"std\": cv_results[f\"test_{metric}\"].std()\n            } for metric in scoring\n        }\n        \n        print(cv_results['indices'].keys())\n        \n        roc_data = []\n        for i in range(self.cv):\n            estimator = cv_results['estimator'][i]\n            train_indices, test_indices = (cv_results['indices'][\"train\"][i], \n                                           cv_results['indices'][\"test\"][i])\n            \n            true_labels = self.data_module.y_train.iloc[test_indices]\n            \n            y_pred_proba = estimator.predict_proba(self.data_module.X_train.iloc[test_indices])[:, 1]\n            roc_data.append((true_labels, y_pred_proba))\n            \n        self.train_metrics_report[\"roc_data\"] = roc_data\n            \n        return self\n\n    def eval_test(self):\n        X_test, y_test = self.data_module.X_test, self.data_module.y_test\n        y_pred_proba = self.model_pipeline.predict_proba(X_test)[:, 1] if hasattr(self.model_pipeline, \"predict_proba\") else np.nan\n        test_metrics = {\n            \"accuracy\": self.model_pipeline.score(X_test, y_test),\n            \"f1_macro\": f1_score(y_test, self.model_pipeline.predict(X_test), average=\"macro\"),\n            \"roc_auc\": roc_auc_score(y_test, y_pred_proba) if hasattr(self.model_pipeline, \"predict_proba\") else np.nan,\n            \"precision\": precision_score(y_test, self.model_pipeline.predict(X_test)),\n            \"recall\": recall_score(y_test, self.model_pipeline.predict(X_test))\n        }\n        self.test_metrics_report = {metric: test_metrics[metric] for metric in test_metrics}\n\n        return self\n\n    def get_trained_model(self):\n        return self.model_pipeline\n\n    def get_preprocessor(self):\n        return self.preprocessor\n\n    def get_train_metrics_report(self):\n        return self.train_metrics_report\n\n    def get_test_metrics_report(self):\n        return self.test_metrics_report\n    \n    def perform_feature_selection(self, n_features_to_select=None, step=1, verbose=0):\n        if self.model_pipeline is None:\n            raise ValueError(\"The trainer model is not fitted. Call the 'fit' method before performing feature selection.\")\n\n        rfe_pipeline = Pipeline([\n            (\"preprocessor\", self.preprocessor),\n            (\"rfe\", RFE(estimator=self.model, n_features_to_select=n_features_to_select, step=step, verbose=verbose))\n        ])\n        \n        rfe_pipeline.fit(self.data_module.X_train, self.data_module.y_train)\n        rfe = rfe_pipeline.named_steps[\"rfe\"]\n        \n        selected_feature_indices = rfe.get_support(indices=True)\n        \n        numerical_features = self.preprocessor.transformers_[0][2]\n        categorical_features = self.preprocessor.transformers_[1][2]\n        \n        onehot_encoder = self.preprocessor.transformers_[1][1].named_steps[\"onehot\"]\n        onehot_feature_names = onehot_encoder.get_feature_names_out(categorical_features)\n        \n        all_features = numerical_features + list(onehot_feature_names)\n        selected_features = [all_features[i] for i in selected_feature_indices]\n        return selected_features, rfe\n\nThe following class handles the visualization of the model evaluation results. It provides various plots and metrics to assess the model’s performance and interpretability. The class can be used to compare multiple models and visualize their evaluation metrics side by side or individually. There is a distinction made between training and test metrics to ensure a comprehensive evaluation of the model’s performance.\n\nfrom sklearn.metrics import roc_curve, classification_report, precision_recall_curve\n\n\nclass Visualizer:\n    def __init__(self, trainer, model_name):\n        self.trainer = trainer\n        self.model_name = model_name\n\n        X_train, X_test, y_train, y_test = (\n            self.trainer.data_module.X_train,\n            self.trainer.data_module.X_test,\n            self.trainer.data_module.y_train,\n            self.trainer.data_module.y_test)\n\n        self.explainer = dx.Explainer(trainer.get_trained_model(), X_test, y_test)\n\n        self.X_test = X_test\n        self.y_true = y_test\n        self.y_test_pred_proba = (trainer.get_trained_model()\n                                  .predict_proba(X_test))\n\n    @staticmethod\n    def compare_evaluation_metrics(visualizers):\n        metrics = ['accuracy', 'f1_macro', 'roc_auc', 'precision', 'recall']\n        model_names = [viz.model_name for viz in visualizers]\n\n        means = {metric: [] for metric in metrics}\n        stds = {metric: [] for metric in metrics}\n        for viz in visualizers:\n            train_metrics = viz.trainer.get_train_metrics_report()\n            for metric in metrics:\n                means[metric].append(np.mean(train_metrics[metric]['folds']))\n                stds[metric].append(np.std(train_metrics[metric]['folds']))\n\n        n_groups = len(metrics)\n        bar_width = 0.15\n        index = np.arange(n_groups)\n        opacity = 0.8\n        \n        plt.figure(figsize=(15, 8))\n        colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n\n        for i, model_name in enumerate(model_names):\n            bar_positions = index + bar_width * i\n            bar_values = [means[metric][i] for metric in metrics]\n            error_values = [stds[metric][i] for metric in metrics]\n\n            bars = plt.bar(bar_positions, bar_values, bar_width, alpha=opacity, color=colors[i],\n                           yerr=error_values, capsize=5, label=model_name)\n\n            for bar, error in zip(bars, error_values):\n                yval = bar.get_height()\n                text_position = yval + error + 0.02\n                plt.text(bar.get_x() + bar.get_width() / 2, text_position, f'{yval:.2f}',\n                         ha='center', va='bottom', fontsize=10)\n\n        plt.xlabel('Metrics', fontsize=14)\n        plt.ylabel('Scores', fontsize=14)\n        plt.title(f'Cross-Validation (k={visualizers[0].trainer.cv}) Evaluation Metrics Comparison', fontsize=16)\n        plt.xticks(index + bar_width * (len(model_names) - 1) / 2, metrics, fontsize=12)\n        plt.ylim(0, 1.1)\n        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n\n        plt.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey')\n        plt.tight_layout()\n        plt.show()\n\n    @staticmethod\n    def compare_roc_curves(visualizers, dataset='test'):\n        if dataset not in ['test', 'train']:\n            raise ValueError(\"Invalid dataset option. Choose 'test' or 'train'.\")\n        \n        plt.figure(figsize=(8, 8))\n        colors = plt.cm.viridis(np.linspace(0, 1, len(visualizers)))\n\n        for i, viz in enumerate(visualizers):\n            if dataset == 'test':\n                y_true = viz.trainer.data_module.y_test\n                y_scores = viz.trainer.get_trained_model().predict_proba(viz.trainer.data_module.X_test)[:, 1]\n            elif dataset == 'train':\n                y_true = []\n                y_scores = []\n                for fold in viz.trainer.get_train_metrics_report()['roc_data']:\n                    y_true.extend(fold[0])\n                    y_scores.extend(fold[1])\n\n            fpr, tpr, _ = roc_curve(y_true, y_scores)\n            auc_score = roc_auc_score(y_true, y_scores)\n            plt.plot(fpr, tpr, label=f\"{viz.model_name} (AUC = {auc_score:.2f})\", color=colors[i])\n\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC Curve Comparison on {dataset.capitalize()} Set')\n        plt.legend(loc='lower right')\n        plt.show()\n\n    def plot_validation_metrics(self):\n        train_metrics = self.trainer.get_train_metrics_report()\n        cv = len(train_metrics['accuracy']['folds'])\n\n        metrics = self.trainer.get_scoring_metrics()\n        fold_scores = {metric: train_metrics[metric]['folds'] for metric in metrics}\n\n        plt.boxplot(fold_scores.values(), labels=metrics, notch=True, patch_artist=True)\n        plt.title(f'{self.model_name}: Validation Metrics Box Plot (CV={cv})')\n        plt.xlabel('Metrics')\n        plt.ylabel('Score')\n        plt.ylim(0, 1)\n        plt.grid(True)\n        plt.show()\n\n    def plot_test_metrics(self):\n        test_metrics = self.trainer.get_test_metrics_report()\n        test_values = list(test_metrics.values())\n        test_names = list(test_metrics.keys())\n\n        sns.barplot(x=test_names, y=test_values)\n        plt.title(f'{self.model_name}: Test Metrics')\n        plt.xlabel('Metrics')\n        plt.ylabel('Score')\n        for i, v in enumerate(test_values):\n            if np.isnan(v):\n                plt.text(i, 0.5, \"N/A\", ha='center', va='bottom')\n            else:\n                plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center', va='bottom')\n        plt.ylim(0, 1)\n        plt.grid(True)\n        plt.show()\n\n    def plot_confusion_matrix_test(self):\n        preds = self.y_test_pred_proba.argmax(axis=1)\n        skplt.metrics.plot_confusion_matrix(self.y_true, preds)\n        plt.title(f'{self.model_name}: Confusion Matrix')\n        plt.show()\n\n    def plot_classification_report_test(self):\n        preds = self.y_test_pred_proba.argmax(axis=1)\n        report = classification_report(self.y_true, preds, output_dict=True)\n\n        report_df = pd.DataFrame(report).transpose()\n        report_df = report_df.round(2)\n\n        table = plt.table(cellText=report_df.values, colLabels=report_df.columns, rowLabels=report_df.index,\n                          cellLoc='center', rowLoc='center', loc='center', fontsize=12)\n        table.auto_set_font_size(False)\n        table.set_fontsize(12)\n        table.scale(1.2, 1.2)\n\n        plt.axis('off')\n        plt.title(f'{self.model_name}: Classification Report')\n        plt.show()\n\n    def plot_threshold_optimization_test(self):\n        precision, recall, thresholds = precision_recall_curve(self.y_true, self.y_test_pred_proba[:, 1])\n        f1_scores = 2 * (precision * recall) / (precision + recall)\n        optimal_idx = np.argmax(f1_scores)\n        optimal_threshold = thresholds[optimal_idx]\n\n        plt.plot(thresholds, f1_scores[:-1], label='F1-score')\n        plt.axvline(x=optimal_threshold, color='red', linestyle='--',\n                    label=f'Optimal Threshold: {optimal_threshold:.2f}')\n        plt.title(f'{self.model_name}: Threshold Optimization')\n        plt.xlabel('Threshold')\n        plt.ylabel('F1-score')\n        plt.legend()\n        plt.show()\n\n    def plot_roc_curve_test(self):\n        skplt.metrics.plot_roc(self.y_true, self.y_test_pred_proba)\n        plt.title(f'{self.model_name}: ROC Curve on Test Set')\n        plt.show()\n\n    def plot_precision_recall_curve_test(self):\n        skplt.metrics.plot_precision_recall(self.y_true, self.y_test_pred_proba)\n        plt.title(f'{self.model_name}: Precision-Recall Curve on Test Set')\n        plt.show()\n\n    def plot_lift_curve_test(self):\n        skplt.metrics.plot_lift_curve(self.y_true, self.y_test_pred_proba)\n        plt.title(f'{self.model_name}: Lift Curve on Test Set')\n        plt.show()\n\n    def plot_cumulative_gain_curve_test(self):\n        skplt.metrics.plot_cumulative_gain(self.y_true, self.y_test_pred_proba)\n        plt.title(f'{self.model_name}: Cumulative Gain Curve on Test Set')\n        plt.show()\n\n    def plot_partial_dependence_test(self, feature):\n        pdp = self.explainer.model_profile(type='partial', variables=feature)\n        pdp.plot()\n\n    def plot_accumulated_local_effects_test(self, feature):\n        ale = self.explainer.model_profile(type='accumulated', variables=feature)\n        ale.plot()\n\n    def plot_breakdown_test(self, observation):\n        breakdown = self.explainer.predict_parts(observation, type='break_down')\n        breakdown.plot()\n\n    def plot_model_explanations_test(self):\n        feature_importance = self.explainer.model_parts()\n        feature_importance.plot()\n\n        model_profile = self.explainer.model_profile(type='partial')\n        model_profile.plot()\n\n    def visualize_explanations_test(self, feature_columns=[]):\n        self.plot_model_explanations()\n\n        if not feature_columns:\n            feature_columns = self.trainer.data_module.feature_columns[0]\n\n        self.plot_partial_dependence(feature_columns)\n        self.plot_accumulated_local_effects(feature_columns)\n\n        observation = self.trainer.data_module.X_test.iloc[0]\n        self.plot_breakdown(observation)\n\n        plt.show()"
  },
  {
    "objectID": "main.html#top-n-customer-selection",
    "href": "main.html#top-n-customer-selection",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "11.2 Top-N Customer Selection",
    "text": "11.2 Top-N Customer Selection\nAs the models are trained and evaluated, we can use them to generate a list of the top N customers who are most likely to not have a card. This list can be used by the marketing team to target potential customers for card acquisition campaigns.\n\ndef create_top_n_customers_list(model, data, feature_columns, n):\n    mandatory_columns = ['client_id', 'has_card']\n    \n    if not hasattr(model, 'predict_proba'):\n        raise ValueError(\"Model does not support probability predictions\")\n\n    if n &gt; len(data):\n        raise ValueError(\"N cannot be greater than the number of customers\")\n\n    if not all(col in data.columns for col in feature_columns):\n        raise ValueError(\"Feature columns not found in data\")\n    \n    if not all(col in data.columns for col in mandatory_columns):\n        raise ValueError(\"Mandatory columns not found in data: 'client_id', 'has_card'\")\n\n    data = data[data['has_card'] == 0] # Filter out customers who already have a card\n    \n    X = data[feature_columns]\n    probabilities = model.predict_proba(X)\n    probabilities = probabilities[:, 1]  # Probability of having a card (class 1). # This essentially gives the clients who should most likely have a card based on the model but don't have one.\n\n    results = pd.DataFrame({\n        'Client ID': data['client_id'],\n        'Probability': probabilities\n    })\n\n    results_sorted = results.sort_values(by='Probability', ascending=False).reset_index(drop=True)\n    top_n_results = results_sorted.head(n)\n\n    return top_n_results\n\ndef compare_top_n_lists(*lists, labels):\n    if len(lists) != len(labels):\n        raise ValueError(\"Each list must have a corresponding label\")\n    \n    if len(set([len(l) for l in lists])) != 1:\n        raise ValueError(\"All lists must have the same length\")\n\n    overlap_matrix = pd.DataFrame(0, index=labels, columns=labels)\n\n    for i, list1 in enumerate(lists):\n        set1 = set(list1['Client ID'])\n        for j, list2 in enumerate(lists):\n            set2 = set(list2['Client ID'])\n            overlap_matrix.iloc[i, j] = len(set1.intersection(set2))\n\n    overlap_matrix = overlap_matrix / len(lists[0])  # Normalize by the list length\n    return overlap_matrix\n\ndef visualize_overlap_matrix(overlap_matrix, title):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(overlap_matrix, annot=True, cmap=\"Blues\", cbar_kws={'label': 'Common Customers [%]'})\n    plt.title(title)\n    plt.ylabel('List from Model/Method')\n    plt.xlabel('List from Model/Method')\n    plt.xticks(ticks=np.arange(len(overlap_matrix.columns)) + 0.5, labels=overlap_matrix.columns, rotation=45, ha='right')\n    plt.yticks(ticks=np.arange(len(overlap_matrix.index)) + 0.5, labels=overlap_matrix.index, rotation=0)\n    plt.show()"
  },
  {
    "objectID": "main.html#baseline-model-logistic-regression",
    "href": "main.html#baseline-model-logistic-regression",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "11.3 Baseline Model: Logistic Regression",
    "text": "11.3 Baseline Model: Logistic Regression\n\nbaseline_feature_columns = [\n                               'age',\n                               'client_region'\n                           ] + [col for col in golden_record_df.columns if\n                                'M_' in col and ('_balance' in col or '_volume' in col)]\n\nbaseline_data_module = create_data_module(golden_record_df, baseline_feature_columns)\n\nprint(f\"Number of baseline feature columns: {len(baseline_feature_columns)}\")\nprint(f\"Baseline feature columns: {baseline_feature_columns}\")\n\nNumber of baseline feature columns: 26\nBaseline feature columns: ['age', 'client_region', 'M_2_volume', 'M_3_volume', 'M_4_volume', 'M_5_volume', 'M_6_volume', 'M_7_volume', 'M_8_volume', 'M_9_volume', 'M_10_volume', 'M_11_volume', 'M_12_volume', 'M_13_volume', 'M_2_balance', 'M_3_balance', 'M_4_balance', 'M_5_balance', 'M_6_balance', 'M_7_balance', 'M_8_balance', 'M_9_balance', 'M_10_balance', 'M_11_balance', 'M_12_balance', 'M_13_balance']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nbaseline_trainer = (Trainer(baseline_data_module,\n                            LogisticRegression(max_iter=10000))\n                    .fit().eval_train())\n\nbaseline_visualizer = Visualizer(baseline_trainer, \"Baseline Logistic Regression\")\nbaseline_visualizer.plot_validation_metrics()\n\ndict_keys(['train', 'test'])\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 26 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7fbd74e119e0&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0111, mean = 0.518, max = 1.0\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.971, mean = -0.0183, max = 0.803\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\nIn order to possibly improve the model performance, we will include more features in the training data. We will include all features except for the ones that are not relevant for the model training.\nAfter merging the transactional and non-transactional data, we have many columns that are unnecessary for model training. We will remove all columns containing card-related information, except for the has_card column. This decision stems from the fact that 50% of our dataset consists of cardholders and the other 50% consists of non-cardholders, which we matched with the cardholders. Therefore, the data in the non-target card-related columns come from the actual cardholders.\nAdditionally we will remove all columns that contain time-dependent information, such as dates and IDs, as they are not relevant for the model.\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.loc[:,\n                   ~golden_record_df.columns.str.contains(\"card\") | golden_record_df.columns.str.contains(\"has_card\")]\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} card-related columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(columns=[\"loan_granted_date\", \"birth_date\", \"account_created\"])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} time-dependent columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(\n    columns=[\"loan_account_id\", \"loan_loan_id\", \"order_account_id\", \"client_district_name\", \"disp_id\", \"account_id\",\n             \"account_district_name\"])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} ID columns. Now {len(golden_record_df.columns)} columns remain.\")\n\nnum_cols_before = len(golden_record_df.columns)\ngolden_record_df = golden_record_df.drop(columns=[col for col in golden_record_df.columns if \"std\" in col])\nprint(\n    f\"Removed {num_cols_before - len(golden_record_df.columns)} std columns. Now {len(golden_record_df.columns)} columns remain.\")\n\ncols_to_exclude_in_train = [\"client_id\", \"has_card\"]\nall_cols_data_module = create_data_module(golden_record_df,\n                                          golden_record_df.drop(columns=cols_to_exclude_in_train).columns)\n\nRemoved 6 card-related columns. Now 226 columns remain.\nRemoved 3 time-dependent columns. Now 223 columns remain.\nRemoved 7 ID columns. Now 216 columns remain.\nRemoved 12 std columns. Now 204 columns remain.\n\n\n\nfor col in golden_record_df.columns:\n    print(col)\n\naccount_district_id\naccount_frequency\naccount_region\naccount_inhabitants\naccount_small_municipalities\naccount_medium_municipalities\naccount_large_municipalities\naccount_huge_municipalities\naccount_cities\naccount_ratio_urban_inhabitants\naccount_average_salary\naccount_unemployment_rate_1995\naccount_unemployment_rate_1996\naccount_entrepreneurs_per_1000_inhabitants\naccount_crimes_committed_1995\naccount_crimes_committed_1996\nclient_id\ntype\nclient_district_id\nsex\nage\nclient_region\nclient_inhabitants\nclient_small_municipalities\nclient_medium_municipalities\nclient_large_municipalities\nclient_huge_municipalities\nclient_cities\nclient_ratio_urban_inhabitants\nclient_average_salary\nclient_unemployment_rate_1995\nclient_unemployment_rate_1996\nclient_entrepreneurs_per_1000_inhabitants\nclient_crimes_committed_1995\nclient_crimes_committed_1996\norder_k_symbol_debited_sum_household\norder_k_symbol_debited_sum_insurance_payment\norder_k_symbol_debited_sum_leasing\norder_k_symbol_debited_sum_loan_payment\norder_k_symbol_debited_sum_na\nloan_amount\nloan_duration\nloan_monthly_payments\nloan_status\nhas_card\nclient_tenure_years_relative\nage_group\nloan_status_simplified\nM_2_volume\nM_3_volume\nM_4_volume\nM_5_volume\nM_6_volume\nM_7_volume\nM_8_volume\nM_9_volume\nM_10_volume\nM_11_volume\nM_12_volume\nM_13_volume\nM_2_total_abs_amount\nM_3_total_abs_amount\nM_4_total_abs_amount\nM_5_total_abs_amount\nM_6_total_abs_amount\nM_7_total_abs_amount\nM_8_total_abs_amount\nM_9_total_abs_amount\nM_10_total_abs_amount\nM_11_total_abs_amount\nM_12_total_abs_amount\nM_13_total_abs_amount\nM_2_transaction_count\nM_3_transaction_count\nM_4_transaction_count\nM_5_transaction_count\nM_6_transaction_count\nM_7_transaction_count\nM_8_transaction_count\nM_9_transaction_count\nM_10_transaction_count\nM_11_transaction_count\nM_12_transaction_count\nM_13_transaction_count\nM_2_positive_transaction_count\nM_3_positive_transaction_count\nM_4_positive_transaction_count\nM_5_positive_transaction_count\nM_6_positive_transaction_count\nM_7_positive_transaction_count\nM_8_positive_transaction_count\nM_9_positive_transaction_count\nM_10_positive_transaction_count\nM_11_positive_transaction_count\nM_12_positive_transaction_count\nM_13_positive_transaction_count\nM_2_negative_transaction_count\nM_3_negative_transaction_count\nM_4_negative_transaction_count\nM_5_negative_transaction_count\nM_6_negative_transaction_count\nM_7_negative_transaction_count\nM_8_negative_transaction_count\nM_9_negative_transaction_count\nM_10_negative_transaction_count\nM_11_negative_transaction_count\nM_12_negative_transaction_count\nM_13_negative_transaction_count\nM_2_average_amount\nM_3_average_amount\nM_4_average_amount\nM_5_average_amount\nM_6_average_amount\nM_7_average_amount\nM_8_average_amount\nM_9_average_amount\nM_10_average_amount\nM_11_average_amount\nM_12_average_amount\nM_13_average_amount\nM_2_median_amount\nM_3_median_amount\nM_4_median_amount\nM_5_median_amount\nM_6_median_amount\nM_7_median_amount\nM_8_median_amount\nM_9_median_amount\nM_10_median_amount\nM_11_median_amount\nM_12_median_amount\nM_13_median_amount\nM_2_min_amount\nM_3_min_amount\nM_4_min_amount\nM_5_min_amount\nM_6_min_amount\nM_7_min_amount\nM_8_min_amount\nM_9_min_amount\nM_10_min_amount\nM_11_min_amount\nM_12_min_amount\nM_13_min_amount\nM_2_max_amount\nM_3_max_amount\nM_4_max_amount\nM_5_max_amount\nM_6_max_amount\nM_7_max_amount\nM_8_max_amount\nM_9_max_amount\nM_10_max_amount\nM_11_max_amount\nM_12_max_amount\nM_13_max_amount\nM_2_type_count\nM_3_type_count\nM_4_type_count\nM_5_type_count\nM_6_type_count\nM_7_type_count\nM_8_type_count\nM_9_type_count\nM_10_type_count\nM_11_type_count\nM_12_type_count\nM_13_type_count\nM_2_operation_count\nM_3_operation_count\nM_4_operation_count\nM_5_operation_count\nM_6_operation_count\nM_7_operation_count\nM_8_operation_count\nM_9_operation_count\nM_10_operation_count\nM_11_operation_count\nM_12_operation_count\nM_13_operation_count\nM_2_k_symbol_count\nM_3_k_symbol_count\nM_4_k_symbol_count\nM_5_k_symbol_count\nM_6_k_symbol_count\nM_7_k_symbol_count\nM_8_k_symbol_count\nM_9_k_symbol_count\nM_10_k_symbol_count\nM_11_k_symbol_count\nM_12_k_symbol_count\nM_13_k_symbol_count\nM_2_balance\nM_3_balance\nM_4_balance\nM_5_balance\nM_6_balance\nM_7_balance\nM_8_balance\nM_9_balance\nM_10_balance\nM_11_balance\nM_12_balance\nM_13_balance"
  },
  {
    "objectID": "main.html#candidate-models",
    "href": "main.html#candidate-models",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "11.4 Candidate Models",
    "text": "11.4 Candidate Models\n\n11.4.1 Logistic Regression\nWe will train a logistic regression model with the new feature set and evaluate its performance as it already showed promising results in the baseline model.\n\nlog_reg_trainer = (Trainer(all_cols_data_module,\n                           LogisticRegression(max_iter=10000))\n                   .fit().eval_train())\n\nlog_reg_visualizer = Visualizer(log_reg_trainer, \"Logistic Regression\")\nlog_reg_visualizer.plot_validation_metrics()\n\ndict_keys(['train', 'test'])\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7fbd74e119e0&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 8.4e-06, mean = 0.479, max = 1.0\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -1.0, mean = 0.0214, max = 0.981\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\nlog_reg_trainer.perform_feature_selection(n_features_to_select=10)\n\n(['M_5_total_abs_amount',\n  'M_3_k_symbol_count',\n  'M_11_k_symbol_count',\n  'M_2_balance',\n  'M_3_balance',\n  'M_6_balance',\n  'account_frequency_MONTHLY_ISSUANCE',\n  'client_region_north Bohemia',\n  'age_group_40-55',\n  'age_group_55-70'],\n RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=10))\n\n\n\n\n11.4.2 Random Forest\nWe will also train a Random Forest model to see if it can outperform the logistic regression model. Random Forest models are known for their robustness and ability to capture complex relationships in the data.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_trainer = (Trainer(all_cols_data_module, RandomForestClassifier())\n              .fit()\n              .eval_train())\n\nrf_visualizer = Visualizer(rf_trainer, \"Random Forest\")\nrf_visualizer.plot_validation_metrics()\n\ndict_keys(['train', 'test'])\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7fbd74e119e0&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0, mean = 0.489, max = 0.98\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.86, mean = 0.0111, max = 0.74\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\n\n11.4.3 Decision Tree\nWe will also train a Decision Tree model to see how it performs compared to the other models. Decision Trees are known for their interpretability and simplicity.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_trainer = (Trainer(all_cols_data_module, DecisionTreeClassifier())\n                         .fit()\n                         .eval_train())\n\ndecision_tree_visualizer = Visualizer(decision_tree_trainer, \"Decision Tree\")\ndecision_tree_visualizer.plot_validation_metrics()\n\ndict_keys(['train', 'test'])\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7fbd74e119e0&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.0, mean = 0.477, max = 1.0\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -1.0, mean = 0.0227, max = 1.0\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!\n\n\n\n\n\n\n\n\n\n\n\n11.4.4 Gradient Boosting\nFinally, we will train a Gradient Boosting model to see if it can outperform the other models. Gradient Boosting models are known for their high accuracy and ability to capture complex relationships in the data.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost_trainer = (Trainer(all_cols_data_module, GradientBoostingClassifier())\n                          .fit()\n                          .eval_train())\n\ngradient_boost_visualizer = Visualizer(gradient_boost_trainer, \"Gradient Boosting\")\ngradient_boost_visualizer.plot_validation_metrics()\n\ndict_keys(['train', 'test'])\nPreparation of a new explainer is initiated\n\n  -&gt; data              : 132 rows 202 cols\n  -&gt; target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n  -&gt; target variable   : 132 values\n  -&gt; model_class       : sklearn.pipeline.Pipeline (default)\n  -&gt; label             : Not specified, model's class short name will be used. (default)\n  -&gt; predict function  : &lt;function yhat_proba_default at 0x7fbd74e119e0&gt; will be used (default)\n  -&gt; predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n  -&gt; predicted values  : min = 0.00355, mean = 0.5, max = 0.985\n  -&gt; model type        : classification will be used (default)\n  -&gt; residual function : difference between y and yhat (default)\n  -&gt; residuals         : min = -0.985, mean = 0.000161, max = 0.875\n  -&gt; model_info        : package sklearn\n\nA new explainer has been created!"
  },
  {
    "objectID": "main.html#selected-model-logistic-regression",
    "href": "main.html#selected-model-logistic-regression",
    "title": "AML Mini-Challenge - Credit Card Affinity Modelling",
    "section": "12.1 Selected Model: Logistic Regression",
    "text": "12.1 Selected Model: Logistic Regression\n\nbest_model_trainer = log_reg_trainer\nbest_model_visualizer = log_reg_visualizer\n\n\nbest_model_trainer.eval_test()\nbest_model_visualizer.plot_test_metrics()\n\n\n\n\n\n\n\n\n\nbest_model_visualizer.plot_roc_curve_test()\n\n\n\n\n\n\n\n\n\n_, _ = best_model_visualizer.plot_confusion_matrix_test(), best_model_visualizer.plot_classification_report_test()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop_n_customers = create_top_n_customers_list(best_model_trainer.get_trained_model(), \n                                              golden_record_df, \n                                              all_cols_data_module.feature_columns, 10)\ntop_n_customers\n\n\n\n\n\n\n\n\nClient ID\nProbability\n\n\n\n\n0\n8327\n0.999948\n\n\n1\n2438\n0.998397\n\n\n2\n1408\n0.997929\n\n\n3\n2071\n0.997545\n\n\n4\n598\n0.974920\n\n\n5\n2088\n0.962497\n\n\n6\n1458\n0.917788\n\n\n7\n4287\n0.885707\n\n\n8\n484\n0.840727\n\n\n9\n4279\n0.838447\n\n\n\n\n\n\n\n\noverlap_matrix = compare_top_n_lists(top_n_customers, top_n_customers, labels=[\"Model 1\", \"Model 2\"])\nvisualize_overlap_matrix(overlap_matrix, \"Overlap Matrix of Top-N Customer Lists\")"
  }
]
{
 "cells": [
  {
   "cell_type": "code",
   "id": "9a899a85",
   "metadata": {},
   "source": [
    "## DEPENDENCY #TODO REMOVE FOR MERGE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "golden_record_df = pd.read_parquet('temp/golden_record.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3e2547f",
   "metadata": {},
   "source": [
    "# Data Partitioning\n",
    "\n",
    "The data is split in a 80/20 ratio for training and testing purposes. The stratification ensures that the distribution of the target variable is maintained in both sets. When actually training the models, we will additionally use cross-validation to ensure robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "id": "700dcea5",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(golden_record_df,\n",
    "                                     test_size=0.2,\n",
    "                                     random_state=1337,\n",
    "                                     stratify=golden_record_df['has_card'],\n",
    "                                     shuffle=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
    "print(f\"Train set distribution:\\n{train_df['has_card'].value_counts(normalize=True)}\")\n",
    "print(f\"Test set distribution:\\n{test_df['has_card'].value_counts(normalize=True)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e5ee73c",
   "metadata": {},
   "source": [
    "As we can see the distribution of the target variable is maintained in both sets after the split.\n",
    "\n",
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdbe6a25",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def train_evaluate_model(\n",
    "        train_df, test_df, feature_columns, model, target_column=\"has_card\", cv=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a given model based on specified feature columns using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df: DataFrame containing the training data.\n",
    "    - test_df: DataFrame containing the test data.\n",
    "    - feature_columns: List of column names to be used as features.\n",
    "    - model: The machine learning model to be trained and evaluated.\n",
    "    - target_column: Name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_report: Summary of evaluation metrics including mean and standard deviation for accuracy, F1, AUC-ROC, precision, recall.\n",
    "    - y_test: True labels for the test set.\n",
    "    - y_pred_proba: Predicted probabilities for the test set.\n",
    "    \"\"\"\n",
    "    numerical_features = [\n",
    "        col\n",
    "        for col in feature_columns\n",
    "        if train_df[col].dtype in [\"int64\", \"float64\"]\n",
    "    ]\n",
    "    categorical_features = [\n",
    "        col for col in feature_columns if train_df[col].dtype == \"object\"\n",
    "    ]\n",
    "\n",
    "    numerical_pipeline = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_pipeline, numerical_features),\n",
    "            (\"cat\", categorical_pipeline, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "    scoring = {\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"f1\": make_scorer(f1_score),\n",
    "        \"roc_auc\": \"roc_auc\",\n",
    "        \"precision\": make_scorer(precision_score),\n",
    "        \"recall\": make_scorer(recall_score),\n",
    "    }\n",
    "\n",
    "    X_train, y_train = train_df[feature_columns], train_df[target_column]\n",
    "    X_test, y_test = test_df[feature_columns], test_df[target_column]\n",
    "\n",
    "    train_metrics_summary = cross_validate(\n",
    "        pipeline, X_train, y_train, scoring=scoring, cv=cv, return_train_score=False\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = pipeline.predict_proba(X_test) if hasattr(pipeline, \"predict_proba\") else np.nan\n",
    "\n",
    "    test_metrics = {\n",
    "        \"accuracy\": pipeline.score(X_test, y_test),\n",
    "        \"f1\": f1_score(y_test, pipeline.predict(X_test)),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba[:, 1]) if hasattr(pipeline, \"predict_proba\") else np.nan,\n",
    "        \"precision\": precision_score(y_test, pipeline.predict(X_test)),\n",
    "        \"recall\": recall_score(y_test, pipeline.predict(X_test))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"train\": {metric: {\"folds\": train_metrics_summary[f\"test_{metric}\"].tolist(), \"mean\": train_metrics_summary[f\"test_{metric}\"].mean(), \"std\": train_metrics_summary[f\"test_{metric}\"].std()} for metric in scoring},\n",
    "        \"test\": {metric: test_metrics[metric] for metric in scoring}\n",
    "    }, y_test, y_pred_proba\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87342d21",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "feature_columns = [\n",
    "                      'age',\n",
    "                      'client_region'\n",
    "                  ] + [col for col in golden_record_df.columns if 'M_' in col and ('_balance' in col or '_volume' in col)]\n",
    "\n",
    "train_evaluate_model(train_df, test_df, feature_columns, model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3491f2ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "def visualize_results(metrics_report, model_name, y_true, y_pred_proba, cv=10):\n",
    "    \"\"\"\n",
    "    Visualizes the results from the given metrics report dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_report: Dictionary containing training and test metrics.\n",
    "    - model_name: Name of the model.\n",
    "    - y_true: True labels.\n",
    "    - y_pred_proba: Predicted probabilities.\n",
    "    - cv: Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the plots.\n",
    "    \"\"\"\n",
    "    train_metrics = metrics_report['train']\n",
    "    test_metrics = metrics_report['test']\n",
    "\n",
    "    metrics = train_metrics.keys()\n",
    "\n",
    "    # Plot validation metrics with error bars\n",
    "    val_means = [train_metrics[metric]['mean'] for metric in metrics]\n",
    "    val_stds = [train_metrics[metric]['std'] for metric in metrics]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(metrics, val_means, yerr=val_stds, capsize=5, color='c', alpha=0.7)\n",
    "    plt.title(f'{model_name}: Validation Metrics with Error Bars (CV={cv})')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    for i, (mean, std) in enumerate(zip(val_means, val_stds)):\n",
    "        plt.text(i, mean + std + 0.01, f\"{mean:.2f} Â± {std:.2f}\", ha='center', va='bottom')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot test metrics\n",
    "    test_values = list(test_metrics.values())\n",
    "    test_names = list(test_metrics.keys())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=test_names, y=test_values)\n",
    "    plt.title(f'{model_name}: Test Metrics')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    for i, v in enumerate(test_values):\n",
    "        if np.isnan(v):\n",
    "            plt.text(i, 0.5, \"N/A\", ha='center', va='bottom')\n",
    "        else:\n",
    "            plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center', va='bottom')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Check if the model has predicted probabilities\n",
    "    if not np.isnan(y_pred_proba).all():\n",
    "        # Plot AUROC curve\n",
    "        skplt.metrics.plot_roc(y_true, y_pred_proba, figsize=(10, 6))\n",
    "        plt.title(f'{model_name}: ROC Curve')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot precision-recall curve\n",
    "        skplt.metrics.plot_precision_recall(y_true, y_pred_proba, figsize=(10, 6))\n",
    "        plt.title(f'{model_name}: Precision-Recall Curve')\n",
    "        plt.show()\n",
    "\n",
    "# Assuming train_evaluate_model and other necessary variables are defined\n",
    "metrics_report, y_test, y_pred_proba = train_evaluate_model(train_df, test_df, feature_columns, model)\n",
    "visualize_results(metrics_report, \"Logistic Regression\", y_test, y_pred_proba)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa106db5",
   "metadata": {},
   "source": [
    "## BAV\n",
    "\n",
    "# Model Engineering\n",
    "\n",
    "# Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "aeac38cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "metrics_report, y_test, y_pred_proba = train_evaluate_model(train_df, test_df, feature_columns, model)\n",
    "visualize_results(metrics_report, \"Random Forest\", y_test, y_pred_proba)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9b7e6f5",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42, max_depth=5)  # Limited depth for better interpretability\n",
    "metrics_report, y_test, y_pred_proba = train_evaluate_model(train_df, test_df, feature_columns, decision_tree_model)\n",
    "visualize_results(metrics_report, \"Decision Tree\", y_test, y_pred_proba)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e1bacac",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "# WORKBENCH - remove later\n",
    "\n",
    "### Some older input\n",
    "\n",
    "We need some categorical indicator wheter a transactions is a transactions incoming or outgoing from the perspective of the account holder. This will be important for the feature engineering later on. We will create a column called `transaction_direction` using the amount to engineer this feature.\n",
    "\n",
    "Balance is the wealth on the account after the transaction.\n",
    "\n",
    "k_symbol is the purpose of the transaction. This is often use in the context of budgeting in E-Banking applications or just personal finance management. A lot of NA values are present in this column. We will have to deal with this later on and weigh the importance of this column.\n",
    "\n",
    "Track the time series of a given account to get a better understanding of the datasets nature.\n",
    "\n",
    "It seems that there can be multiple transactions on the same day. We will have to aggregate the transactions on the same day to get a better understanding of the transactions as the timestamp resolution is not high enough to track the transactions on a daily basis.\n",
    "\n",
    "We need some handling for this as the ID is not informative as well (Dani).\n",
    "\n",
    "For the feature enginnering a per month evaluation of the transactions is sufficient (Dani).\n",
    "\n",
    "We need to make sure across the board that for the prediction we only use the data that is available at the time of the prediction. This means that we can only use the data from the past to predict the future. This is important to keep in mind when we engineer the features as some entities do not have any information about the date and therefore we cannot use them for the prediction as we cannot rule out that they are not from the future.\n",
    "\n",
    "Frequency analysis of the transactions could be interesting as the hypothesis might be that the more frequent the transactions the more likely the account holder is to be interested in a credit card. Fourier transformation could be used to get a better understanding of the frequency of the transactions.\n",
    "\n",
    "### JITT 05.03.24\n",
    "\n",
    "-   New customers are handled differently\n",
    "-   Customer without the required history should be ignored otherwise they are treated as irrelevant\n",
    "-   Lag is ignored like (12 + 1) months\n",
    "-   Age should be in relation to the time of the event (card issued / reference date for refrence clients)\n",
    "-   How old are customers with a Junior Card? This should be evaluated based on the data\n",
    "    -   Example with Junior Card model with Age as most important feature as a negative example\n",
    "-   Reference clients\n",
    "    -   They should not be as similar as possible (Twin brother problem)\n",
    "    -   Same external market conditions\n",
    "    -   Same environment\n",
    "    -   See slide 6\n",
    "-   Owner and disponents cannot be distinguished directly and assumptions are required\n",
    "    -   MasterCards vs Visa war: as much cards as possible for both client of an account\n",
    "    -   AGain the Twin brother problem as features are too similar possibly\n",
    "\n",
    "#### General notes to self\n",
    "\n",
    "-   Visualise monthly product puchases\n",
    "-   Viz environment of selected client and reference clients and answer the questions are they from a comparable environment"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

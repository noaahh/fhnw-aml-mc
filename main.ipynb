{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: AML Mini-Challenge - Credit Card Affinity Modelling\n",
        "author: Dominik Filliger & Noah Leuenberger (2024)\n",
        "---"
      ],
      "id": "48e0f3c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "data_reduction = OrderedDict()\n",
        "\n",
        "SEED = 1337\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "seed_everything(SEED)"
      ],
      "id": "17d31d47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Import & Wrangling\n",
        "\n",
        "## Helper Functions"
      ],
      "id": "6a28304b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def remap_values(df, column, mapping):\n",
        "    # assert that all values in the column are in the mapping except for NaN\n",
        "    assert df[column].dropna().isin(mapping.keys()).all()\n",
        "\n",
        "    df[column] = df[column].map(mapping, na_action=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def map_empty_to_nan(df, column):\n",
        "    if df[column].dtype != \"object\":\n",
        "        return df\n",
        "\n",
        "    df[column] = df[column].replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def read_csv(file_path, sep=\";\", dtypes=None):\n",
        "    df = pd.read_csv(file_path, sep=sep, dtype=dtypes)\n",
        "\n",
        "    for col in df.columns:\n",
        "        df = map_empty_to_nan(df, col)\n",
        "\n",
        "    return df"
      ],
      "id": "7e1169e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_categorical_variables(df, categorical_columns, fill_na_value=\"NA\", rotate_x=False):\n",
        "    \"\"\"\n",
        "    Plots count plots for categorical variables in a DataFrame, filling NA values with a specified string.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas.DataFrame containing the data.\n",
        "    - categorical_vars: list of strings, names of the categorical variables in df to plot.\n",
        "    - fill_na_value: string, the value to use for filling NA values in the categorical variables.\n",
        "    \"\"\"\n",
        "    # Fill NA values in the specified categorical variables\n",
        "    for var in categorical_columns:\n",
        "        if df[var].isna().any():\n",
        "            df[var] = df[var].fillna(fill_na_value)\n",
        "\n",
        "    total = float(len(df))\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=len(categorical_columns), figsize=(8, len(categorical_columns) * 5)\n",
        "    )\n",
        "\n",
        "    if len(categorical_columns) == 1:  # If there's only one categorical variable, wrap axes in a list\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, var in enumerate(categorical_columns):\n",
        "        ax = sns.countplot(\n",
        "            x=var, data=df, ax=axes[i], order=df[var].value_counts().index\n",
        "        )\n",
        "\n",
        "        axes[i].set_title(f\"Distribution of {var}\")\n",
        "        axes[i].set_ylabel(\"Count\")\n",
        "        axes[i].set_xlabel(var)\n",
        "\n",
        "        # if the number is more than 5 rotate the x labels or if specified\n",
        "        if len(df[var].value_counts()) > 5 or rotate_x:\n",
        "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            ax.text(\n",
        "                p.get_x() + p.get_width() / 2.0,\n",
        "                height + 3,\n",
        "                \"{:1.2f}%\".format((height / total) * 100),\n",
        "                ha=\"center\",\n",
        "            )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_numerical_distributions(df, numerical_columns, kde=True, bins=30):\n",
        "    \"\"\"\n",
        "    Plots the distribution of all numerical variables in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas.DataFrame containing the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine the number of rows needed for subplots based on the number of numerical variables\n",
        "    nrows = len(numerical_columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(8, 5 * nrows))\n",
        "\n",
        "    # If there's only one numerical variable, wrap axes in a list\n",
        "    if nrows == 1:  \n",
        "        axes = [axes]\n",
        "\n",
        "    for i, var in enumerate(numerical_columns):\n",
        "        sns.histplot(df[var], ax=axes[i], kde=kde, bins=bins)\n",
        "        axes[i].set_title(f\"Distribution of {var}\")\n",
        "        axes[i].set_xlabel(var)\n",
        "        axes[i].set_ylabel(\"Frequency\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_date_monthly_counts(df, date_column, title):\n",
        "    \"\"\"\n",
        "    Plots the monthly counts of a date column in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas.DataFrame containing the data.\n",
        "    - date_column: string, name of the date column in df to plot.\n",
        "    - title: string, title of the plot.\n",
        "    \"\"\"\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "    df[\"month\"] = df[date_column].dt.to_period(\"M\")\n",
        "\n",
        "    monthly_counts = df[\"month\"].value_counts().sort_index()\n",
        "    monthly_counts.plot(kind=\"bar\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def add_percentage_labels(ax, hue_order):\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        width = p.get_width()\n",
        "        x = p.get_x()\n",
        "        y = p.get_y()\n",
        "        label_text = f\"{height:.1f}%\"\n",
        "        label_x = x + width / 2\n",
        "        label_y = y + height / 2\n",
        "        ax.text(\n",
        "            label_x,\n",
        "            label_y,\n",
        "            label_text,\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=9,\n",
        "            color=\"white\",\n",
        "            weight=\"bold\"\n",
        "        )"
      ],
      "id": "615d63b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entities\n",
        "The following section will describe the individual entities in the dataset.\n",
        "\n",
        "Accordingly there is some simple remapping done to allow for easier understanding of the data. This can include translating the values from Czech to English or remapping the values to a more understandable format.\n",
        "\n",
        "See code comments for more details of how the data is transformed.\n",
        "\n",
        "### Accounts"
      ],
      "id": "e06d282a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accounts_df = read_csv(\"data/account.csv\")\n",
        "\n",
        "# translated frequency from Czech to English\n",
        "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "accounts_df = remap_values(\n",
        "    accounts_df,\n",
        "    \"frequency\",\n",
        "    {\n",
        "        \"POPLATEK MESICNE\": \"MONTHLY_ISSUANCE\",\n",
        "        \"POPLATEK TYDNE\": \"WEEKLY_ISSUANCE\",\n",
        "        \"POPLATEK PO OBRATU\": \"ISSUANCE_AFTER_TRANSACTION\",\n",
        "    },\n",
        ")\n",
        "\n",
        "accounts_df[\"date\"] = pd.to_datetime(accounts_df[\"date\"], format=\"%y%m%d\")\n",
        "\n",
        "accounts_df.rename(\n",
        "    columns={\"date\": \"account_created\", \"frequency\": \"account_frequency\"}, inplace=True\n",
        ")\n",
        "\n",
        "data_reduction[\"Total number of accounts\"] = len(accounts_df)\n",
        "accounts_df.info()"
      ],
      "id": "8a24ecbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accounts_df.head()"
      ],
      "id": "a30785dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accounts_df.nunique()"
      ],
      "id": "a99dd02d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(accounts_df, [\"account_frequency\"])"
      ],
      "id": "9f28bf41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(accounts_df, [\"account_created\"])"
      ],
      "id": "59962bab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clients"
      ],
      "id": "60218dbe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clients_df = read_csv(\"data/client.csv\")\n",
        "\n",
        "\n",
        "def parse_birth_number(birth_number):\n",
        "    birth_number_str = str(birth_number)\n",
        "\n",
        "    # extract year, month, and day from birth number from string\n",
        "    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "    year = int(birth_number_str[:2])\n",
        "    month = int(birth_number_str[2:4])\n",
        "    day = int(birth_number_str[4:6])\n",
        "\n",
        "    # determine sex based on month and adjust month for female clients\n",
        "    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "    if month > 50:\n",
        "        sex = \"Female\"\n",
        "        month -= 50\n",
        "    else:\n",
        "        sex = \"Male\"\n",
        "\n",
        "    # quick validation\n",
        "    assert 1 <= month <= 12\n",
        "    assert 1 <= day <= 31\n",
        "    assert 0 <= year <= 99\n",
        "\n",
        "    if month in [4, 6, 9, 11]:\n",
        "        assert 1 <= day <= 30\n",
        "    elif month == 2:\n",
        "        assert 1 <= day <= 29\n",
        "    else:\n",
        "        assert 1 <= day <= 31\n",
        "\n",
        "    # assuming all dates are in the 1900s\n",
        "    birth_date = datetime(1900 + year, month, day)\n",
        "    return pd.Series([sex, birth_date])\n",
        "\n",
        "\n",
        "clients_df[[\"sex\", \"birth_date\"]] = clients_df[\"birth_number\"].apply(parse_birth_number)\n",
        "\n",
        "# calculate 'age' assuming the reference year is 1999\n",
        "clients_df[\"age\"] = clients_df[\"birth_date\"].apply(lambda x: 1999 - x.year)\n",
        "\n",
        "# drop 'birth_number' column as it is no longer needed\n",
        "clients_df = clients_df.drop(columns=[\"birth_number\"])\n",
        "\n",
        "clients_df.info()"
      ],
      "id": "9a976cce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clients_df.head()"
      ],
      "id": "920839ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clients_df.describe()"
      ],
      "id": "8984e38e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(clients_df, [\"birth_date\", \"age\"], bins=40)"
      ],
      "id": "8ec8068e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the distribution of of birth dates and the age of the clients. We see that the majority of clients are between 20 and 60 years old. There are a few noticeable drops at around age 30, 50 and 70 years, which is not really of any concern. \n",
        "\n",
        "We also notice that there are a few underage clients.\n",
        "\n",
        "### Dispositions"
      ],
      "id": "46f16111"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dispositions_df = read_csv(\"data/disp.csv\")\n",
        "dispositions_df.info()"
      ],
      "id": "b939fbcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dispositions_df.head()"
      ],
      "id": "76d8515d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dispositions_df.describe()"
      ],
      "id": "536f210c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(dispositions_df, [\"type\"])"
      ],
      "id": "f3d9b372",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows the distribution of account types. We can see that roughly 4 out 5 accounts are categorized as \"OWNER\" accounts.\n",
        "\n",
        "As the goal of this model is to address accounts and not client directly we will focus on the clients which own an account and focus solely on them."
      ],
      "id": "69fb8eef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dispositions_df = dispositions_df[dispositions_df[\"type\"] == \"OWNER\"]"
      ],
      "id": "f5f107ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Orders"
      ],
      "id": "6c25a9fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_df = read_csv(\"data/order.csv\")\n",
        "\n",
        "# Translated from Czech to English\n",
        "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "orders_df = remap_values(\n",
        "    orders_df,\n",
        "    \"k_symbol\",\n",
        "    {\n",
        "        \"POJISTNE\": \"Insurance_Payment\",\n",
        "        \"SIPO\": \"Household\",\n",
        "        \"LEASING\": \"Leasing\",\n",
        "        \"UVER\": \"Loan_Payment\",\n",
        "    },\n",
        ")\n",
        "\n",
        "orders_df[\"account_to\"] = orders_df[\"account_to\"].astype(\"category\")\n",
        "\n",
        "orders_df = orders_df.rename(columns={\"amount\": \"debited_amount\"})\n",
        "\n",
        "orders_df.info()"
      ],
      "id": "56c03e42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_df.head()"
      ],
      "id": "ef0de5d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_df.describe()"
      ],
      "id": "43a153c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_df.nunique()"
      ],
      "id": "aa20237a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There appear to be as many order ids as there are rows. This implies that each order is unique which makes sense."
      ],
      "id": "66099634"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(orders_df, [\"k_symbol\", \"bank_to\"])"
      ],
      "id": "3cccb954",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking a closer look at the distribution of type of order (k_symbol) and receiving bank (bank_to) we can see that the majority of orders are Household orders and NA. \n",
        "\n",
        "We also see that the receiving bank is somewhat equally distributed, meaning that the orders are not concentrated on a single bank."
      ],
      "id": "952570c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(orders_df, [\"debited_amount\"], bins=80)"
      ],
      "id": "5b21b53f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows that the debited amount are primarily between 0 and 4000. From there on we can see a steady decline up to around 15000.\n",
        "\n",
        "### Transactions"
      ],
      "id": "a03f438a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# column 8 is the 'bank' column which contains NaNs and must be read as string\n",
        "transactions_df = read_csv(\"data/trans.csv\", dtypes={8: str})\n",
        "\n",
        "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"], format=\"%y%m%d\")\n",
        "\n",
        "# Translated type, operations and characteristics from Czech to English\n",
        "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "transactions_df = remap_values(\n",
        "    transactions_df,\n",
        "    \"type\",\n",
        "    {\n",
        "        \"VYBER\": \"Withdrawal\",  # Also withdrawal as it is against the documentation present in the dataset\n",
        "        \"PRIJEM\": \"Credit\",\n",
        "        \"VYDAJ\": \"Withdrawal\",\n",
        "    },\n",
        ")\n",
        "\n",
        "transactions_df = remap_values(\n",
        "    transactions_df,\n",
        "    \"operation\",\n",
        "    {\n",
        "        \"VYBER KARTOU\": \"Credit Card Withdrawal\",\n",
        "        \"VKLAD\": \"Credit in Cash\",\n",
        "        \"PREVOD Z UCTU\": \"Collection from Another Bank\",\n",
        "        \"VYBER\": \"Withdrawal in Cash\",\n",
        "        \"PREVOD NA UCET\": \"Remittance to Another Bank\",\n",
        "    },\n",
        ")\n",
        "\n",
        "transactions_df = remap_values(\n",
        "    transactions_df,\n",
        "    \"k_symbol\",\n",
        "    {\n",
        "        \"POJISTNE\": \"Insurance Payment\",\n",
        "        \"SLUZBY\": \"Payment on Statement\",\n",
        "        \"UROK\": \"Interest Credited\",\n",
        "        \"SANKC. UROK\": \"Sanction Interest\",\n",
        "        \"SIPO\": \"Household\",\n",
        "        \"DUCHOD\": \"Old-age Pension\",\n",
        "        \"UVER\": \"Loan Payment\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# set the amount to negative for withdrawals and positive for credits\n",
        "transactions_df[\"amount\"] = np.where(\n",
        "    transactions_df[\"type\"] == \"Credit\",\n",
        "    transactions_df[\"amount\"],\n",
        "    -transactions_df[\"amount\"],\n",
        ")\n",
        "\n",
        "transactions_df.rename(columns={\"type\": \"transaction_type\"}, inplace=True)\n",
        "\n",
        "transactions_df.info()"
      ],
      "id": "46d65967",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transactions_df.head()"
      ],
      "id": "288c751d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transactions_df.describe()"
      ],
      "id": "8d8c47f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(\n",
        "    transactions_df, [\"transaction_type\", \"operation\", \"k_symbol\"]\n",
        ")"
      ],
      "id": "901576e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(transactions_df, [\"date\", \"amount\", \"balance\"])"
      ],
      "id": "ec6869d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the distributions of the transaction table we can see that the count of transactions per year increase over time. So we can conclude that the bank has a growing client base. It may however also show that there are more transactions per client.\n",
        "\n",
        "However, the other plots are not very useful. For one the transaction amount seems to be very sparse, ranging from values between -80000 and 80000.\n",
        "\n",
        "The balance distribution also showcases that there are accounts with a negative balance after a transaction, which would only make sense if debt is also included in this value.\n",
        "\n",
        "According to description of the field balance: \"balance after transaction\"\n",
        "\n",
        "#### {{{{POTENTIAL ADD A PLOT SHOWCASING CLIENT BASE GROWTH}}}}\n",
        "\n",
        "#### Transaction Amounts and Counts by Month"
      ],
      "id": "d0c54f7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting a list of unique years from the dataset\n",
        "transactions_df[\"year\"] = transactions_df[\"date\"].dt.year\n",
        "transactions_df[\"month\"] = transactions_df[\"date\"].dt.month\n",
        "\n",
        "months = [\n",
        "    \"Jan\",\n",
        "    \"Feb\",\n",
        "    \"Mar\",\n",
        "    \"Apr\",\n",
        "    \"May\",\n",
        "    \"Jun\",\n",
        "    \"Jul\",\n",
        "    \"Aug\",\n",
        "    \"Sep\",\n",
        "    \"Oct\",\n",
        "    \"Nov\",\n",
        "    \"Dec\",\n",
        "]\n",
        "years = sorted(transactions_df[\"year\"].unique())\n",
        "\n",
        "fig, axs = plt.subplots(\n",
        "    len(years) * 2,\n",
        "    1,\n",
        "    figsize=(8, 6 * len(years)),\n",
        "    sharex=True,\n",
        "    gridspec_kw={\"height_ratios\": [3, 1] * len(years)},\n",
        ")\n",
        "\n",
        "for i, year in enumerate(years):\n",
        "    # filter transactions for the current year\n",
        "    yearly_transactions = transactions_df[transactions_df[\"year\"] == year]\n",
        "\n",
        "    # preparing data for the box plot: a list of amounts for each month for the current year\n",
        "    amounts_per_month_yearly = [\n",
        "        yearly_transactions[yearly_transactions[\"month\"] == month][\"amount\"]\n",
        "        for month in range(1, 13)\n",
        "    ]\n",
        "\n",
        "    # preparing data for the bar chart for the current year\n",
        "    monthly_summary_yearly = (\n",
        "        yearly_transactions.groupby(\"month\")\n",
        "        .agg(TotalAmount=(\"amount\", \"sum\"), TransactionCount=(\"amount\", \"count\"))\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # box plot for transaction amounts by month for the current year\n",
        "    axs[i * 2].boxplot(amounts_per_month_yearly, patch_artist=True)\n",
        "    axs[i * 2].set_title(f\"Transaction Amounts Per Month in {year} (Box Plot)\")\n",
        "    axs[i * 2].set_yscale(\"symlog\")\n",
        "    axs[i * 2].set_ylabel(\"Transaction Amounts (log scale)\")\n",
        "    axs[i * 2].grid(True, which=\"both\")\n",
        "\n",
        "    # Bar chart for transaction count by month for the current year\n",
        "    axs[i * 2 + 1].bar(\n",
        "        monthly_summary_yearly[\"month\"],\n",
        "        monthly_summary_yearly[\"TransactionCount\"],\n",
        "        color=\"tab:red\",\n",
        "        alpha=0.6,\n",
        "    )\n",
        "    axs[i * 2 + 1].set_ylabel(\"Transaction Count\")\n",
        "    axs[i * 2 + 1].grid(True, which=\"both\")\n",
        "\n",
        "# Setting x-ticks and labels for the last bar chart (shared x-axis for all)\n",
        "axs[-1].set_xticks(range(1, 13))\n",
        "axs[-1].set_xticklabels(months)\n",
        "axs[-1].set_xlabel(\"Month\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e909f07a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negative Balances"
      ],
      "id": "40963084"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negative_balances = transactions_df[transactions_df[\"balance\"] < 0]\n",
        "plot_numerical_distributions(negative_balances, [\"balance\", \"amount\"])\n",
        "print(f\"Number of transactions with negative balance: {len(negative_balances)}\")"
      ],
      "id": "a48a6a40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There appear to be 2999 transactions which have a negative balance, therefore after the transaction the account balance was negative. This implies that these accounts are in some kind of debt.\n",
        "\n",
        "### Loans"
      ],
      "id": "ef01f5e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df = read_csv(\"data/loan.csv\")\n",
        "\n",
        "loans_df[\"date\"] = pd.to_datetime(loans_df[\"date\"], format=\"%y%m%d\")\n",
        "\n",
        "loans_df[\"status\"] = loans_df[\"status\"].map(\n",
        "    {\n",
        "        \"A\": \"Contract finished, no problems\",\n",
        "        \"B\": \"Contract finished, loan not paid\",\n",
        "        \"C\": \"Contract running, OK thus-far\",\n",
        "        \"D\": \"Contract running, client in debt\",\n",
        "    }\n",
        ")\n",
        "\n",
        "loans_df.rename(\n",
        "    columns={\n",
        "        \"date\": \"granted_date\",\n",
        "        \"amount\": \"amount\",\n",
        "        \"duration\": \"duration\",\n",
        "        \"payments\": \"monthly_payments\",\n",
        "        \"status\": \"status\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "loans_df.info()"
      ],
      "id": "3612f270",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df.head()"
      ],
      "id": "fd9a49c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df.describe()"
      ],
      "id": "e7887559",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df.nunique()"
      ],
      "id": "f61cd065",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems as if one account can have at max one loan."
      ],
      "id": "1336dcca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(loans_df, [\"duration\", \"status\"], rotate_x=True)"
      ],
      "id": "e6f90036",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of durations seems to be even. Most of the loans are still running and the majority of loans are in good standing. Around 200 loans are finished without problems. Around 11% of loans are in a potentially problematic state."
      ],
      "id": "2e9876b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(loans_df, [\"granted_date\"])"
      ],
      "id": "4e0cafef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of granted dates shows a steady increase with a drop at 1995-1996 before reaching its peak at 1998.\n",
        "\n",
        "### Credit Cards"
      ],
      "id": "980dc620"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cards_df = read_csv(\"data/card.csv\")\n",
        "\n",
        "cards_df[\"issued\"] = pd.to_datetime(\n",
        "    cards_df[\"issued\"], format=\"%y%m%d %H:%M:%S\"\n",
        ").dt.date\n",
        "\n",
        "cards_df.info()"
      ],
      "id": "ee1c30f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cards_df.head()"
      ],
      "id": "033aa9ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cards_df.describe()"
      ],
      "id": "1a733a77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_categorical_variables(cards_df, [\"type\"])"
      ],
      "id": "b9341548",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that a majority of the credit cards issued are classic. A small fraction of 16.26% of the credit cards are junior cards and 9.87% are gold cards. "
      ],
      "id": "4a30b20c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(cards_df, [\"issued\"])"
      ],
      "id": "b8786ff7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows that over time there appear to be more credit cards issued which aligns with assumed growth of the bank (as shown in the previous plots with increasing number of transactions).\n",
        "\n",
        "### Demographic data"
      ],
      "id": "4c536dda"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df = read_csv(\"data/district.csv\")\n",
        "\n",
        "# rename columns\n",
        "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
        "districts_df.rename(\n",
        "    columns={\n",
        "        \"A1\": \"district_id\",\n",
        "        \"A2\": \"district_name\",\n",
        "        \"A3\": \"region\",\n",
        "        \"A4\": \"inhabitants\",\n",
        "        \"A5\": \"small_municipalities\",\n",
        "        \"A6\": \"medium_municipalities\",\n",
        "        \"A7\": \"large_municipalities\",\n",
        "        \"A8\": \"huge_municipalities\",\n",
        "        \"A9\": \"cities\",\n",
        "        \"A10\": \"ratio_urban_inhabitants\",\n",
        "        \"A11\": \"average_salary\",\n",
        "        \"A12\": \"unemployment_rate_1995\",\n",
        "        \"A13\": \"unemployment_rate_1996\",\n",
        "        \"A14\": \"entrepreneurs_per_1000_inhabitants\",\n",
        "        \"A15\": \"crimes_committed_1995\",\n",
        "        \"A16\": \"crimes_committed_1996\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "for col in [\n",
        "    \"unemployment_rate_1995\",\n",
        "    \"unemployment_rate_1996\",\n",
        "    \"crimes_committed_1995\",\n",
        "    \"crimes_committed_1996\",\n",
        "]:\n",
        "    districts_df[col] = pd.to_numeric(districts_df[col], errors=\"coerce\")\n",
        "\n",
        "districts_df.info()"
      ],
      "id": "b16c1785",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears as if there is 1 null value for unemployment rate in 1995 and crimes committed in 1995."
      ],
      "id": "39ad69eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df[\"crimes_committed_1995\"].corr(districts_df[\"crimes_committed_1996\"])"
      ],
      "id": "ad9fd449",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation between crimes committed in 1995 and 1996 is 0.99, which is very high. This implies that the number of crimes committed in 1995 is a good predictor for the number of crimes committed in 1996. Since 1995 is missing a data point we will impute the missing value with crimes committed in 1996."
      ],
      "id": "af47cfa2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df[\"unemployment_rate_1995\"].corr(districts_df[\"unemployment_rate_1996\"])"
      ],
      "id": "7a7a8814",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same goes for the unemployment rate in 1995 and 1996. \n",
        "\n",
        "We can impute the values therefore using a linear regression model to approximate the missing values."
      ],
      "id": "5f1c4158"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "districts_df_imputed = districts_df.copy()\n",
        "\n",
        "X = districts_df_imputed[['crimes_committed_1996']].values.reshape(-1, 1)\n",
        "y = districts_df_imputed['crimes_committed_1995'].values.reshape(-1, 1)\n",
        "mask = ~np.isnan(y).flatten()\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X[mask], y[mask])\n",
        "imputed_values = reg_model.predict(X[~mask])\n",
        "districts_df_imputed.loc[districts_df_imputed['crimes_committed_1995'].isnull(), 'crimes_committed_1995'] = imputed_values.flatten()\n",
        "\n",
        "X = districts_df_imputed[['unemployment_rate_1996']].values.reshape(-1, 1)\n",
        "y = districts_df_imputed['unemployment_rate_1995'].values.reshape(-1, 1)\n",
        "mask = ~np.isnan(y).flatten()\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X[mask], y[mask])\n",
        "imputed_values = reg_model.predict(X[~mask])\n",
        "districts_df_imputed.loc[districts_df_imputed['unemployment_rate_1995'].isnull(), 'unemployment_rate_1995'] = imputed_values.flatten()\n",
        "\n",
        "districts_df = districts_df_imputed.copy()\n",
        "districts_df.dropna(inplace=True)"
      ],
      "id": "4c886cf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df.isnull().sum()"
      ],
      "id": "f5e624d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now there are no missing values in the dataset and the integrity of the data is preserved."
      ],
      "id": "19305513"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df.head()"
      ],
      "id": "61c4f6cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df.describe()"
      ],
      "id": "aaf28670",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "districts_df.nunique()"
      ],
      "id": "a1b4abc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_numerical_distributions(districts_df, [\"average_salary\", \"crimes_committed_1995\"])"
      ],
      "id": "b0ebba71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot how many inhabitants each region has, group by region first then sum\n",
        "plt.figure(figsize=(10, 6))\n",
        "sum_inhabitants_per_region = districts_df.groupby(\"region\")[\"inhabitants\"].sum()\n",
        "sum_inhabitants_per_region.plot(kind=\"bar\")\n",
        "plt.title(\"Total Number of Inhabitants per Region\")\n",
        "plt.xlabel(\"Region\")\n",
        "plt.ylabel(\"Total Number of Inhabitants\")\n",
        "\n",
        "# add exact numbers to the bar \n",
        "for i, v in enumerate(sum_inhabitants_per_region):\n",
        "    plt.text(i, v + 10000, str(v), ha=\"center\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "d8b19799",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the distribution of inhabitants per region we can see that the majority of inhabitants are in the region of Prague. This is not surprising as Prague is the capital of the Czech Republic and is the most populous region.\n",
        "\n",
        "A quick google search seems to show that the inhabitants per region are approximately correctly set. Here an example for (Prague)[https://www.google.com/search?q=prague+city+population+1999&sourceid=chrome&ie=UTF-8] \n",
        "\n",
        "This is not the case for south Moravia and north Moravia, which appear to be overestimated, however we couldn't find a very reliable source for this information.\n",
        "\n",
        "## Data Relationships\n",
        "\n",
        "Following the documentation of the dataset, there are multiple relationships that need to be validated. https://sorry.vse.cz/\\~berka/challenge/PAST/index.html\n",
        "\n",
        "The ERD according to the descriptions on https://sorry.vse.cz/\\~berka/challenge/PAST/index.html\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNqtV1Fv4jgQ_itWXu6l7SZ0gQatTsqGdhddCxVQrXSqFJnEgLWJnbOd7bGl__3GTgA3JGyvWh7ajPPNZ898nrHz7MQ8Ic7AIWJI8Urg7JEh-AVhOHkYz9FzaeqfVIKyFcJxzAumIpqg-7_QoxOUNhoNH50jcEL1Q2zQNxo9rAZq8AQrUv45EIaCYEU5Q0MYb-BeCvJPQVi8AZ-RlAVmMUE3u8Gdw0v5L7wdXTeGE6eUWNGExmxa3YIKtY5YkS2IAOBnbbYt7S1hVwsbjmb3k9loPpqMG1YHRPl-bUCSc0lNThqzfYjlpjWWYxVvfqmi2uRamMkTI-LDgySiFsNkOryeNqyei4SI_fIn2voNq1lg9j1SHGBTEtPcxPgZxk7w1tAVve2wTDlWCGdmWkg1WVBFEhRkdWDF_D2Sm2zBU8De402mWcM1FjhWRNCfZt_WkjSfBuNZELYIrQRmcp-qubaAq1XptyXMriqbsmXXVjJD3SVUffhG1ToR-AmnDVCeE1EW52vmO2gmJ9JqQ49TW4IXODWVDCVWPQVLSCqyfE_rYU_SpkltOxkVhYLt_YudZAFrm6hS-XYSNMkLkbG9urdg_C5ZDdcXiFjv1rqutewb7HHaKbxMir2cBjWs7GO2vNztEpB3nKl1ukFVAciGgKTCqpA71pmx6p05mA6b-jIWyaErg9F6wOR2lz3dIHc7XNPN4fkoqxTOEUjkbsbSstN66Nrz6SicN7fsfe-_P3HkVXCGM2KDxmA3wARZlfJMzUNdP8YjvowoW2PoW7iUZ8wvEF-i0WGw2SsrGIXOiFNIHJHRE9S9zRSlKuq6rtb7FRB9QjD8PkpwhJ4ceb7vH_PCy3P95n3UHbfk9hu59dtz_93kKxV5wNCQjD_NeDNrXEL2ioTGPi4tU3LaoRAL3S5eqfmgx2wx0VTDG8r9B7TmFYkkdE-h70ZBOYBmZuDYoWAky1NuajiCNZDI7-oJrWE9F0F_-N03efdavHvN6QGEILkgjBRCRnCwmBzXwr-2QQhAyGvPN88yqqAhRrGgGeho4gnNcz2IU14926tndYBaD9huz8_58_7KPDDtLsa6I4_2lVpHVxfSgSlpSROY5ADeUVVY-46oHQIpeUzNBPqU3nlVlP_PyYZpz-227Mja5SuWLSsqb3wa8wUk0bK3Ie1rTxlsDNfCZvS2PD4Hpv8y9BW2ssY5Z05GRIZpAt8ppuE-OmpNdJ_U0IQscZGaI-0FoLhQfLZhsTNQoiBnTpHrzl593DiDJU4ljOaY_c15tgPpGw8Xd-WnkPkiMhBn8Oz86wyuOhf9bsf96Lndvuu5ff_M2TgDz-1ddK763qXvel237131X86cn4bUvbi67PU6Xt_z3Y--e9ntv_wHEyE3kA?type=png)](https://mermaid.live/edit#pako:eNqtV1Fv4jgQ_itWXu6l7SZ0gQatTsqGdhddCxVQrXSqFJnEgLWJnbOd7bGl__3GTgA3JGyvWh7ajPPNZ898nrHz7MQ8Ic7AIWJI8Urg7JEh-AVhOHkYz9FzaeqfVIKyFcJxzAumIpqg-7_QoxOUNhoNH50jcEL1Q2zQNxo9rAZq8AQrUv45EIaCYEU5Q0MYb-BeCvJPQVi8AZ-RlAVmMUE3u8Gdw0v5L7wdXTeGE6eUWNGExmxa3YIKtY5YkS2IAOBnbbYt7S1hVwsbjmb3k9loPpqMG1YHRPl-bUCSc0lNThqzfYjlpjWWYxVvfqmi2uRamMkTI-LDgySiFsNkOryeNqyei4SI_fIn2voNq1lg9j1SHGBTEtPcxPgZxk7w1tAVve2wTDlWCGdmWkg1WVBFEhRkdWDF_D2Sm2zBU8De402mWcM1FjhWRNCfZt_WkjSfBuNZELYIrQRmcp-qubaAq1XptyXMriqbsmXXVjJD3SVUffhG1ToR-AmnDVCeE1EW52vmO2gmJ9JqQ49TW4IXODWVDCVWPQVLSCqyfE_rYU_SpkltOxkVhYLt_YudZAFrm6hS-XYSNMkLkbG9urdg_C5ZDdcXiFjv1rqutewb7HHaKbxMir2cBjWs7GO2vNztEpB3nKl1ukFVAciGgKTCqpA71pmx6p05mA6b-jIWyaErg9F6wOR2lz3dIHc7XNPN4fkoqxTOEUjkbsbSstN66Nrz6SicN7fsfe-_P3HkVXCGM2KDxmA3wARZlfJMzUNdP8YjvowoW2PoW7iUZ8wvEF-i0WGw2SsrGIXOiFNIHJHRE9S9zRSlKuq6rtb7FRB9QjD8PkpwhJ4ceb7vH_PCy3P95n3UHbfk9hu59dtz_93kKxV5wNCQjD_NeDNrXEL2ioTGPi4tU3LaoRAL3S5eqfmgx2wx0VTDG8r9B7TmFYkkdE-h70ZBOYBmZuDYoWAky1NuajiCNZDI7-oJrWE9F0F_-N03efdavHvN6QGEILkgjBRCRnCwmBzXwr-2QQhAyGvPN88yqqAhRrGgGeho4gnNcz2IU14926tndYBaD9huz8_58_7KPDDtLsa6I4_2lVpHVxfSgSlpSROY5ADeUVVY-46oHQIpeUzNBPqU3nlVlP_PyYZpz-227Mja5SuWLSsqb3wa8wUk0bK3Ie1rTxlsDNfCZvS2PD4Hpv8y9BW2ssY5Z05GRIZpAt8ppuE-OmpNdJ_U0IQscZGaI-0FoLhQfLZhsTNQoiBnTpHrzl593DiDJU4ljOaY_c15tgPpGw8Xd-WnkPkiMhBn8Oz86wyuOhf9bsf96Lndvuu5ff_M2TgDz-1ddK763qXvel237131X86cn4bUvbi67PU6Xt_z3Y--e9ntv_wHEyE3kA)\n",
        "\n",
        "This ERD shows how the data appears in the dataset:\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNqtV99P2zAQ_lesvOyFbjCJSq2mSSHlRzRoUVq0F6TITdzWIrEz2xnqgP99ZydNTeIUhOgD5JzvPvvufJ-dJy_hKfHGHhETitcC5_cMwc8PgtnddIGeKlP_pBKUrRFOEl4yFdMU3f5C955f2Sic3HsdcEr1Q2LQFxo9qQda8BQrUv3ZEwaCYEU5QxMYd3CvBPlTEpZswSeUssQsIehiN7hzeKn-BdfhuTOcJKPEiiYwpmt1SyrUJmZlviQCgGfa7Fvae8KuFzYJ57ezebgIZ1PH6oCoaNYGJAWX1OTEme19LBe9sXSrePFmFdW20IWZPTIivt1JIloxzKLJeeRYPRcpEc3yZ9r6hNUsMXuIFQdYRBJamBjPYOwAbwtd09sOq4xjhXBupoVUkyVVJEV-3gbWzA-x3OZLngH2Fm9zzRpssMCJIoL-M_u2laRF5E_nftBTaCUwk02qFtoCrt5Kvy9hdlfZlD27ti4z9F1K1bffVG1SgR9x5oDygoiqOV8z34CYHEirDe2mtgIvcWY6GVqsfvJXkFRk-R6uhz1JX01a28lUUSjY3m_sJAvY2kR1la9nvqu8EBlrqnsNxmeV1XBdQsR6t7br2sq-wXbTTuFlWjblNKhJbXfZimq3S0DecKY22RbVDSAdAUmFVSl3rHNjtZXZjyYuXcYi3asyGL0HTGGr7GGB3O1wTbeA505WKZwjkMjdjJVlp3Wv2osoDBZuyW60__bAkVfDGc6JDZqC7YAJsq7KE5mHdv0Yj_kqpmyDQbdwVZ4p_4r4CoX7QbdXXjIKyogzSByR8SP0vc0UZyo-PT7W9X4FRD8QDH-MEhxBk-OT0WjU5YWXA_3mY9TfjyvukZNbvx2MPky-VvEJMDiS8dOMu1mTCtJUJDB2t7VMy2mHUiy1XLyq5p0es4uJIg13tPtfkOY1iSWop9B3I78aQHMz0HUoGcmLjJsejmENJB6d6gmtYT0XQV9Gp-_yHvZ4D93pAYQghSCMlELGcLCYHLfCP7dBCEDopD_fPM-pAkGME0FzqKOJJzDP7SAOeQ1tr6GlAC0NeH4eDPhTc2UeG7lLsFbksOnUNrq-kI5NS0uawiR78I6qxtp3RO3gS8kTaibQp_TOq6bUTs_P73WyYbWnUWTtcoVlz4qqG5_GXEJJdNn7kPa1pwo2gWuhEw1Tm-NzbPSXoSvYyhrnHXk5ETmmKXynGMG999SGaJ3U0JSscJmZI-0FoLhUfL5liTdWoiRHXlloZa8_brzxCmcSRvUVh4ub6tsn4WxF197LfwBRISI?type=png)](https://mermaid.live/edit#pako:eNqtV99P2zAQ_lesvOyFbjCJSq2mSSHlRzRoUVq0F6TITdzWIrEz2xnqgP99ZydNTeIUhOgD5JzvPvvufJ-dJy_hKfHGHhETitcC5_cMwc8PgtnddIGeKlP_pBKUrRFOEl4yFdMU3f5C955f2Sic3HsdcEr1Q2LQFxo9qQda8BQrUv3ZEwaCYEU5QxMYd3CvBPlTEpZswSeUssQsIehiN7hzeKn-BdfhuTOcJKPEiiYwpmt1SyrUJmZlviQCgGfa7Fvae8KuFzYJ57ezebgIZ1PH6oCoaNYGJAWX1OTEme19LBe9sXSrePFmFdW20IWZPTIivt1JIloxzKLJeeRYPRcpEc3yZ9r6hNUsMXuIFQdYRBJamBjPYOwAbwtd09sOq4xjhXBupoVUkyVVJEV-3gbWzA-x3OZLngH2Fm9zzRpssMCJIoL-M_u2laRF5E_nftBTaCUwk02qFtoCrt5Kvy9hdlfZlD27ti4z9F1K1bffVG1SgR9x5oDygoiqOV8z34CYHEirDe2mtgIvcWY6GVqsfvJXkFRk-R6uhz1JX01a28lUUSjY3m_sJAvY2kR1la9nvqu8EBlrqnsNxmeV1XBdQsR6t7br2sq-wXbTTuFlWjblNKhJbXfZimq3S0DecKY22RbVDSAdAUmFVSl3rHNjtZXZjyYuXcYi3asyGL0HTGGr7GGB3O1wTbeA505WKZwjkMjdjJVlp3Wv2osoDBZuyW60__bAkVfDGc6JDZqC7YAJsq7KE5mHdv0Yj_kqpmyDQbdwVZ4p_4r4CoX7QbdXXjIKyogzSByR8SP0vc0UZyo-PT7W9X4FRD8QDH-MEhxBk-OT0WjU5YWXA_3mY9TfjyvukZNbvx2MPky-VvEJMDiS8dOMu1mTCtJUJDB2t7VMy2mHUiy1XLyq5p0es4uJIg13tPtfkOY1iSWop9B3I78aQHMz0HUoGcmLjJsejmENJB6d6gmtYT0XQV9Gp-_yHvZ4D93pAYQghSCMlELGcLCYHLfCP7dBCEDopD_fPM-pAkGME0FzqKOJJzDP7SAOeQ1tr6GlAC0NeH4eDPhTc2UeG7lLsFbksOnUNrq-kI5NS0uawiR78I6qxtp3RO3gS8kTaibQp_TOq6bUTs_P73WyYbWnUWTtcoVlz4qqG5_GXEJJdNn7kPa1pwo2gWuhEw1Tm-NzbPSXoSvYyhrnHXk5ETmmKXynGMG999SGaJ3U0JSscJmZI-0FoLhUfL5liTdWoiRHXlloZa8_brzxCmcSRvUVh4ub6tsn4WxF197LfwBRISI)\n",
        "\n",
        "In order to also validate the relationships from a algorithmic perspective, we can use the following code:"
      ],
      "id": "5bdb858f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# verify 1:1 relationships between CLIENT, LOAN and DISPOSITION\n",
        "assert dispositions_df[\n",
        "    \"client_id\"\n",
        "].is_unique, \"Each client_id should appear exactly once in the DISPOSITION DataFrame.\"\n",
        "assert loans_df[\n",
        "    \"account_id\"\n",
        "].is_unique, \"Each account_id should appear exactly once in the LOAN DataFrame.\"\n",
        "\n",
        "# verify 1:M relationships between ACCOUNT and DISPOSITION\n",
        "# assert dispositions['account_id'].is_unique == False, \"An account_id should appear more than once in the DISPOSITION DataFrame.\"\n",
        "assert (\n",
        "    dispositions_df[\"account_id\"].is_unique == True\n",
        "), \"An account_id should appear once in the DISPOSITION DataFrame.\"\n",
        "\n",
        "# verify each district_id in ACCOUNT and CLIENT exists in DISTRICT\n",
        "assert set(accounts_df[\"district_id\"]).issubset(\n",
        "    set(districts_df[\"district_id\"])\n",
        "), \"All district_ids in ACCOUNT should exist in DISTRICT.\"\n",
        "assert set(clients_df[\"district_id\"]).issubset(\n",
        "    set(districts_df[\"district_id\"])\n",
        "), \"All district_ids in CLIENT should exist in DISTRICT.\"\n",
        "\n",
        "# verify each account_id in DISPOSITION, ORDER, TRANSACTION, and LOAN exists in ACCOUNT\n",
        "assert set(dispositions_df[\"account_id\"]).issubset(\n",
        "    set(accounts_df[\"account_id\"])\n",
        "), \"All account_ids in DISPOSITION should exist in ACCOUNT.\"\n",
        "assert set(orders_df[\"account_id\"]).issubset(\n",
        "    set(accounts_df[\"account_id\"])\n",
        "), \"All account_ids in ORDER should exist in ACCOUNT.\"\n",
        "assert set(transactions_df[\"account_id\"]).issubset(\n",
        "    set(accounts_df[\"account_id\"])\n",
        "), \"All account_ids in TRANSACTION should exist in ACCOUNT.\"\n",
        "assert set(loans_df[\"account_id\"]).issubset(\n",
        "    set(accounts_df[\"account_id\"])\n",
        "), \"All account_ids in LOAN should exist in ACCOUNT.\"\n",
        "\n",
        "# verify each client_id in DISPOSITION exists in CLIENT\n",
        "assert set(dispositions_df[\"client_id\"]).issubset(\n",
        "    set(clients_df[\"client_id\"])\n",
        "), \"All client_ids in DISPOSITION should exist in CLIENT.\"\n",
        "\n",
        "# verify each disp_id in CARD exists in DISPOSITION\n",
        "assert set(cards_df[\"disp_id\"]).issubset(\n",
        "    set(dispositions_df[\"disp_id\"])\n",
        "), \"All disp_ids in CARD should exist in DISPOSITION.\""
      ],
      "id": "9219129b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation: Non-Transactional Data\n",
        "\n",
        "This section covers on how we prepare the non-transactional data in order to create the golden record."
      ],
      "id": "6c51dc66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_pivot_df = orders_df.pivot_table(\n",
        "    index=\"account_id\",\n",
        "    columns=\"k_symbol\",\n",
        "    values=\"debited_amount\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0,\n",
        ")\n",
        "\n",
        "orders_pivot_df.columns = [\n",
        "    f\"k_symbol_debited_sum_{col.lower()}\" for col in orders_pivot_df.columns\n",
        "]\n",
        "orders_pivot_df = orders_pivot_df.reset_index()\n",
        "orders_pivot_df.head()"
      ],
      "id": "88cb1f02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def merge_non_transactional_data(\n",
        "    clients, districts, dispositions, accounts, orders, loans, cards\n",
        "):\n",
        "    # rename district_id for clarity in clients and accounts DataFrames\n",
        "    clients = clients.rename(columns={\"district_id\": \"client_district_id\"})\n",
        "    accounts = accounts.rename(columns={\"district_id\": \"account_district_id\"})\n",
        "\n",
        "    # prepare districts dataframe for merge with prefix for clients and accounts\n",
        "    districts_client_prefixed = districts.add_prefix(\"client_\")\n",
        "    districts_account_prefixed = districts.add_prefix(\"account_\")\n",
        "\n",
        "    # merge district information for clients and accounts with prefixed columns\n",
        "    clients_with_districts = pd.merge(\n",
        "        clients,\n",
        "        districts_client_prefixed,\n",
        "        left_on=\"client_district_id\",\n",
        "        right_on=\"client_district_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "    accounts_with_districts = pd.merge(\n",
        "        accounts,\n",
        "        districts_account_prefixed,\n",
        "        left_on=\"account_district_id\",\n",
        "        right_on=\"account_district_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # merge cards with dispositions and prefix card-related columns to avoid confusion\n",
        "    cards_prefixed = cards.add_prefix(\"card_\")\n",
        "    dispositions_with_cards = pd.merge(\n",
        "        dispositions,\n",
        "        cards_prefixed,\n",
        "        left_on=\"disp_id\",\n",
        "        right_on=\"card_disp_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # merge clients (with district info) with dispositions and cards\n",
        "    clients_dispositions_cards = pd.merge(\n",
        "        dispositions_with_cards, clients_with_districts, on=\"client_id\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    # merge the above with accounts (with district info) on account_id\n",
        "    accounts_clients_cards = pd.merge(\n",
        "        accounts_with_districts, clients_dispositions_cards, on=\"account_id\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    # merge orders DataFrame, assuming orders might contain columns that could overlap, prefix as needed\n",
        "    orders_prefixed = orders.add_prefix(\"order_\")\n",
        "    comprehensive_df_with_orders = pd.merge(\n",
        "        accounts_clients_cards,\n",
        "        orders_prefixed,\n",
        "        left_on=\"account_id\",\n",
        "        right_on=\"order_account_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # merge loans with the comprehensive dataframe (now including orders) on account_id\n",
        "    # prefix loan-related columns to maintain clarity\n",
        "    loans_prefixed = loans.add_prefix(\"loan_\")\n",
        "    final_df = pd.merge(\n",
        "        comprehensive_df_with_orders,\n",
        "        loans_prefixed,\n",
        "        left_on=\"account_id\",\n",
        "        right_on=\"loan_account_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    final_df[\"account_created\"] = pd.to_datetime(final_df[\"account_created\"])\n",
        "    final_df[\"card_issued\"] = pd.to_datetime(final_df[\"card_issued\"])\n",
        "    final_df[\"has_card\"] = final_df[\"card_issued\"].notna()\n",
        "    return final_df\n",
        "\n",
        "\n",
        "non_transactional_df = merge_non_transactional_data(\n",
        "    clients_df,\n",
        "    districts_df,\n",
        "    dispositions_df,\n",
        "    accounts_df,\n",
        "    orders_pivot_df,\n",
        "    loans_df,\n",
        "    cards_df,\n",
        ")\n",
        "non_transactional_df.to_csv(\"data/non_transactional.csv\", index=False)\n",
        "non_transactional_df.info()"
      ],
      "id": "f1d0b1c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "## Non-transactional Data\n",
        "\n",
        "### Card Holders"
      ],
      "id": "5cb0470b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.title(\"Number of Clients by Card Type\")\n",
        "sns.barplot(\n",
        "    x=[\"No Card\", \"Classic/Gold Card Holders\", \"Junior Card Holders\"],\n",
        "    y=[\n",
        "        non_transactional_df[\"card_type\"].isna().sum(),\n",
        "        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n",
        "        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n",
        "    ],\n",
        ")\n",
        "# ensure that the number of clients is shown on the bars\n",
        "for i, v in enumerate(\n",
        "    [\n",
        "        non_transactional_df[\"card_type\"].isna().sum(),\n",
        "        non_transactional_df[\"card_type\"].isin([\"gold\", \"classic\"]).sum(),\n",
        "        non_transactional_df[\"card_type\"].eq(\"junior\").sum(),\n",
        "    ]\n",
        "):\n",
        "    plt.text(i, v + 10, str(v), ha=\"center\", va=\"bottom\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "79486259",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the distribution of card holders in general we can see that the most clients are not in a possession of a credit card."
      ],
      "id": "e1048c46"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.title(\n",
        "    f'Distribution of Age for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n",
        ")\n",
        "sns.histplot(\n",
        "    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\"age\"],\n",
        "    kde=True,\n",
        "    bins=30,\n",
        ")\n",
        "plt.xlabel(\"Age of Client (presumably in 1999)\")\n",
        "plt.show()"
      ],
      "id": "a4daf96c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the age distribution of Junior Card holders paints a picture on this group, however only looking at the current age may be misleading as we need to understand how old they were when the card was issued to determine if they could have been eligble for a Classic/Gold card (at least 18 when the card was issued)."
      ],
      "id": "dcb16d85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "non_transactional_df[\"card_issued\"] = pd.to_datetime(\n",
        "    non_transactional_df[\"card_issued\"]\n",
        ")\n",
        "\n",
        "non_transactional_df[\"age_at_card_issuance\"] = (\n",
        "    non_transactional_df[\"card_issued\"] - non_transactional_df[\"birth_date\"]\n",
        ")\n",
        "non_transactional_df[\"age_at_card_issuance\"] = (\n",
        "    non_transactional_df[\"age_at_card_issuance\"].dt.days // 365\n",
        ")\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\n",
        "    f'Distribution of Age at Card Issuance for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}'\n",
        ")\n",
        "sns.histplot(\n",
        "    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n",
        "        \"age_at_card_issuance\"\n",
        "    ],\n",
        "    kde=True,\n",
        "    bins=30,\n",
        ")\n",
        "plt.xlabel(\"Age at Card Issuance\")\n",
        "plt.show()"
      ],
      "id": "97b6d51b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that roughly 1/3 of the Junior Card holders were not of legal age (assuming legal age is 18) when receiving their Junior Card. "
      ],
      "id": "f028caf0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.title(\n",
        "    f\"Distribution of Age at Card Issuance for All Card Types\\n total count = {len(non_transactional_df)}\"\n",
        ")\n",
        "sns.histplot(\n",
        "    non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"][\n",
        "        \"age_at_card_issuance\"\n",
        "    ],\n",
        "    kde=True,\n",
        "    bins=10,\n",
        "    color=\"blue\",\n",
        "    label=\"Junior Card Holders\",\n",
        ")\n",
        "sns.histplot(\n",
        "    non_transactional_df[non_transactional_df[\"card_type\"] != \"junior\"][\n",
        "        \"age_at_card_issuance\"\n",
        "    ],\n",
        "    kde=True,\n",
        "    bins=30,\n",
        "    color=\"red\",\n",
        "    label=\"Non-Junior Card Holders\",\n",
        ")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Age at Card Issuance\")\n",
        "plt.show()"
      ],
      "id": "ced9fd09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the age at issue date between Junior and non-Junior (Classic/Gold) card holders shows that there is no overlap between the two groups, which makes intutively sense.\n",
        "\n",
        "Therefore removing the subset of Junior Cards seems as valid as there is no reason to believe that there are Junior Cards issued wrongly, the subset being relatively small compared to the remaining issued cards and the fact that our target is specifically Classic/Gold Card owners."
      ],
      "id": "f57f3319"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "before_len = len(non_transactional_df)\n",
        "non_transactional_df = non_transactional_df[\n",
        "    non_transactional_df[\"card_type\"] != \"junior\"\n",
        "]\n",
        "data_reduction[\"Junior Card Holders\"] = -(before_len - len(non_transactional_df))\n",
        "del before_len"
      ],
      "id": "b0838533",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time factors on Card Status\n",
        "\n",
        "The time between creating an account and issuing a card may also be important when filtering customers based on their history. We should avoid filtering out potentially interesting periods and understand how the timespans between account creation and card issuance are distributed."
      ],
      "id": "01692513"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "non_transactional_w_cards_df = non_transactional_df[\n",
        "    non_transactional_df[\"card_issued\"].notna()\n",
        "    & non_transactional_df[\"account_created\"].notna()\n",
        "]\n",
        "non_transactional_w_cards_df[\"duration_days\"] = (\n",
        "    non_transactional_w_cards_df[\"card_issued\"]\n",
        "    - non_transactional_w_cards_df[\"account_created\"]\n",
        ").dt.days\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(\n",
        "    non_transactional_w_cards_df[\"duration_days\"], bins=50, edgecolor=\"black\", kde=True\n",
        ")\n",
        "plt.title(\"Distribution of Duration Between Account Creation and Card Issuance\")\n",
        "plt.xlabel(\"Duration in Days\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "a81b1848",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The histogram displays a distribution with multiple peaks, indicating that there are several typical time frames for card issuance after account creation. The highest peak occurs within the first 250 days, suggesting that a significant number of cards are issued during this period. The frequency decreases as duration increases, with noticeable peaks that may correspond to specific processing batch cycles or policy changes over time. The distribution also has a long tail, suggesting that in some cases, card issuance can take a very long time.\n",
        "\n",
        "Analyzing the length of time a client has been with the bank in relation to their account creation date and card ownership can provide valuable insights for a bank's customer relationship management and product targeting strategies. Long-standing clients may exhibit different banking behaviors, such as product adoption and loyalty patterns, compared to newer clients."
      ],
      "id": "8cb642dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_account_creation_date = non_transactional_df[\"card_issued\"].max()\n",
        "\n",
        "non_transactional_df[\"client_tenure_years_relative\"] = (\n",
        "    max_account_creation_date - non_transactional_df[\"account_created\"]\n",
        ").dt.days / 365.25\n",
        "\n",
        "plt.figure()\n",
        "ax = sns.histplot(\n",
        "    data=non_transactional_df,\n",
        "    x=\"client_tenure_years_relative\",\n",
        "    hue=\"has_card\",\n",
        "    multiple=\"stack\",\n",
        "    binwidth=1,\n",
        "    stat=\"percent\",\n",
        ")\n",
        "\n",
        "# Call the function to add labels\n",
        "add_percentage_labels(ax, non_transactional_df[\"has_card\"].unique())\n",
        "\n",
        "# Additional plot formatting\n",
        "plt.title(\"Client Tenure Relative to Latest Card Issued Date and Card Ownership\")\n",
        "plt.xlabel(\"Client Tenure (Years, Relative to Latest Card Issuance)\")\n",
        "plt.ylabel(\"Percentage of Clients\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "id": "e8d183ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bar chart shows the tenure of clients in years, categorized by whether they own a credit card (True) or not (False). Each bar represents the percentage of clients within a specific tenure range, allowing for comparison of the distribution of card ownership among clients with different lengths of association with the bank.\n",
        "\n",
        "### Demographics\n",
        "\n",
        "Using the available demographic data, we can investigate the potential correlation between demographic data and card status. The average salary may indicate a difference between cardholders and non-cardholders, as it is reasonable to assume that cardholders have a higher average salary than non-cardholders."
      ],
      "id": "c75d38f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "sns.boxplot(x=\"has_card\", y=\"client_average_salary\", data=non_transactional_df)\n",
        "plt.title(\"Average Salary in Client's Region by Card Ownership\")\n",
        "plt.xlabel(\"Has Card\")\n",
        "plt.ylabel(\"Average Salary\")\n",
        "plt.xticks([0, 1], [\"No Card Owner\", \"Card Owner\"])\n",
        "\n",
        "plt.show()"
      ],
      "id": "13522f23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The box plot compares the average salaries of clients who own a credit card with those who do not. Both groups have a substantial overlap in salary ranges, suggesting that while there might be a trend for card owners to have higher salaries, the difference is not significant. The median salary for card owners is slightly higher than that for non-card owners, as indicated by the median line within the respective boxes.\n",
        "\n",
        "Both distributions have outliers on the higher end, indicating that some individuals have salaries significantly above the average in both groups. However, these outliers do not dominate the general trend.\n",
        "\n",
        "It should also be noted that this plot assumes that the average salary of the region's clients remained constant over the years, which is unlikely to be true.\n",
        "\n",
        "The group of bar charts represents the distribution of credit card ownership across various demographics, showing the percentage of clients with and without cards within different age groups, sexes, and regions."
      ],
      "id": "eff64693"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "non_transactional_df[\"age_group\"] = pd.cut(\n",
        "    non_transactional_df[\"age\"],\n",
        "    bins=[0, 25, 40, 55, 70, 100],\n",
        "    labels=[\"<25\", \"25-40\", \"40-55\", \"55-70\", \">70\"],\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 12))\n",
        "\n",
        "# Age Group\n",
        "plt.subplot(3, 1, 1)\n",
        "age_group_counts = (\n",
        "    non_transactional_df.groupby([\"age_group\", \"has_card\"]).size().unstack(fill_value=0)\n",
        ")\n",
        "age_group_percentages = (age_group_counts.T / age_group_counts.sum(axis=1)).T * 100\n",
        "age_group_plot = age_group_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\n",
        "age_group_plot.set_title(\"Card Ownership by Age Group\")\n",
        "age_group_plot.set_ylabel(\"Percentage\")\n",
        "add_percentage_labels(age_group_plot, non_transactional_df[\"has_card\"].unique())\n",
        "\n",
        "# Sex\n",
        "plt.subplot(3, 1, 2)\n",
        "sex_counts = (\n",
        "    non_transactional_df.groupby([\"sex\", \"has_card\"]).size().unstack(fill_value=0)\n",
        ")\n",
        "sex_percentages = (sex_counts.T / sex_counts.sum(axis=1)).T * 100\n",
        "sex_plot = sex_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\n",
        "sex_plot.set_title(\"Card Ownership by Sex\")\n",
        "sex_plot.set_ylabel(\"Percentage\")\n",
        "add_percentage_labels(sex_plot, non_transactional_df[\"has_card\"].unique())\n",
        "\n",
        "# Client Region\n",
        "plt.subplot(3, 1, 3)\n",
        "region_counts = (\n",
        "    non_transactional_df.groupby([\"client_region\", \"has_card\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "region_percentages = (region_counts.T / region_counts.sum(axis=1)).T * 100\n",
        "region_plot = region_percentages.plot(kind=\"bar\", stacked=True, ax=plt.gca())\n",
        "region_plot.set_title(\"Card Ownership by Client Region\")\n",
        "region_plot.set_ylabel(\"Percentage\")\n",
        "region_plot.tick_params(axis=\"x\", rotation=45)\n",
        "add_percentage_labels(region_plot, non_transactional_df[\"has_card\"].unique())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "df59fcf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Card Ownership by Age Group:** The bar chart displays the proportion of cardholders in different age groups. The percentage of cardholders is lowest in the age group of over 70, followed by the age group of 55-70, indicating that card ownership is more prevalent among younger demographics.\n",
        "\n",
        "**Card Ownership by Sex:** The bar chart shows the breakdown of card ownership by sex. The data reveals that the percentage of cardholders is comparable between both sexes, and no significant difference is present.\n",
        "\n",
        "**Card Ownership by Region** The bar chart at the bottom illustrates card ownership across different regions, showing a relatively consistent pattern among most regions.\n",
        "\n",
        "### Impact of Loans / Debt"
      ],
      "id": "e230cbc6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simplified_loan_status_mapping = {\n",
        "    \"Contract finished, no problems\": \"Finished\",\n",
        "    \"Contract finished, loan not paid\": \"Not Paid\",\n",
        "    \"Contract running, OK thus-far\": \"Running\",\n",
        "    \"Contract running, client in debt\": \"In Debt\",\n",
        "    \"No Loan\": \"No Loan\",\n",
        "}\n",
        "\n",
        "non_transactional_df[\"loan_status_simplified\"] = non_transactional_df[\n",
        "    \"loan_status\"\n",
        "].map(simplified_loan_status_mapping)\n",
        "\n",
        "# this variable wants to kill itself\n",
        "loan_status_simplified_card_ownership_counts = (\n",
        "    non_transactional_df.groupby([\"loan_status_simplified\", \"has_card\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "loan_status_simplified_card_ownership_percentages = (\n",
        "    loan_status_simplified_card_ownership_counts.T\n",
        "    / loan_status_simplified_card_ownership_counts.sum(axis=1)\n",
        ").T * 100\n",
        "\n",
        "loan_status_simplified_card_ownership_percentages.plot(\n",
        "    kind=\"bar\", stacked=True, figsize=(8, 6)\n",
        ")\n",
        "plt.title(\"Interaction Between Simplified Loan Status and Card Ownership\")\n",
        "plt.xlabel(\"Simplified Loan Status\")\n",
        "plt.ylabel(\"Percentage of Clients\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Has Card\", labels=[\"No Card\", \"Has Card\"])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "a112fbd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transactional Data"
      ],
      "id": "155b15e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zero_amount_transactions_df = transactions_df[transactions_df[\"amount\"] == 0]\n",
        "\n",
        "zero_amount_transactions_info = {\n",
        "    \"total_zero_amount_transactions\": len(zero_amount_transactions_df),\n",
        "    \"unique_accounts_with_zero_amount\": zero_amount_transactions_df[\n",
        "        \"account_id\"\n",
        "    ].nunique(),\n",
        "    \"transaction_type_distribution\": zero_amount_transactions_df[\n",
        "        \"transaction_type\"\n",
        "    ].value_counts(normalize=True),\n",
        "    \"operation_distribution\": zero_amount_transactions_df[\"operation\"].value_counts(\n",
        "        normalize=True\n",
        "    ),\n",
        "    \"k_symbol_distribution\": zero_amount_transactions_df[\"k_symbol\"].value_counts(\n",
        "        normalize=True\n",
        "    ),\n",
        "}\n",
        "\n",
        "zero_amount_transactions_info, len(zero_amount_transactions_info)"
      ],
      "id": "aa427ae8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accounts_with_zero_amount_transactions = accounts_df[\n",
        "    accounts_df[\"account_id\"].isin(zero_amount_transactions_df[\"account_id\"].unique())\n",
        "]\n",
        "accounts_with_zero_amount_transactions"
      ],
      "id": "5ec754b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean up unnecessary variables\n",
        "del accounts_with_zero_amount_transactions\n",
        "del zero_amount_transactions_df\n",
        "del zero_amount_transactions_info"
      ],
      "id": "9c79b2a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating first transactions where the amount equals the balance is essential for the integrity of our aggregated data analysis. This specific assertion underpins the reliability of our subsequent aggregation operations by ensuring each account's financial history starts from a verifiable point."
      ],
      "id": "c185926d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def validate_first_transactions(transactions):\n",
        "    \"\"\"\n",
        "    Validates that for each account in the transactions DataFrame, there is at least\n",
        "    one transaction where the amount equals the balance on the account's first transaction date.\n",
        "\n",
        "    Parameters:\n",
        "    - transactions (pd.DataFrame): DataFrame containing transaction data with columns\n",
        "      'account_id', 'date', 'amount', and 'balance'.\n",
        "\n",
        "    Raises:\n",
        "    - AssertionError: If not every account has a first transaction where the amount equals the balance.\n",
        "    \"\"\"\n",
        "\n",
        "    first_dates = (\n",
        "        transactions.groupby(\"account_id\")[\"date\"].min().reset_index(name=\"first_date\")\n",
        "    )\n",
        "\n",
        "    first_trans = pd.merge(transactions, first_dates, how=\"left\", on=[\"account_id\"])\n",
        "\n",
        "    first_trans_filtered = first_trans[\n",
        "        (first_trans[\"date\"] == first_trans[\"first_date\"])\n",
        "        & (first_trans[\"amount\"] == first_trans[\"balance\"])\n",
        "    ]\n",
        "\n",
        "    first_trans_filtered = first_trans_filtered.drop_duplicates(subset=[\"account_id\"])\n",
        "\n",
        "    unique_accounts = transactions[\"account_id\"].nunique()\n",
        "    assert (\n",
        "        unique_accounts == first_trans_filtered[\"account_id\"].nunique()\n",
        "    ), \"Not every account has a first transaction where the amount equals the balance.\"\n",
        "\n",
        "    return \"Validation successful: Each account has a first transaction where the amount equals the balance.\"\n",
        "\n",
        "\n",
        "validate_first_transactions(transactions_df)"
      ],
      "id": "1d1f0f0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can confirm the truth of the assertions made. It is certain that there is a transaction with an amount equal to the balance in the transaction history of every account on the first date."
      ],
      "id": "3148023f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import json\n",
        "\n",
        "\n",
        "transactions_df = pd.read_parquet(\"temp/transactions.parquet\")\n",
        "accounts_df = pd.read_parquet(\"temp/accounts.parquet\")\n",
        "non_transactional_df = pd.read_parquet(\"temp/non_transactional.parquet\")\n",
        "with open(\"temp/data_reduction.json\", \"r\") as f:\n",
        "    data_reduction = json.load(f)"
      ],
      "id": "a81be5b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation: Transactional Data\n",
        "\n",
        "As we already prepared the non-transactional data in a previous section, we now focus on the transactional data. As the end goal is to have a single record per customer, we need to aggregate the transactional data. In addition, absolute temporal data such as transaction dates needs to be transformed into relative data, such as the number of months before the event of card issuance.\n",
        "\n",
        "## Set artificial issue date for non-card holders\n",
        "\n",
        "One crucial step in the data preparation process is to set an artificial card issue date for non-card holders. This date is necessary to align the transactional data of non-card holders with that of card holders. By setting an artificial card issue date, we can create a unified timeline for all customers, enabling a more accurate comparison of transactional behaviors across different groups. \n",
        "\n",
        "First we will explore the distribution of the months between account creation and card issuance to understand the typical timeline for card issuance after account creation.\n"
      ],
      "id": "cf3e63da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def add_months_since_account_to_card(df):\n",
        "    \"\"\"Add a column to the DataFrame with the number of months between account creation and card issuance.\"\"\"\n",
        "    df[\"months_since_account_to_card\"] = df.apply(\n",
        "        lambda row: (\n",
        "            (\n",
        "                row[\"card_issued\"].to_period(\"M\")\n",
        "                - row[\"account_created\"].to_period(\"M\")\n",
        "            ).n\n",
        "            if pd.notnull(row[\"card_issued\"]) and pd.notnull(row[\"account_created\"])\n",
        "            else np.nan\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "    return df"
      ],
      "id": "a3c93da9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we need enough history to make a valid comparison between card holders and non-card holders, we filter out non-card holders with less than 25 months of history. This ensures that we have a sufficient amount of data to analyze and compare the transactional behaviors of both groups. Here is the reasoning behind this threshold:\n",
        "\n",
        "- **New Customer Period (12 months)**: We need at least one year of transactional history to capture the typical spending patterns of customers.\n",
        "- **One Year of History (12 months)**: An additional year of data provides a more comprehensive view of transactional behaviors, allowing us to identify trends and patterns more accurately across seasons and economic cycles.\n",
        "- **Lag Period (1 month)**: The month immediately preceding card issuance is excluded to avoid any potential bias caused by transactional changes due to the impending card issuance."
      ],
      "id": "c4700174"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def filter_clients_without_sufficient_history(\n",
        "    non_transactional_df, min_history_months=25\n",
        "):\n",
        "    if \"months_since_account_to_card\" not in non_transactional_df.columns:\n",
        "        non_transactional_df = add_months_since_account_to_card(non_transactional_df)\n",
        "\n",
        "    count_before = len(non_transactional_df)\n",
        "    filtered_df = non_transactional_df[\n",
        "        non_transactional_df[\"months_since_account_to_card\"].isnull()\n",
        "        | (non_transactional_df[\"months_since_account_to_card\"] >= min_history_months)\n",
        "    ]\n",
        "    print(\n",
        "        f\"Filtered out {count_before - len(filtered_df)} records with less than {min_history_months} months of history. \"\n",
        "        f\"Percentage: {(count_before - len(filtered_df)) / count_before * 100:.2f}%.\"\n",
        "    )\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "before_len = len(non_transactional_df)\n",
        "\n",
        "non_transactional_w_sufficient_history_df = filter_clients_without_sufficient_history(\n",
        "    non_transactional_df\n",
        ")\n",
        "\n",
        "data_reduction[\"Clients without sufficient history\"] = -(\n",
        "    before_len - len(non_transactional_w_sufficient_history_df)\n",
        ")\n",
        "del before_len"
      ],
      "id": "85fd02ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case roughly 10% of the non-card holders were filtered out due to insufficient history. This is a reasonable amount and will not impact the analysis significantly.\n",
        "\n",
        "Next, we will explore the distribution of the months between account creation and card issuance for card holders to understand the typical timeline for card issuance after account creation."
      ],
      "id": "3ae27c62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "non_transactional_w_card_df = non_transactional_w_sufficient_history_df.dropna(\n",
        "    subset=[\"card_issued\"]\n",
        ").copy()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(\n",
        "    non_transactional_w_card_df[\"months_since_account_to_card\"], kde=True, bins=30\n",
        ")\n",
        "plt.title(\n",
        "    \"Distribution of Months from Account Creation to Card Issuance (for Card Holders)\"\n",
        ")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "da3557f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot shows a right-skewed distribution, with the majority of card holders receiving their cards roughly 25 to 30 months after account creation. On the long-tail end, some clients receive their cards after 60 months or more.\n",
        "\n",
        "After briefly exploring the distribution of the months between account creation and card issuance for card holders, we will now set the artificial card issuance date for non-card holders. This date will be used to align the transactional data of non-card holders with that of card holders, enabling a more accurate comparison of transactional behaviors across different groups.\n",
        "\n",
        "The following approaches were considered to match non-card holders with card holders:\n",
        "\n",
        "1. Looking at the distributions above extract the amount of history a buyer most likely has at the issue data of the card\n",
        "2. For each non buyer, find a buyer which was active in a similar time window (Jaccard similarity on the Year-Month sets). Instead of looking at the full activity of a buyer, we only look at the pre-purchase activity as there is reason to believe that clients may change their patterns after purchasing date and therefore add unwanted bias.\n",
        "\n",
        "The second approach is chosen as it is provides an intuitive way to match clients based on their activity which is not only explainable but also provides a way to match clients based on their behavior. It strikes a balance of not finding a perfect match but a good enough match to focus on the discriminative features of the data.\n",
        "\n",
        "Both approaches have their advantages and disadvantages. The first approach is more straightforward and less computationally intensive, but it may not capture the nuances of client behavior. The second approach is more complex and computationally intensive but offers a more nuanced view of client activity, potentially leading to better matches.\n",
        "\n",
        "## Match by similar transaction activity\n",
        "\n",
        "The process emphasizes matching based on the timing of activity, rather than a wide array of characteristics. By identifying when both existing cardholders and non-cardholders interacted with the bank, we can infer a level of behavioral alignment that extends beyond mere transactional data. This alignment suggests a shared response to external conditions. Intuitively we are constructing tuples of non-card holders and card holders based on the similarity of their activity patterns but one of them is not a card holder yet.\n",
        "\n",
        "**Assumption**: This assumes that clients active during similar periods might be influenced by the same economic and societal conditions, providing a more nuanced foundation for establishing connections between current cardholders and potential new ones.\n",
        "\n",
        "### Construction of the Activity Matrix\n",
        "\n",
        "To hold the needed information about every customer we create a so called activity matrix. The resolution of the activity matrix is a binary matrix where each row represents a client and each column represents a month. A value of 1 indicates activity in a given month, while 0 indicates inactivity. Therefore we concentrate on the periods during which clients engage with the bank in the form of transactions rather than the specifics of those transactions. Here is a step-by-step breakdown of the construction process:\n",
        "\n",
        "1.  **Data Aggregation**: We start with transaction data, which records each client's interactions across various months. This data includes every transaction made by both current cardholders and non-cardholders.\n",
        "\n",
        "2.  **Temporal Transformation**: Each transaction is associated with a specific date. These dates are then transformed into monthly periods, consolidating daily transactions into a monthly view of activity. This step simplifies the data, focusing on the presence of activity within each month rather than the specific dates or frequencies of transactions.\n",
        "\n",
        "3.  **Matrix Structure**: The transformed data is arranged into a matrix format. Rows represent individual clients, identified by their account IDs. Columns correspond to monthly periods, spanning the entire range of months covered by the transaction data.\n",
        "\n",
        "4.  **Activity Indication**: In the matrix, a cell value is set to indicate the presence of activity for a given client in a given month. If a client made one or more transactions in a month, the corresponding cell is marked to reflect this activity. The absence of transactions for a client in a month leaves the cell unmarked. Active months are represented by a '1', indicating the presence of transactions, while inactive months are denoted by a '0', indicating no transactions."
      ],
      "id": "ba2210b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_activity_matrix(transactions):\n",
        "    \"\"\"\n",
        "    Create an activity matrix from transaction data.\n",
        "\n",
        "    The function transforms transaction data into a binary matrix that indicates\n",
        "    whether an account was active in a given month.\n",
        "\n",
        "    Parameters:\n",
        "    - transactions (pd.DataFrame): A DataFrame containing the transaction data.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: An activity matrix with accounts as rows and months as columns.\n",
        "    \"\"\"\n",
        "    transactions[\"month_year\"] = transactions[\"date\"].dt.to_period(\"M\")\n",
        "    transactions[\"active\"] = 1\n",
        "\n",
        "    activity_matrix = transactions.pivot_table(\n",
        "        index=\"account_id\", columns=\"month_year\", values=\"active\", fill_value=0\n",
        "    )\n",
        "\n",
        "    activity_matrix.columns = [f\"active_{str(col)}\" for col in activity_matrix.columns]\n",
        "    return activity_matrix\n",
        "\n",
        "\n",
        "def plot_activity_matrix(activity_matrix):\n",
        "    activity_matrix = activity_matrix.reindex(\n",
        "        activity_matrix.sum(axis=1).sort_values(ascending=False).index # sort by activity across time\n",
        "    )\n",
        "    \n",
        "    activity_matrix.columns = activity_matrix.columns.str.replace(\"active_\", \"\")\n",
        "    sparse_matrix = activity_matrix.astype(bool)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(sparse_matrix, cmap=\"binary\", yticklabels=False, cbar=False)\n",
        "    plt.title(f\"Activity Matrix across all clients sorted by account creation date\")\n",
        "    plt.xlabel(\"Month-Year\")\n",
        "    plt.ylabel(\"Accounts\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    active_patch = mpatches.Patch(color='black', label='Active')\n",
        "    inactive_patch = mpatches.Patch(color='white', label='Not Active')\n",
        "    plt.legend(handles=[active_patch, inactive_patch], loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "activity_matrix = prepare_activity_matrix(transactions_df)\n",
        "plot_activity_matrix(activity_matrix.copy())"
      ],
      "id": "8c8ea4d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The heatmap provided offers a visual representation of the activity matrix for clients, depicting the levels of engagement over various periods. Clients are sorted by activity, with the most active clients at the top and the least active at the bottom.\n",
        "\n",
        "There is a distinct diagonal pattern, indicating that newer accounts (those created more recently) perhaps have fewer periods of activity. This makes sense as these accounts have not had the opportunity to transact over the earlier periods displayed on the heatmap.\n",
        "\n",
        "Also interesting are some gaps withing the activity of some clients. This could be due to a variety of reasons, such as seasonal spending patterns or changes in financial circumstances.\n",
        "\n",
        "### Eligibility Criteria\n",
        "\n",
        "After constructing the activity matrix, we check for eligibility of non-cardholders to be matched with cardholders. This ensures alignment for later model construction. The eligibility criteria are as follows:\n",
        "\n",
        "1.  **Account History**: Non-cardholders must have an established history of interaction, with at least 25 months of history between account creation and card issuance (12 months (= New customer period) + 13 months (= one year of history) + 1 month (Lag period)) as described above.\n",
        "2.  **Account Creation Date**: The account creation date of a non-cardholder must precede the card issuance date of the cardholder as this is a prerequisite for the matching process to work correctly when we set the issue date for non-card holders following the intuition that nobody can have a card before the account is created."
      ],
      "id": "a598a84d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ELIGIBILITY_THRESHOLD_HIST_MONTHS = 25\n",
        "\n",
        "\n",
        "def check_eligibility_for_matching(non_cardholder, cardholder, verbose=False):\n",
        "    \"\"\"\n",
        "    Determine if a non-cardholder is eligible for matching with a cardholder.\n",
        "\n",
        "    This function checks whether the card issuance to a cardholder occurred at least\n",
        "    25 months after the non-cardholder's account was created.\n",
        "\n",
        "    Parameters:\n",
        "    - non_cardholder (pd.Series): A data series containing the non-cardholder's details.\n",
        "    - cardholder (pd.Series): A data series containing the cardholder's details.\n",
        "    - verbose (bool): If True, print detailed eligibility information. Default is False.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the non-cardholder is eligible for matching, False otherwise.\n",
        "    \"\"\"\n",
        "    if cardholder[\"card_issued\"] <= non_cardholder[\"account_created\"]:\n",
        "        return False\n",
        "\n",
        "    period_diff = (\n",
        "        cardholder[\"card_issued\"].to_period(\"M\")\n",
        "        - non_cardholder[\"account_created\"].to_period(\"M\")\n",
        "    ).n\n",
        "\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"Card issued: {cardholder['card_issued']}, Account created: {non_cardholder['account_created']}, Period diff: {period_diff}, Eligible: {period_diff >= ELIGIBILITY_THRESHOLD_HIST_MONTHS}\"\n",
        "        )\n",
        "\n",
        "    return period_diff >= ELIGIBILITY_THRESHOLD_HIST_MONTHS"
      ],
      "id": "ce1207a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matching Process\n",
        "\n",
        "Next up we will implement the matching process. Our matching utilizes the Jaccard similarity index to compare activity patterns: We compare a vector representing an existing cardholder's monthly activity against a matrix of non-cardholders' activity patterns. Here we only consider the activity from the first transaction period across all customers to the card issue date.\n",
        "\n",
        "The Jaccard similarity index is calculated as the intersection of active months divided by the union of active months between the two clients. This index ranges from 0 to 1, with higher values indicating greater overlap in activity patterns.\n",
        "\n",
        "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
        "\n",
        "The function `select_non_cardholders` randomly selects a non-cardholder match for a cardholder from the top N eligible candidates. The selection process is based on the Jaccard similarity scores calculated between the cardholder and non-cardholders. The function performs the following steps:\n",
        "\n",
        "1. **Sorting by Similarity**: The function sorts the Jaccard distances between a cardholder and non-cardholders to identify the top N similar non-cardholders.\n",
        "2. **Random Selection**: From the top N similar non-cardholders, the function randomly selects one non-cardholder match for the cardholder. This random selection helps avoid bias and ensures a fair matching process."
      ],
      "id": "575df467"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def select_non_cardholders(\n",
        "    distances,\n",
        "    eligible_non_cardholders,\n",
        "    matches,\n",
        "    matched_applicants,\n",
        "    cardholder,\n",
        "    without_card_activity,\n",
        "    top_n,\n",
        "):\n",
        "    \"\"\"\n",
        "    Randomly select a non-cardholder match for a cardholder from the top N eligible candidates.\n",
        "\n",
        "    Parameters:\n",
        "    - distances (np.array): An array of Jaccard distances between a cardholder and non-cardholders.\n",
        "    - eligible_non_cardholders (list): A list of indices for non-cardholders who are eligible for matching.\n",
        "    - matches (list): A list to which the match will be appended.\n",
        "    - matched_applicants (set): A set of indices for non-cardholders who have already been matched.\n",
        "    - cardholder (pd.Series): The data series of the current cardholder.\n",
        "    - without_card_activity (pd.DataFrame): A DataFrame of non-cardholders without card issuance.\n",
        "    - top_n (int): The number of top similar non-cardholders to consider for matching.\n",
        "\n",
        "    Returns:\n",
        "    - None: The matches list is updated in place with the selected match (Object by reference).\n",
        "    \"\"\"\n",
        "    eligible_distances = distances[eligible_non_cardholders]\n",
        "    sorted_indices = np.argsort(eligible_distances)[:top_n]\n",
        "\n",
        "    if sorted_indices.size > 0:\n",
        "        selected_index = np.random.choice(sorted_indices)\n",
        "        actual_selected_index = eligible_non_cardholders[selected_index]\n",
        "\n",
        "        if actual_selected_index not in matched_applicants:\n",
        "            matched_applicants.add(actual_selected_index)\n",
        "            applicant = without_card_activity.iloc[actual_selected_index]\n",
        "            similarity = 1 - eligible_distances[selected_index]\n",
        "\n",
        "            matches.append(\n",
        "                (cardholder[\"client_id\"], applicant[\"client_id\"], similarity)\n",
        "            )"
      ],
      "id": "5f477044",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `match_cardholders_with_non_cardholders` brings together the data preparation, matching process, and match selection steps. It performs the following operations:\n",
        "\n",
        "1. **Data Preparation**: The function prepares the activity matrix and splits the non-cardholders into two groups: those with and without cards.\n",
        "2. **Matching Process**: For each cardholder, the function calculates the Jaccard similarity between their activity pattern and those of eligible non-cardholders. It then selects the top N similar non-cardholders and randomly assigns one match per cardholder.\n",
        "3. **Match Selection**: The function selects a non-cardholder match for each cardholder based on the Jaccard similarity scores. It ensures that each non-cardholder is matched only once and that the top N similar non-cardholders are considered for matching.\n",
        "   1. The selection among the top N similar non-cardholders is done randomly to avoid bias. This process is defined in the `select_non_cardholders` function.\n",
        "   2. The function also checks for the eligibility as defined above.\n",
        "   3. If no eligible non-cardholders are found, the function prints a warning message.\n",
        "4. **Output**: The function returns a list of tuples containing the matched cardholder and non-cardholder client IDs along with their similarity scores."
      ],
      "id": "8504cde8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def match_cardholders_with_non_cardholders(non_transactional, transactions, top_n=5):\n",
        "    \"\"\"\n",
        "    Match cardholders with non-cardholders based on the similarity of their activity patterns.\n",
        "\n",
        "    The function creates an activity matrix, identifies eligible non-cardholders, calculates\n",
        "    the Jaccard similarity to find matches, and randomly selects one match per cardholder\n",
        "    from the top N similar non-cardholders.\n",
        "\n",
        "    Parameters:\n",
        "    - non_transactional (pd.DataFrame): A DataFrame containing non-cardholders.\n",
        "    - transactions (pd.DataFrame): A DataFrame containing transactional data.\n",
        "    - top_n (int): The number of top similar non-cardholders to consider for matching.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of tuples with the cardholder and matched non-cardholder client IDs and similarity scores.\n",
        "    \"\"\"\n",
        "    with_card = non_transactional[non_transactional[\"card_issued\"].notna()]\n",
        "    without_card = non_transactional[non_transactional[\"card_issued\"].isna()]\n",
        "\n",
        "    activity_matrix = prepare_activity_matrix(transactions)\n",
        "\n",
        "    with_card_activity = with_card.join(activity_matrix, on=\"account_id\", how=\"left\")\n",
        "    without_card_activity = without_card.join(\n",
        "        activity_matrix, on=\"account_id\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    matched_non_cardholders = set()\n",
        "    matches = []\n",
        "\n",
        "    for idx, cardholder in tqdm(\n",
        "        with_card_activity.iterrows(),\n",
        "        total=len(with_card_activity),\n",
        "        desc=\"Matching cardholders\",\n",
        "    ):\n",
        "        issue_period = cardholder[\"card_issued\"].to_period(\"M\")\n",
        "        eligible_cols = [\n",
        "            col\n",
        "            for col in activity_matrix\n",
        "            if col.startswith(\"active\") and pd.Period(col.split(\"_\")[1]) <= issue_period\n",
        "        ]\n",
        "\n",
        "        if not eligible_cols:\n",
        "            print(\n",
        "                f\"No eligible months found for cardholder client_id {cardholder['client_id']}.\"\n",
        "            )\n",
        "            continue\n",
        "        \n",
        "        cardholder_vector = cardholder[eligible_cols].values.reshape(1, -1)\n",
        "        non_cardholder_matrix = without_card_activity[eligible_cols].values\n",
        "        \n",
        "        cardholder_vector = np.where(cardholder_vector > 0, 1, 0).astype(bool)\n",
        "        non_cardholder_matrix = np.where(non_cardholder_matrix > 0, 1, 0).astype(bool)\n",
        "\n",
        "        assert (\n",
        "            cardholder_vector.shape[1] == non_cardholder_matrix.shape[1]\n",
        "        ), \"Dimension mismatch between cardholder and applicant activity matrix.\"\n",
        "\n",
        "        distances = pairwise_distances(\n",
        "            cardholder_vector, non_cardholder_matrix, \n",
        "            metric=\"jaccard\", n_jobs=-1 \n",
        "        ).flatten()\n",
        "        eligible_non_cardholders = [\n",
        "            i\n",
        "            for i, applicant in without_card_activity.iterrows()\n",
        "            if check_eligibility_for_matching(applicant, cardholder)\n",
        "            and i not in matched_non_cardholders\n",
        "        ]\n",
        "\n",
        "        if eligible_non_cardholders:\n",
        "            select_non_cardholders(\n",
        "                distances,\n",
        "                eligible_non_cardholders,\n",
        "                matches,\n",
        "                matched_non_cardholders,\n",
        "                cardholder,\n",
        "                without_card_activity,\n",
        "                top_n,\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"No eligible non-cardholders found for cardholder client_id {cardholder['client_id']}.\"\n",
        "            )\n",
        "\n",
        "    return matches"
      ],
      "id": "726d1316",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The matching process is executed, and the results are stored in the `matched_non_card_holders_df` DataFrame. The percentage of clients with a card issued before and after matching is calculated to assess the impact of the matching process. We expect the percentage of clients with a card issued to increase by 100% after matching, as each non-cardholder should be matched with a cardholder."
      ],
      "id": "66e851e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matched_non_card_holders_df = match_cardholders_with_non_cardholders(\n",
        "    non_transactional_w_sufficient_history_df, transactions_df\n",
        ")\n",
        "\n",
        "percentage_before_matching = non_transactional_w_sufficient_history_df[\"card_issued\"].notna().mean() * 100\n",
        "print(f\"Percentage of clients with card issued: {percentage_before_matching:.2f}%\")"
      ],
      "id": "a3a5076e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last but not least we set the artificial card issue date for each non-cardholder based on the matching results."
      ],
      "id": "9ae13e7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_artificial_issue_dates(non_transactional_df, matches):\n",
        "    \"\"\"\n",
        "    Augment the non-transactional DataFrame with artificial card issue dates based on matching results.\n",
        "\n",
        "    Each matched non-cardholder is assigned a card issue date corresponding to their matched\n",
        "    cardholder. The 'has_card' flag for each non-cardholder is updated accordingly.\n",
        "\n",
        "    Parameters:\n",
        "    - non_transactional_df (pd.DataFrame): The DataFrame of non-cardholders to augment.\n",
        "    - matches (list): A list of tuples containing the matched cardholder and non-cardholder IDs and similarity scores.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The augmented DataFrame with artificial card issue dates.\n",
        "    \"\"\"\n",
        "    augmented_df = non_transactional_df.copy()\n",
        "    augmented_df[\"has_card\"] = True\n",
        "\n",
        "    for cardholder_id, non_cardholder_id, _ in matches:\n",
        "        card_issue_date = augmented_df.loc[\n",
        "            augmented_df[\"client_id\"] == cardholder_id, \"card_issued\"\n",
        "        ].values[0]\n",
        "        augmented_df.loc[\n",
        "            augmented_df[\"client_id\"] == non_cardholder_id, [\"card_issued\", \"has_card\"]\n",
        "        ] = [card_issue_date, False]\n",
        "\n",
        "    return augmented_df\n",
        "\n",
        "matched_non_card_holders_w_issue_date_df = set_artificial_issue_dates(\n",
        "    non_transactional_w_sufficient_history_df, matched_non_card_holders_df\n",
        ")\n",
        "\n",
        "percentage_after_matching = matched_non_card_holders_w_issue_date_df[\"card_issued\"].notna().mean() * 100\n",
        "assert np.isclose(percentage_after_matching, percentage_before_matching * 2), \"Percentage of clients with card issued after matching should be double the initial percentage.\"\n",
        "print(f\"Percentage of clients with card issued after matching: {percentage_after_matching:.2f}%\")\n",
        "print(f\"Percentage without card issued after matching: {(1 - percentage_after_matching / 100) * 100:.2f}%\")"
      ],
      "id": "1a7be770",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After each non-cardholder got the artifical card issued date assigned we drop the remaining non-cardholders without a match."
      ],
      "id": "bfd8a24b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "before_len = len(matched_non_card_holders_w_issue_date_df)\n",
        "\n",
        "matched_non_card_holders_w_issue_date_df = (\n",
        "    matched_non_card_holders_w_issue_date_df.dropna(subset=[\"card_issued\"])\n",
        ")\n",
        "\n",
        "data_reduction[\"Non-cardholders without match\"] = -(\n",
        "    before_len - len(matched_non_card_holders_w_issue_date_df)\n",
        ")\n",
        "\n",
        "print(f\"Filtered out {before_len - len(matched_non_card_holders_w_issue_date_df)} non-cardholders without a match.\")\n",
        "del before_len"
      ],
      "id": "1af4ae41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In total 83% of the non-card holders were filtered out due to ineligibility or not being matched with a card holder.\n",
        "\n",
        "## Monthly Summary of Transactions\n",
        "\n",
        "After matching cardholders with non-cardholders and setting artificial card issue dates, we aggregate the transactional data on a monthly basis to remove the transactional nature of the data. The monthly aggregation makes sense for multiple reasons:\n",
        "\n",
        "- Monthly aggregation standardizes the time frame across which we analyze transactions, allowing us to compare transactional behaviors consistently across all accounts.\n",
        "- Aggregating data on a monthly level illuminates patterns that daily data might obscure. It enables us to discern trends over a broader time scale, capturing cyclical behaviors, seasonal effects, and response to macroeconomic events.\n",
        "- Daily transaction data can be \"noisy\" with random fluctuations. By considering monthly totals and averages, we reduce this noise, revealing underlying trends more clearly.\n",
        "\n",
        "The function `aggregate_transactions_monthly` simplifies financial transactions by summarizing them every month for each account. Here's a simplified explanation of how it works:\n",
        "\n",
        "1. **Sorting Transactions**: First, the function arranges the transactions chronologically by `account_id` and `date` within the `transactions_df` DataFrame. This helps in organizing transactions for each account by date.\n",
        "\n",
        "2. **Monthly Grouping**: It converts each transaction's date into a monthly period. This categorizes transactions by the month and year they occurred, making it easier to group them monthly.\n",
        "\n",
        "3. **Aggregation of Monthly Data**: The function groups these transactions by `account_id` and the `month` column. For each group, it calculates:\n",
        "   - `volume`: Total transaction amount for the month.\n",
        "   - `total_abs_amount`: Total of absolute values of all transactions, showing the total money movement regardless of direction.\n",
        "   - `transaction_count`: Number of transactions to show how active the account was.\n",
        "   - Counts of positive and negative transactions, indicating money coming in and going out.\n",
        "   - Other statistics like average, median, minimum, maximum, and standard deviation of transaction amounts, which provide insights into how transaction amounts vary.\n",
        "   - Counts of different transaction types, operations, and symbols, showing the variety of transactions.\n",
        "\n",
        "4. **Cumulative Balance Calculation**: Finally, the function calculates a running total of the `volume` to track how the account balance changes over time.\n",
        "\n",
        "This method is effective for understanding the financial behavior of accounts on a monthly basis, helping in further analyses and model building."
      ],
      "id": "4149ea3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def aggregate_transactions_monthly(df):\n",
        "    \"\"\"\n",
        "    Aggregate financial transaction data on a monthly basis per account.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): DataFrame containing financial transaction data with 'account_id', 'date', and other relevant columns.\n",
        "\n",
        "    - validate (bool): If True, validate the aggregated data. Default is True.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Monthly aggregated financial transaction data per account.\n",
        "    \"\"\"\n",
        "    df_sorted = df.sort_values(by=[\"account_id\", \"date\"]) # sort by account_id and date to ensure correct running balance calculation\n",
        "    df_sorted[\"month\"] = df_sorted[\"date\"].dt.to_period(\"M\")\n",
        "\n",
        "    monthly_aggregated_data = (\n",
        "        df_sorted.groupby([\"account_id\", \"month\"])\n",
        "        .agg(\n",
        "            volume=(\"amount\", \"sum\"),\n",
        "            total_abs_amount=(\"amount\", lambda x: x.abs().sum()),\n",
        "            transaction_count=(\"amount\", \"count\"),\n",
        "            positive_transaction_count=(\"amount\", lambda x: (x >= 0).sum()),\n",
        "            negative_transaction_count=(\"amount\", lambda x: (x < 0).sum()),\n",
        "            average_amount=(\"amount\", \"mean\"),\n",
        "            median_amount=(\"amount\", \"median\"),\n",
        "            min_amount=(\"amount\", \"min\"),\n",
        "            max_amount=(\"amount\", \"max\"),\n",
        "            std_amount=(\"amount\", \"std\"),\n",
        "            type_count=(\"transaction_type\", \"nunique\"),\n",
        "            operation_count=(\"operation\", \"nunique\"),\n",
        "            k_symbol_count=(\"k_symbol\", \"nunique\"),\n",
        "        )\n",
        "        .reset_index()\n",
        "        .sort_values(by=[\"account_id\", \"month\"])\n",
        "    )\n",
        "    \n",
        "    monthly_aggregated_data[\"balance\"] = monthly_aggregated_data.groupby(\"account_id\")[\n",
        "        \"volume\"\n",
        "    ].cumsum()\n",
        "    \n",
        "    return monthly_aggregated_data\n",
        "\n",
        "agg_transactions_monthly_df = aggregate_transactions_monthly(transactions_df)\n",
        "agg_transactions_monthly_df.describe()"
      ],
      "id": "afa43280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `validate_monthly_aggregated_transactions` function is invoked to ensure the integrity and correctness of the aggregated data through several assertions:\n",
        "\n",
        "1. The balance should consistently increase or decrease based on whether the total monthly transaction volume is positive or negative, respectively.\n",
        "2. For each account, the balance in the first month should equal the total transaction volume of that month.\n",
        "3. The sum of positive and negative transaction counts must equal the total transaction count for each month.\n",
        "4. The number of unique accounts in the aggregated data should match that in the original dataset.\n",
        "5. The final balances of accounts in the aggregated data should closely match their last recorded transactions in the original dataset."
      ],
      "id": "86ad528c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def validate_monthly_aggregated_transactions(aggregated_data, original_df):\n",
        "    \"\"\"\n",
        "    Validate the integrity and correctness of aggregated monthly financial transactions.\n",
        "\n",
        "    Parameters:\n",
        "    - aggregated_data (pd.DataFrame): Aggregated monthly transaction data.\n",
        "    - original_df (pd.DataFrame): Original dataset of financial transactions.\n",
        "\n",
        "    Raises:\n",
        "    - AssertionError: If validation conditions are not met.\n",
        "    \"\"\"\n",
        "    # Assertion 1: Balance should consistently increase or decrease based on total monthly transaction volume\n",
        "    assert (aggregated_data[\"volume\"] >= 0).all() == (\n",
        "        aggregated_data[\"balance\"].diff() >= 0\n",
        "    ).all(), \"If the total amount is positive, the balance should go up.\"\n",
        "    assert (aggregated_data[\"volume\"] < 0).all() == (\n",
        "        aggregated_data[\"balance\"].diff() < 0\n",
        "    ).all(), \"If the total amount is negative, the balance should go down.\"\n",
        "\n",
        "    # Assertion 2: Balance in the first month should equal the total transaction volume of that month\n",
        "    first_month = aggregated_data.groupby(\"account_id\").nth(0)\n",
        "    assert (\n",
        "        first_month[\"volume\"] == first_month[\"balance\"]\n",
        "    ).all(), \"The balance should equal the volume for the first month.\"\n",
        "\n",
        "    # Assertion 3: The sum of positive and negative transaction counts should equal the total transaction count\n",
        "    assert (\n",
        "        aggregated_data[\"positive_transaction_count\"]\n",
        "        + aggregated_data[\"negative_transaction_count\"]\n",
        "        == aggregated_data[\"transaction_count\"]\n",
        "    ).all(), \"The sum of positive and negative transaction counts should equal the total transaction count.\"\n",
        "\n",
        "    # Assertion 4: The number of unique accounts in the aggregated data should match that in the original dataset\n",
        "    assert (\n",
        "        aggregated_data[\"account_id\"].nunique() == original_df[\"account_id\"].nunique()\n",
        "    ), \"The number of unique account_ids in the aggregated DataFrame should be the same as the original DataFrame.\"\n",
        "\n",
        "    # Assertion 5: The final balances of accounts in the aggregated data should closely match their last recorded transactions in the original dataset\n",
        "    assert (\n",
        "        pd.merge(\n",
        "            aggregated_data.groupby(\"account_id\")\n",
        "            .last()\n",
        "            .reset_index()[[\"account_id\", \"balance\"]],\n",
        "            original_df[\n",
        "                original_df.groupby(\"account_id\")[\"date\"].transform(\"max\")\n",
        "                == original_df[\"date\"]\n",
        "            ][[\"account_id\", \"balance\"]],\n",
        "            on=\"account_id\",\n",
        "            suffixes=(\"_final\", \"_last\"),\n",
        "        )\n",
        "        .apply(\n",
        "            lambda x: np.isclose(x[\"balance_final\"], x[\"balance_last\"], atol=5), axis=1 # allow for small differences due to floating point precision\n",
        "        )\n",
        "        .any()\n",
        "    ), \"Some accounts' final balances do not match their last transactions.\"\n",
        "\n",
        "\n",
        "validate_monthly_aggregated_transactions(agg_transactions_monthly_df, transactions_df)"
      ],
      "id": "70745420",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis: Aggregated Monthly Transactions\n",
        "\n",
        "Further we explore the aggregated monthly transactions to gain insights into the financial behavior of accounts over time. We will visualize the monthly transaction volume, balance, and number of transactions to understand how these metrics evolve over time.\n",
        "\n",
        "## Monthly Balance Difference and Volume\n",
        "\n",
        "The `plot_monthly_balance_diff_and_volume` function visualizes the monthly balance difference and volume for a specific account. The balance difference is calculated as the difference between the current month's balance and the previous month's balance. This metric helps identify the impact of monthly transactions on the account balance. The volume represents the total transaction amount for each month."
      ],
      "id": "1b57ca0b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_monthly_balance_diff_and_volume(\n",
        "    transactions_monthly, account_id \n",
        "):\n",
        "    account_transactions = transactions_monthly[\n",
        "        transactions_monthly[\"account_id\"] == account_id\n",
        "    ].sort_values(by=\"month\")\n",
        "    account_transactions[\"balance_diff\"] = account_transactions[\"balance\"].diff()\n",
        "\n",
        "    plt.figure(figsize=(9.5, 6))\n",
        "\n",
        "    plt.plot(\n",
        "        account_transactions[\"month\"].astype(str),\n",
        "        account_transactions[\"balance_diff\"],\n",
        "        marker=\"o\",\n",
        "        label=\"Balance Difference\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        account_transactions[\"month\"].astype(str),\n",
        "        account_transactions[\"volume\"],\n",
        "        marker=\"x\",\n",
        "        linestyle=\"--\",\n",
        "        label=\"Volume\",\n",
        "    )\n",
        "\n",
        "    plt.title(f\"Monthly Balance Difference and Volume for Account {account_id}\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.xticks(rotation=90, fontsize=7)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_monthly_balance_diff_and_volume(agg_transactions_monthly_df, 2)"
      ],
      "id": "add8ed8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot gives a clear picture of how money moves in and out of an account each month and how these movements affect the overall balance. It does this by showing two things:\n",
        "\n",
        "- **Balance Difference**: This line shows whether the account balance went up or down each month. If the line goes up, it means the account gained money that month. If it goes down, the account lost money.\n",
        "- **Volume**: This line shows the total amount of money that moved in the account each month, regardless of whether it was coming in or going out.\n",
        "\n",
        "There is a direct link between the amount of money moved (volume) and changes in the account balance. High incoming money should lead to an uptick in the balance, and lots of outgoing money should lead to a downturn. It further confirms the aggregation made in the previous step was correct.\n",
        "\n",
        "## Monthly Balance and Volume\n",
        "\n",
        "Instead of the difference in balance, we can also look at the monthly balance and volume directly. The `plot_monthly_transactions_balance_and_volume` function visualizes the monthly transactions and balance for a specific account."
      ],
      "id": "58224653"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_monthly_transactions_balance_and_volume(agg_transactions_monthly, account_id):\n",
        "    account_transactions = agg_transactions_monthly[\n",
        "        agg_transactions_monthly[\"account_id\"] == account_id\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(9.5, 6))\n",
        "\n",
        "    plt.plot(\n",
        "        account_transactions[\"month\"].astype(str),\n",
        "        account_transactions[\"volume\"],\n",
        "        marker=\"o\",\n",
        "        label=\"Volume\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        account_transactions[\"month\"].astype(str),\n",
        "        account_transactions[\"balance\"],\n",
        "        marker=\"x\",\n",
        "        linestyle=\"--\",\n",
        "        label=\"Balance\",\n",
        "    )\n",
        "\n",
        "    plt.title(f\"Monthly Transactions and Balance for Account {account_id}\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.xticks(rotation=90, fontsize=7)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 2)"
      ],
      "id": "df55277b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This visualization offers a snapshot of an accounts activity over time by comparing money movement each month with the overall account balance. Similarly to the previous plot, it shows two key metrics:\n",
        "\n",
        "- **Volume**: How much money came in or went out of the account each month. Incoming money is shown as up, and outgoing money as down.\n",
        "- **Balance**: The total money in the account at the end of each month, showing how it's changed over time due to the monthly transactions.\n",
        "\n",
        "It shows how the monthly money movement impacts the account's growing or shrinking balance. For example, a few months of high income should visibly increase the balance. It further validates the aggregation made in the previous step.\n",
        "\n",
        "## Deliverable: Closer Look at Account 14\n",
        "\n",
        "Let's take a closer look at the monthly transactions, balance, and volume for account 14 as requested by the task."
      ],
      "id": "896d64dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 14)"
      ],
      "id": "7e61ab18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Account 14 shows a rather conservative transaction history. The spending habits are all withing range of 10k to -10k per month. We can see little volatility, the account shows a slight trend of growing.\n",
        "\n",
        "## Deliverable: Closer Look at Account 18\n",
        "\n",
        "Let's also examine the monthly transactions, balance, and volume for account 18."
      ],
      "id": "da417c7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_monthly_transactions_balance_and_volume(agg_transactions_monthly_df, 18)"
      ],
      "id": "1b2bd7e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Account 18 paints a different picture in comparison to account 14.\n",
        "\n",
        "The volatility here is a lot higher, indicating a potential for a business account or high income household. Especially March 1994 to December 1994 show some volatile transaction habits.\n",
        "\n",
        "Looking at the balance and volume per month for the accounts 14 and 18 we can notice different patterns. Account 14 shows a rather conservative transaction history with little volatility and a slight trend of growing. Account 18, on the other hand, shows a lot more volatility, indicating a potential business account or high-income household. This highlights the importance of understanding the financial behavior of accounts to identify patterns and trends that can inform decision-making. Ultimately, this is the job of the models we will build in the next steps.\n",
        "\n",
        "# Pivot Transactions: Rolling Up to Monthly Aggregates\n",
        "\n",
        "Now we pivot the aggregated transaction data to have each account as a row and the months leading up to card issuance as columns. This transformation aligns with the goal for a single record per account, summarizing transactional behavior in the months before card issuance.\n",
        "\n",
        "The `pivot_transactions` function aggregates monthly transaction data and merges it with non-transactional account data. It focuses on the time frame leading up to the card issuance, filtering transactions based on a specified range of months before card issuance and aggregating various transaction metrics.\n",
        "\n",
        "We are mainly interested in the time frame 2 months to 13 months before card issuance. This range allows us to capture transactional behavior in the year leading up to the card issuance, ignoring the month directly before the card issuance to avoid any potential bias from the card issuance itself."
      ],
      "id": "b24dd41b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pivot_transactions(\n",
        "    non_transactional, transactions_monthly, months_before_card_range=(2, 13)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregate monthly transaction data and merge it with non-transactional account data,\n",
        "    focusing on the time frame leading up to the card issuance.\n",
        "\n",
        "    This function merges monthly transaction data with non-transactional data to associate each\n",
        "    transaction with the respective account and card issued date. It then filters transactions based\n",
        "    on a specified range of months before card issuance and aggregates various transaction metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - non_transactional (pd.DataFrame): A DataFrame containing non-transactional account data. This is only used to map card issuance dates to transactions.\n",
        "    - transactions_monthly (pd.DataFrame): A DataFrame containing monthly transaction data.\n",
        "    - months_before_card_range (tuple): A tuple specifying the inclusive range of months before card\n",
        "                                        issuance to filter the transactions for aggregation.\n",
        "\n",
        "    The aggregation includes the sum of volume and transaction counts, as well as the mean and other\n",
        "    statistical measures of transaction amounts, for each account within the specified months before\n",
        "    card issuance.\n",
        "\n",
        "    The resulting DataFrame is pivoted to have 'account_id' as rows and the months before card\n",
        "    issuance as columns, with aggregated metrics as values. Column names are constructed to\n",
        "    describe the month and the metric represented.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The final aggregated and pivoted dataset ready for analysis, with each row\n",
        "                    representing an account and each column a specific metric in the months before\n",
        "                    card issuance.\n",
        "    \"\"\"\n",
        "    merged_df = transactions_monthly.merge(\n",
        "        non_transactional[[\"account_id\"]], on=\"account_id\"\n",
        "    )\n",
        "\n",
        "    merged_df[\"card_issued_date\"] = merged_df[\"account_id\"].map(\n",
        "        non_transactional.set_index(\"account_id\")[\"card_issued\"]\n",
        "    )\n",
        "    merged_df[\"months_before_card\"] = merged_df.apply(\n",
        "        lambda row: (row[\"card_issued_date\"].to_period(\"M\") - row[\"month\"]).n, axis=1\n",
        "    )\n",
        "\n",
        "    start_month, end_month = months_before_card_range\n",
        "    filtered_df = merged_df.query(f\"{start_month} <= months_before_card <= {end_month}\")\n",
        "\n",
        "    aggregated_data = (\n",
        "        filtered_df.groupby([\"account_id\", \"months_before_card\"])\n",
        "        .agg(\n",
        "            {\n",
        "                \"volume\": \"sum\",\n",
        "                \"total_abs_amount\": \"sum\",\n",
        "                \"transaction_count\": \"sum\",\n",
        "                \"positive_transaction_count\": \"sum\",\n",
        "                \"negative_transaction_count\": \"sum\",\n",
        "                \"average_amount\": \"mean\",\n",
        "                \"median_amount\": \"median\",\n",
        "                \"min_amount\": \"min\",\n",
        "                \"max_amount\": \"max\",\n",
        "                \"std_amount\": \"std\",\n",
        "                \"type_count\": \"sum\",\n",
        "                \"operation_count\": \"sum\",\n",
        "                \"k_symbol_count\": \"sum\",\n",
        "                \"balance\": \"mean\",\n",
        "            }\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    pivoted_data = aggregated_data.pivot(\n",
        "        index=\"account_id\", columns=\"months_before_card\"\n",
        "    )\n",
        "    \n",
        "    pivoted_data.columns = [\n",
        "        \"_\".join([\"M\", str(col[1]), col[0]]) for col in pivoted_data.columns.values\n",
        "    ]\n",
        "    return pivoted_data.reset_index()\n",
        "\n",
        "transactions_pivoted_df = pivot_transactions(\n",
        "    matched_non_card_holders_w_issue_date_df, agg_transactions_monthly_df\n",
        ")\n",
        "transactions_pivoted_df.describe()"
      ],
      "id": "b05bf96d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merge everything together\n",
        "\n",
        "Finally, we merge the non-transactional data with the pivoted transactional data to create the final golden record. This record contains all relevant information for each account, including the aggregated transactional data for each month leading up to the card issuance date.\n",
        "\n",
        "The resulting DataFrame has one row per account, with columns representing various metrics for each month before card issuance along with non-transactional data like client and account IDs, card issuance dates, and other relevant information.\n",
        "\n",
        "We can merge the non-transactional data with the pivoted transactional data using the `account_id` as the common key. As each transaction is linked to an account, this key ensures that the transactional data is correctly associated with the respective account."
      ],
      "id": "96b77761"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "golden_record_df = matched_non_card_holders_w_issue_date_df.merge(\n",
        "    transactions_pivoted_df, on=\"account_id\", how=\"left\" # left join as we \n",
        ")\n",
        "\n",
        "data_reduction[\"Final Golden Record\"] = len(golden_record_df)\n",
        "golden_record_df.head()"
      ],
      "id": "a3851281",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the first few rows of the final golden record, we can see the aggregated transactional data for each account, with columns representing various metrics for each month leading up to the card issuance date.\n",
        "\n",
        "Additionally we can verify the uniqueness of `client_id` and `account_id` in the final DataFrame."
      ],
      "id": "a1a9df2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert golden_record_df[\n",
        "    \"client_id\"\n",
        "].is_unique, \"Each client_id should appear exactly once in the final DataFrame.\"\n",
        "\n",
        "assert golden_record_df[\n",
        "    \"account_id\"\n",
        "].is_unique, \"Each account_id should appear exactly once in the final DataFrame.\""
      ],
      "id": "9a6da3b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.title(\"Number of Clients by Card Issuance Status\")\n",
        "sns.countplot(x=\"has_card\", data=golden_record_df)\n",
        "plt.xlabel(\"Card Issued\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "id": "c3aae7c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the card issuance status we can see that the number of clients with a card issued is equal to the number of clients without a card issued."
      ],
      "id": "7dace759"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.title(\"Distribution of Card Issuance Dates\")\n",
        "sns.histplot(\n",
        "    golden_record_df, x=\"card_issued\", hue=\"has_card\", kde=True, bins=30, alpha=0.5\n",
        ")\n",
        "plt.xlabel(\"Card Issuance Date\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "id": "e66f7c19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of card issuance dates shows that the card issuance process was spread out over time, with an expected identical distribution for clients with and without cards issued. This makes sense as we set the artificial card issue date for each non-cardholder based on the matching results.\n",
        "\n",
        "# Data Reduction Summary\n",
        "\n",
        "The following waterfall chart visualizes the data reduction process, highlighting the number of records retained or lost at each stage."
      ],
      "id": "84288e75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_reduction_df = pd.DataFrame(\n",
        "    list(data_reduction.items()), columns=[\"Category\", \"Amount\"]\n",
        ")\n",
        "colors = [\"skyblue\" if amt >= 0 else \"orange\" for amt in data_reduction_df[\"Amount\"]]\n",
        "\n",
        "fig = go.Figure(\n",
        "    go.Waterfall(\n",
        "        name=\"20\",\n",
        "        orientation=\"v\",\n",
        "        measure=[\"relative\"] * (len(data_reduction_df) - 1) + [\"total\"],\n",
        "        x=data_reduction_df[\"Category\"],\n",
        "        textposition=\"outside\",\n",
        "        text=[f\"{amt:,.0f}\" for amt in data_reduction_df[\"Amount\"]],\n",
        "        y=data_reduction_df[\"Amount\"],\n",
        "        connector={\"line\": {\"color\": \"black\", \"width\": 2}},\n",
        "        decreasing={\"marker\": {\"color\": \"orange\"}},\n",
        "        increasing={\"marker\": {\"color\": \"skyblue\"}},\n",
        "        totals={\"marker\": {\"color\": \"skyblue\"}},\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Enhanced Data Reduction Waterfall Chart\",\n",
        "    xaxis=dict(title=\"Category\"),\n",
        "    yaxis=dict(title=\"Amount\", range=[0, 5500]),\n",
        "    waterfallgap=0.3,\n",
        ")\n",
        "fig.show()"
      ],
      "id": "1c13c79d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The waterfall chart provides a visual representation of the data reduction process, illustrating the number of records retained or lost at each stage. The chart shows the reduction in the number of records from the initial dataset to the final golden record, highlighting the impact of each step in the data preparation pipeline:\n",
        "\n",
        "- **Initial Dataset**: The starting point with the full dataset of 4500 accounts/records.\n",
        "- **Junior Accounts**: The removal of junior accounts, reducing the dataset by 145 records.\n",
        "- **Clients without sufficient history**: The elimination of clients without sufficient transactional history, resulting in a reduction of 419 records.\n",
        "- **Non-cardholders without match**: The filtering out of non-cardholders without a match, leading to a decrease of 3'280 records.\n",
        "- **Final Golden Record**: The final dataset with 656 records, each representing a unique account with aggregated transactional data for each month leading up to the card issuance date.\n",
        "\n",
        "# Exploratory Data Analysis: Golden Record\n",
        "\n",
        "With the final golden record in hand, we can now perform exploratory data analysis to gain insights into the financial behavior of cardholders and non-cardholders.\n",
        "\n",
        "## Comparing Cardholders and Non-Cardholders\n",
        "\n",
        "We will focus on comparing the financial behavior of cardholders and non-cardholders to identify any significant differences in their transactional patterns. This analysis can help us understand how cardholders differ from non-cardholders in terms of transaction volume, balance, and other financial metrics giving us an impression of the financial behavior of cardholders and non-cardholders.\n",
        "\n",
        "### Trends in Financial Metrics\n",
        "\n",
        "The function `plot_trends_with_medians` generates line graphs for average monthly values and annotates medians for specified ranges. This visualization helps identify trends in financial metrics over time and highlights the median values for specific periods, providing insights into the distribution of values."
      ],
      "id": "abdd5bb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "golden_cardholders = golden_record_df[golden_record_df[\"has_card\"]]\n",
        "golden_non_cardholders = golden_record_df[~golden_record_df[\"has_card\"]]\n",
        "\n",
        "\n",
        "def plot_trends_with_medians(\n",
        "    cardholders, non_cardholders, columns, title, median_ranges\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots line graphs for average monthly values and annotates medians for specified ranges,\n",
        "    adjusting x-axis indices to match the month sequence from the start.\n",
        "\n",
        "    Parameters:\n",
        "    - cardholders (pd.DataFrame): DataFrame containing data for cardholders.\n",
        "    - non_cardholders (pd.DataFrame): DataFrame containing data for non-cardholders.\n",
        "    - columns (list of str): List of column names ordered by time.\n",
        "    - title (str): Title for the plot.\n",
        "    - median_ranges (list of tuples): Each tuple contains start and end indices for calculating medians.\n",
        "    \"\"\"\n",
        "    cardholder_avgs = cardholders[columns].mean()\n",
        "    non_cardholder_avgs = non_cardholders[columns].mean()\n",
        "\n",
        "    months = list(range(1, 1 + len(columns)))\n",
        "    plt.figure()\n",
        "    plt.plot(\n",
        "        months,\n",
        "        cardholder_avgs.values,\n",
        "        marker=\"o\",\n",
        "        linestyle=\"-\",\n",
        "        color=\"blue\",\n",
        "        label=\"Cardholders\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        months,\n",
        "        non_cardholder_avgs.values,\n",
        "        marker=\"o\",\n",
        "        linestyle=\"-\",\n",
        "        color=\"orange\",\n",
        "        label=\"Non-Cardholders\",\n",
        "    )\n",
        "\n",
        "    for start, end in median_ranges:\n",
        "        median_cardholder = cardholders[columns[start : end + 1]].median().median()\n",
        "        median_non_cardholder = (\n",
        "            non_cardholders[columns[start : end + 1]].median().median()\n",
        "        )\n",
        "        plt.hlines(\n",
        "            median_cardholder,\n",
        "            months[start],\n",
        "            months[end],\n",
        "            colors=\"darkblue\",\n",
        "            linestyles=\"--\",\n",
        "            label=f\"Median {start+1}-{end+1} (Cardholders): {median_cardholder:.2f}\",\n",
        "        )\n",
        "        plt.hlines(\n",
        "            median_non_cardholder,\n",
        "            months[start],\n",
        "            months[end],\n",
        "            colors=\"red\",\n",
        "            linestyles=\"--\",\n",
        "            label=f\"Median {start+1}-{end+1} (Non-Cardholders): {median_non_cardholder:.2f}\",\n",
        "        )\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(months, labels=[f\"M_{month}\" for month in months])  # Proper month labels\n",
        "    plt.show()"
      ],
      "id": "50677d55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monthly Balance Trends"
      ],
      "id": "58640e77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_trends_with_medians(\n",
        "    golden_cardholders,\n",
        "    golden_non_cardholders,\n",
        "    [f\"M_{i}_balance\" for i in range(2, 14)],\n",
        "    \"Monthly Balance Trends\",\n",
        "    [(0, 2), (9, 11)]\n",
        ")"
      ],
      "id": "79ac4c2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starting with the monthly balance trends, we can observe how the average balance changes over time for cardholders and non-cardholders. The line graph shows the average monthly balance for each group, with annotations indicating the median balance for specific periods.\n",
        "\n",
        "It is interesting to note that the median balance for cardholders is consistently higher than that of non-cardholders, indicating a potential difference in financial stability or spending habits between the two groups over time.\n",
        "\n",
        "### Monthly Volume Trends"
      ],
      "id": "fda71b6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_trends_with_medians(\n",
        "    golden_cardholders,\n",
        "    golden_non_cardholders,\n",
        "    [f\"M_{i}_volume\" for i in range(2, 14)],\n",
        "    \"Monthly Volume Trends\",\n",
        "    [(0, 2), (9, 11)]\n",
        ")"
      ],
      "id": "cd87d009",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The monthly volume trends show the average monthly transaction volume for cardholders and non-cardholders over time. The line graph illustrates how the transaction volume changes each month, with annotations highlighting the median volume for specific periods.\n",
        "\n",
        "The median volume for cardholders is higher than that of non-cardholders, indicating a potential difference in transactional activity or spending patterns between the two groups. However, when sole looking at the volume, the difference is not as pronounced as with the balance and shows very volatile behavior. Generally this could come from the fact that the volume is the sum of all transactions, which can be naturally very volatile.\n",
        "\n",
        "### Monthly Transaction Count Trends"
      ],
      "id": "83651632"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_trends_with_medians(\n",
        "    golden_cardholders,\n",
        "    golden_non_cardholders,\n",
        "    [f\"M_{i}_transaction_count\" for i in range(2, 14)],\n",
        "    \"Monthly Transaction Count Trends\",\n",
        "    [(0, 2), (9, 11)]\n",
        ")"
      ],
      "id": "3d9a75e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The monthly transaction count trends show the average number of transactions per month for cardholders and non-cardholders over time. The line graph displays how the transaction count changes each month, with annotations indicating the median count for specific periods.\n",
        "\n",
        "The median transaction count for cardholders is higher than that of non-cardholders, suggesting a difference in transactional activity or spending habits between the two groups. This is in line with the volume trends, indicating that cardholders tend to have more transactions on average than non-cardholders.\n",
        "\n",
        "### Monthly Positive and Negative Transaction Count Trends"
      ],
      "id": "13426a26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_trends_with_medians(\n",
        "    golden_cardholders,\n",
        "    golden_non_cardholders,\n",
        "    [f\"M_{i}_positive_transaction_count\" for i in range(2, 14)],\n",
        "    \"Monthly Positive Transaction Count Trends\",\n",
        "    [(0, 2), (9, 11)]\n",
        ")"
      ],
      "id": "e303a46f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The monthly transaction count trends show the average number of transactions per month for cardholders and non-cardholders over time. The line graph displays how the transaction count changes each month, with annotations indicating the median count for specific periods.\n",
        "\n",
        "The median transaction count for cardholders is only slightly higher than that of non-cardholders, indicating that the difference in transactional activity is not as pronounced for positive transactions. This suggests that both groups have similar patterns of money coming in each month."
      ],
      "id": "77f05cc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_trends_with_medians(\n",
        "    golden_cardholders,\n",
        "    golden_non_cardholders,\n",
        "    [f\"M_{i}_negative_transaction_count\" for i in range(2, 14)],\n",
        "    \"Monthly Negative Transaction Count Trends\",\n",
        "    [(0, 2), (9, 11)]\n",
        ")"
      ],
      "id": "6e576d5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The monthly negative transaction count trends show the average number of transactions per month for cardholders and non-cardholders over time. The line graph displays how the transaction count changes each month, with annotations indicating the median count for specific periods.\n",
        "\n",
        "The picture is pretty similar to the positive transaction count trends, indicating that both groups have similar patterns of money going out each month. This suggests that the difference in transactional activity between cardholders and non-cardholders is not as pronounced for negative transactions as well as for positive transactions.\n",
        "\n",
        "### Loan Amount"
      ],
      "id": "1eb7e178"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "avg_loan_amount_cardholders = golden_cardholders[\"loan_amount\"].mean()\n",
        "avg_loan_amount_non_cardholders = golden_non_cardholders[\"loan_amount\"].mean()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Average Loan Amount by Card Issuance Status\")\n",
        "sns.barplot(\n",
        "    x=[\"Cardholders\", \"Non-Cardholders\"],\n",
        "    y=[avg_loan_amount_cardholders, avg_loan_amount_non_cardholders],\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Average Loan Amount\")\n",
        "plt.show()"
      ],
      "id": "f17e3d38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average loan amount for cardholders is higher than that of non-cardholders, indicating that non-cardholders tend to have higher loan amounts. Yet, this difference is not as pronounced as with the balance, volume, and transaction count trends and not as significant."
      ],
      "id": "54bf6c28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "golden_record_df.to_parquet(\"temp/golden_record.parquet\")"
      ],
      "id": "5951f191",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "SEED = 1337\n",
        "\n",
        "golden_record_df = pd.read_parquet(\"temp/golden_record.parquet\")\n",
        "gs_cache_file = \"data/grid_search_cache.pkl\"\n",
        "reduced_model_cache_file = \"data/reduced_best_model.pkl\"\n",
        "\n",
        "np.random.seed(1337)\n",
        "random.seed(1337)"
      ],
      "id": "6780118c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Partitioning\n",
        "\n",
        "The data is split in a 80/20 ratio for training and testing purposes. The stratification ensures that the distribution of the target variable is maintained in both sets. When actually training the models, we will additionally use cross-validation to ensure robust evaluation.\n",
        "\n",
        "Additionally, we will create a `DataModule` class to encapsulate the training and testing data, as well as the feature columns used in the model. This class will help us manage the data and features throughout the model training and evaluation process.\n"
      ],
      "id": "3ad0cacc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class DataModule:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test, feature_columns=None):\n",
        "        self.feature_columns = (\n",
        "            feature_columns if feature_columns is not None else X_train.columns\n",
        "        )\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "def create_data_module(df, feature_cols, target_col=\"has_card\", test_size=0.2):\n",
        "    X = df.drop(columns=[target_col])[feature_cols]\n",
        "    y = df[target_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, shuffle=True\n",
        "    )\n",
        "\n",
        "    return DataModule(X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "data_module = create_data_module(\n",
        "    golden_record_df, golden_record_df.drop(columns=[\"has_card\"]).columns\n",
        ")\n",
        "\n",
        "print(f\"Train set size: {len(data_module.X_train)}\")\n",
        "print(f\"Test set size: {len(data_module.X_test)}\")\n",
        "\n",
        "print(f\"Train set distribution:\\n{data_module.y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test set distribution:\\n{data_module.y_test.value_counts(normalize=True)}\")"
      ],
      "id": "a0eff8c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see the distribution of the target variable is maintained in both sets after the split. The ratios are as specified in the 80/20 split.\n",
        "\n",
        "\n",
        "# Model Construction\n",
        "\n",
        "We will now construct a pipeline for training and evaluating machine learning models. The pipeline will handle preprocessing, model training, cross-validation, and evaluation. We will use this pipeline to train and evaluate multiple candidate models.\n",
        "\n",
        "## Pipeline for Training and Evaluation\n",
        "\n",
        "The `train_evaluate_model` function is designed to streamline the process of training and evaluating machine learning models. It performs the following steps:\n",
        "\n",
        "1. **Preprocessing**: The function automatically handles numerical and categorical features, imputing missing values, scaling numerical features, and one-hot encoding categorical features.\n",
        "2. **Model Training**: The specified model is trained on the training data.\n",
        "3. **Cross-Validation**: The model is evaluated using cross-validation with specified evaluation metrics.\n",
        "4. **Model Evaluation**: The model is evaluated on the test set using various metrics, including accuracy, F1 score, AUC-ROC, precision, and recall.\n",
        "\n",
        "The pipeline is flexible and can accommodate various models and feature sets, making it a versatile tool for model development and evaluation. It returns a summary of evaluation metrics for both training and test sets, as well as the true labels and predicted probabilities for the test set.\n",
        "\n",
        "Additionally, the pipeline supports feature selection using Recursive Feature Elimination with Cross-Validation (RFECV). This feature selection method automatically selects the most relevant features based on the model's performance during cross-validation. The selected features can be retrieved from the pipeline after training.\n",
        "\n",
        "Last but not least, the pipeline supports hyperparameter tuning using Grid Search with Cross-Validation. This functionality allows for optimizing the model's hyperparameters to improve performance. The best hyperparameters can be retrieved after training the model."
      ],
      "id": "6575ed2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    make_scorer,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "import scikitplot as skplt\n",
        "import dalex as dx\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_module,\n",
        "        model,\n",
        "        cv=10,\n",
        "        select_features=False,\n",
        "        param_grid=None,\n",
        "        verbose=False,\n",
        "        n_jobs=-1,\n",
        "    ):\n",
        "        self.data_module = data_module\n",
        "        self.model = model\n",
        "        self.cv = cv\n",
        "        self.verbose = verbose\n",
        "        self.preprocessor = self._create_preprocessor()\n",
        "        self.select_features = select_features\n",
        "        self.param_grid = param_grid\n",
        "        self.n_jobs = n_jobs\n",
        "        self.pipeline = None\n",
        "        self.train_metrics_report = None\n",
        "        self.test_metrics_report = None\n",
        "\n",
        "    def _create_preprocessor(self):\n",
        "        numerical_features = [\n",
        "            col\n",
        "            for col in self.data_module.X_train.columns\n",
        "            if self.data_module.X_train[col].dtype in [\"int64\", \"float64\"]\n",
        "        ]\n",
        "        categorical_features = [\n",
        "            col\n",
        "            for col in self.data_module.X_train.columns\n",
        "            if col not in numerical_features\n",
        "        ]\n",
        "\n",
        "        other_features = [\n",
        "            col\n",
        "            for col in self.data_module.X_train.columns\n",
        "            if col not in numerical_features + categorical_features\n",
        "        ]\n",
        "        if len(other_features) > 0:\n",
        "            raise ValueError(\n",
        "                f\"Columns with unsupported data types found: {other_features}\"\n",
        "            )\n",
        "\n",
        "        numerical_pipeline = Pipeline(\n",
        "            [(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
        "        )\n",
        "\n",
        "        categorical_pipeline = Pipeline(\n",
        "            [\n",
        "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", numerical_pipeline, numerical_features),\n",
        "                (\"cat\", categorical_pipeline, categorical_features),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        model_pipeline_steps = [(\"model\", self.model)]\n",
        "        if self.select_features:\n",
        "            model_pipeline_steps.insert(\n",
        "                0,\n",
        "                (\n",
        "                    \"feature_selection\",\n",
        "                    RFECV(self.model, verbose=3 if self.verbose else 0, cv=self.cv),\n",
        "                )\n",
        "            )\n",
        "            \n",
        "        model_pipeline = Pipeline(model_pipeline_steps)\n",
        "        \n",
        "        if self.param_grid is not None:\n",
        "            model_pipeline = GridSearchCV(\n",
        "                model_pipeline,\n",
        "                self.param_grid,\n",
        "                cv=self.cv,\n",
        "                verbose=3 if self.verbose else 0,\n",
        "                n_jobs=self.n_jobs\n",
        "            )\n",
        "\n",
        "        self.pipeline = Pipeline(\n",
        "            [(\"preprocessor\", self.preprocessor), (\"model_pipeline\", model_pipeline)]\n",
        "        )\n",
        "\n",
        "        self.pipeline.fit(self.data_module.X_train, self.data_module.y_train)\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def get_scoring_metrics():\n",
        "        return [\"accuracy\", \"f1_macro\", \"roc_auc\", \"precision\", \"recall\"]\n",
        "\n",
        "    def eval_train(self):\n",
        "        scoring = {\n",
        "            \"accuracy\": \"accuracy\",\n",
        "            \"f1_macro\": make_scorer(f1_score),\n",
        "            \"roc_auc\": \"roc_auc\",\n",
        "            \"precision\": make_scorer(precision_score),\n",
        "            \"recall\": make_scorer(recall_score),\n",
        "        }\n",
        "\n",
        "        cv_results = cross_validate(\n",
        "            self.pipeline,\n",
        "            self.data_module.X_train,\n",
        "            self.data_module.y_train,\n",
        "            scoring=scoring,\n",
        "            cv=self.cv,\n",
        "            return_train_score=False,\n",
        "            n_jobs=self.n_jobs,\n",
        "            verbose=3 if self.verbose else 0,\n",
        "            return_estimator=True,\n",
        "            return_indices=True,\n",
        "            error_score=\"raise\",\n",
        "        )\n",
        "\n",
        "        self.train_metrics_report = {\n",
        "            metric: {\n",
        "                \"folds\": cv_results[f\"test_{metric}\"].tolist(),\n",
        "                \"mean\": cv_results[f\"test_{metric}\"].mean(),\n",
        "                \"std\": cv_results[f\"test_{metric}\"].std(),\n",
        "            }\n",
        "            for metric in scoring\n",
        "        }\n",
        "\n",
        "        roc_data = []\n",
        "        for i in range(self.cv):\n",
        "            estimator = cv_results[\"estimator\"][i]\n",
        "            train_indices, test_indices = (\n",
        "                cv_results[\"indices\"][\"train\"][i],\n",
        "                cv_results[\"indices\"][\"test\"][i],\n",
        "            )\n",
        "\n",
        "            true_labels = self.data_module.y_train.iloc[test_indices]\n",
        "            y_pred_proba = estimator.predict_proba(\n",
        "                self.data_module.X_train.iloc[test_indices]\n",
        "            )\n",
        "            roc_data.append((true_labels, y_pred_proba))\n",
        "\n",
        "        self.train_metrics_report[\"roc_data\"] = roc_data\n",
        "\n",
        "        return self\n",
        "\n",
        "    def eval_test(self):\n",
        "        X_test, y_test = self.data_module.X_test, self.data_module.y_test\n",
        "        y_pred_proba = (\n",
        "            self.pipeline.predict_proba(X_test)[:, 1]\n",
        "            if hasattr(self.pipeline, \"predict_proba\")\n",
        "            else np.nan\n",
        "        )\n",
        "        test_metrics = {\n",
        "            \"accuracy\": self.pipeline.score(X_test, y_test),\n",
        "            \"f1_macro\": f1_score(\n",
        "                y_test, self.pipeline.predict(X_test), average=\"macro\"\n",
        "            ),\n",
        "            \"roc_auc\": (\n",
        "                roc_auc_score(y_test, y_pred_proba)\n",
        "                if hasattr(self.pipeline, \"predict_proba\")\n",
        "                else np.nan\n",
        "            ),\n",
        "            \"precision\": precision_score(y_test, self.pipeline.predict(X_test)),\n",
        "            \"recall\": recall_score(y_test, self.pipeline.predict(X_test)),\n",
        "        }\n",
        "        self.test_metrics_report = {\n",
        "            metric: test_metrics[metric] for metric in test_metrics\n",
        "        }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_pipeline(self):\n",
        "        return self.pipeline\n",
        "\n",
        "    def get_preprocessor(self):\n",
        "        return self.preprocessor\n",
        "\n",
        "    def get_train_metrics_report(self):\n",
        "        return self.train_metrics_report\n",
        "\n",
        "    def get_test_metrics_report(self):\n",
        "        return self.test_metrics_report\n",
        "\n",
        "    def get_best_params(self):\n",
        "        if self.param_grid is None:\n",
        "            raise ValueError(\n",
        "                \"No hyperparameter grid was provided during model training.\"\n",
        "            )\n",
        "\n",
        "        best_param = self.pipeline[\"model_pipeline\"].best_params_\n",
        "        return {key.split('__')[1]: value for key, value in best_param.items()}\n",
        "\n",
        "    def get_selected_features(self):\n",
        "        if not self.select_features:\n",
        "            raise ValueError(\"Feature selection was not enabled during model training.\")\n",
        "\n",
        "        if (\n",
        "            self.pipeline is None\n",
        "            or \"feature_selection\"\n",
        "            not in self.pipeline.named_steps[\"model_pipeline\"].named_steps\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"Feature selection has not been performed or the model is not fitted.\"\n",
        "            )\n",
        "\n",
        "        rfe = self.pipeline.named_steps[\"model_pipeline\"].named_steps[\n",
        "            \"feature_selection\"\n",
        "        ]\n",
        "        feature_mask = rfe.support_\n",
        "\n",
        "        feature_names = self._get_feature_names_from_preprocessor()\n",
        "\n",
        "        selected_features = [\n",
        "            feature\n",
        "            for feature, is_selected in zip(feature_names, feature_mask)\n",
        "            if is_selected\n",
        "        ]\n",
        "        return [\n",
        "            feature\n",
        "            for feature in self.data_module.feature_columns\n",
        "            if any([feature in col for col in selected_features])\n",
        "        ]\n",
        "\n",
        "    def _get_feature_names_from_preprocessor(self):\n",
        "        transformers = self.preprocessor.transformers_\n",
        "        feature_names = []\n",
        "\n",
        "        for name, transformer, column in transformers:\n",
        "            if hasattr(transformer, \"get_feature_names_out\"):\n",
        "                feature_names.extend(transformer.get_feature_names_out(column))\n",
        "            else:\n",
        "                feature_names.extend(column)\n",
        "\n",
        "        return feature_names"
      ],
      "id": "89a9b7fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to the `Trainer` class, the `Visualizer` class is designed to streamline the process of visualizing model performance and explanations. It provides a variety of visualization methods for evaluating models, including confusion matrices, classification reports, ROC curves, precision-recall curves, and feature importances."
      ],
      "id": "95a4ea0e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import roc_curve, classification_report, precision_recall_curve\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self, trainer, model_name):\n",
        "        self.trainer = trainer\n",
        "        self.model_name = model_name\n",
        "\n",
        "        X_train, X_test, y_train, y_test = (\n",
        "            self.trainer.data_module.X_train,\n",
        "            self.trainer.data_module.X_test,\n",
        "            self.trainer.data_module.y_train,\n",
        "            self.trainer.data_module.y_test,\n",
        "        )\n",
        "\n",
        "        self.explainer = dx.Explainer(trainer.get_pipeline(), X_test, y_test)\n",
        "\n",
        "        self.X_test = X_test\n",
        "        self.y_true = y_test\n",
        "        self.y_test_pred_proba = trainer.get_pipeline().predict_proba(X_test)\n",
        "\n",
        "    @staticmethod\n",
        "    def compare_evaluation_metrics(visualizers):\n",
        "        model_names = [viz.model_name for viz in visualizers]\n",
        "        metrics = Trainer.get_scoring_metrics()\n",
        "        \n",
        "        means = {metric: [] for metric in metrics}\n",
        "        stds = {metric: [] for metric in metrics}\n",
        "        for viz in visualizers:\n",
        "            train_metrics = viz.trainer.get_train_metrics_report()\n",
        "            for metric in metrics:\n",
        "                means[metric].append(np.mean(train_metrics[metric][\"folds\"]))\n",
        "                stds[metric].append(np.std(train_metrics[metric][\"folds\"]))\n",
        "        \n",
        "        n_groups = len(metrics)\n",
        "        bar_width = 0.15\n",
        "        index = np.arange(n_groups)\n",
        "        opacity = 0.8\n",
        "        \n",
        "        plt.figure(figsize=(9, 7))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
        "        \n",
        "        for i, model_name in enumerate(model_names):\n",
        "            bar_positions = index + bar_width * i\n",
        "            bar_values = [means[metric][i] for metric in metrics]\n",
        "            error_values = [stds[metric][i] for metric in metrics]\n",
        "            \n",
        "            bars = plt.bar(\n",
        "                bar_positions,\n",
        "                bar_values,\n",
        "                bar_width,\n",
        "                alpha=opacity,\n",
        "                color=colors[i],\n",
        "                yerr=error_values,\n",
        "                capsize=5,\n",
        "                label=model_name\n",
        "            )\n",
        "            \n",
        "            for bar, error in zip(bars, error_values):\n",
        "                yval = bar.get_height()\n",
        "                plt.text(\n",
        "                    bar.get_x() + bar.get_width() / 2,\n",
        "                    yval + error + 0.01,\n",
        "                    f\"{yval:.2f}  {error:.2f}\",\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=9,\n",
        "                    rotation=90\n",
        "                )\n",
        "        \n",
        "        plt.xlabel('Metrics', fontsize=14)\n",
        "        plt.ylabel('Scores', fontsize=14)\n",
        "        plt.title('Cross-Validation (k={}) Evaluation Metrics Comparison'.format(visualizers[0].trainer.cv), fontsize=16)\n",
        "        plt.xticks(index + bar_width * (len(model_names) - 1) / 2, metrics, fontsize=12, rotation=90)\n",
        "        plt.ylim(0, 1.21)\n",
        "        plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "        plt.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def compare_roc_curves(visualizers, dataset):\n",
        "        if dataset not in [\"test\", \"eval\"]:\n",
        "            raise ValueError(\"Invalid dataset option. Choose 'test' or 'eval'.\")\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(visualizers)))\n",
        "\n",
        "        for i, viz in enumerate(visualizers):\n",
        "            if dataset == \"test\":\n",
        "                y_true = viz.trainer.data_module.y_test\n",
        "                y_scores = viz.trainer.get_trained_model().predict_proba(\n",
        "                    viz.trainer.data_module.X_test\n",
        "                )[:, 1]\n",
        "            elif dataset == \"eval\":\n",
        "                y_true = []\n",
        "                y_scores = []\n",
        "                for fold in viz.trainer.get_train_metrics_report()[\"roc_data\"]:\n",
        "                    y_true.extend(fold[0])\n",
        "                    y_scores.extend(fold[1][:, 1])\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "            auc_score = roc_auc_score(y_true, y_scores)\n",
        "            plt.plot(\n",
        "                fpr,\n",
        "                tpr,\n",
        "                label=f\"{viz.model_name} (AUC = {auc_score:.2f})\",\n",
        "                color=colors[i],\n",
        "            )\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], \"k--\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        \n",
        "        title = None\n",
        "        if dataset == \"test\":\n",
        "            title = \"ROC Curve Comparison on Test Set\"\n",
        "        elif dataset == \"eval\":\n",
        "            title = \"ROC Curve Comparison on Evaluation Set (Averaged over Folds with CV=10)\"\n",
        "        \n",
        "        plt.title(title)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_validation_metrics(self):\n",
        "        train_metrics = self.trainer.get_train_metrics_report()\n",
        "        cv = len(train_metrics[\"accuracy\"][\"folds\"])\n",
        "        metrics = self.trainer.get_scoring_metrics()\n",
        "        fold_scores = {metric: train_metrics[metric][\"folds\"] for metric in metrics}\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        bp = ax1.boxplot(fold_scores.values(), labels=metrics, notch=True, patch_artist=True, positions=np.arange(len(metrics))+1)\n",
        "        for box in bp['boxes']:\n",
        "            box.set(color='blue', linewidth=2)\n",
        "            box.set(facecolor='lightblue')\n",
        "        ax1.set_title('Boxplot of Metrics')\n",
        "        ax1.set_ylabel('Scores')\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.grid(True)\n",
        "\n",
        "        means = [np.mean(values) for values in fold_scores.values()]\n",
        "        std_devs = [np.std(values) for values in fold_scores.values()]\n",
        "        bar_positions = np.arange(1, len(metrics)+1)\n",
        "        ax2.bar(bar_positions, means, align='center', alpha=0.7, color='green', capsize=10)\n",
        "        ax2.set_xticks(bar_positions)\n",
        "        ax2.set_xticklabels(metrics)\n",
        "        ax2.set_title('Bar Chart of Average Metrics')\n",
        "        ax2.set_ylabel('Average Score')\n",
        "        ax2.set_ylim(0, 1)\n",
        "        ax2.grid(True)\n",
        "\n",
        "        for idx, (mean, std) in enumerate(zip(means, std_devs)):\n",
        "            ax2.text(idx + 1, mean, f'{mean:.2f}{std:.2f}', ha='center', va='bottom', fontsize=9, color='darkred')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle(f\"{self.model_name}: Validation Metrics Comparison (CV={cv})\", fontsize=16)\n",
        "        plt.subplots_adjust(top=0.85)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_test_metrics(self):\n",
        "        test_metrics = self.trainer.get_test_metrics_report()\n",
        "        test_values = list(test_metrics.values())\n",
        "        test_names = list(test_metrics.keys())\n",
        "\n",
        "        sns.barplot(x=test_names, y=test_values)\n",
        "        plt.title(f\"{self.model_name}: Test Metrics\")\n",
        "        plt.xlabel(\"Metrics\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        for i, v in enumerate(test_values):\n",
        "            if np.isnan(v):\n",
        "                plt.text(i, 0.5, \"N/A\", ha=\"center\", va=\"bottom\")\n",
        "            else:\n",
        "                plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix_test(self):\n",
        "        preds = self.y_test_pred_proba.argmax(axis=1)\n",
        "        skplt.metrics.plot_confusion_matrix(self.y_true, preds)\n",
        "        plt.title(f\"{self.model_name}: Confusion Matrix on Test Set\")\n",
        "        plt.show()\n",
        "        \n",
        "    def plot_confusion_matrix_eval(self):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for fold in self.trainer.get_train_metrics_report()[\"roc_data\"]:\n",
        "            y_true.extend(fold[0])\n",
        "            y_pred.extend(fold[1][:, 1].argmax(axis=1))\n",
        "                \n",
        "    def plot_classification_report_test(self):\n",
        "        preds = self.y_test_pred_proba.argmax(axis=1)\n",
        "        report = classification_report(self.y_true, preds, output_dict=True)\n",
        "\n",
        "        report_df = pd.DataFrame(report).transpose()\n",
        "        report_df = report_df.round(2)\n",
        "\n",
        "        table = plt.table(\n",
        "            cellText=report_df.values,\n",
        "            colLabels=report_df.columns,\n",
        "            rowLabels=report_df.index,\n",
        "            cellLoc=\"center\",\n",
        "            rowLoc=\"center\",\n",
        "            loc=\"center\",\n",
        "            fontsize=12,\n",
        "        )\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(12)\n",
        "        table.scale(1.2, 1.2)\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"{self.model_name}: Classification Report on Test Set\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_roc_curve_test(self):\n",
        "        skplt.metrics.plot_roc(\n",
        "            self.y_true, self.y_test_pred_proba, plot_micro=False, plot_macro=True\n",
        "        )\n",
        "        plt.title(f\"{self.model_name}: ROC Curve on Test Set\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_roc_curve_eval(self, show_folds=False):\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, self.trainer.cv))\n",
        "\n",
        "        roc_data = self.trainer.get_train_metrics_report()[\"roc_data\"]\n",
        "        for k in range(self.trainer.cv):\n",
        "            true_labels, y_pred_proba = roc_data[k]\n",
        "            fpr, tpr, _ = roc_curve(true_labels, y_pred_proba[:, 1])\n",
        "            auc_score = roc_auc_score(true_labels, y_pred_proba[:, 1])\n",
        "            ax.plot(\n",
        "                fpr, tpr, color=colors[k], label=f\"Fold {k + 1} (AUC = {auc_score:.2f})\"\n",
        "            )\n",
        "\n",
        "        plt.title(\n",
        "            f\"{self.model_name}: ROC Curves for each fold (CV={self.trainer.cv}, \"\n",
        "            f'Mean AUROC={self.trainer.train_metrics_report[\"roc_auc\"][\"mean\"]:.2f})'\n",
        "        )\n",
        "        if show_folds:\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            \n",
        "        plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_precision_recall_curve_test(self):\n",
        "        skplt.metrics.plot_precision_recall(self.y_true, self.y_test_pred_proba)\n",
        "        plt.title(f\"{self.model_name}: Precision-Recall Curve on Test Set\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_lift_curve_test(self):\n",
        "        skplt.metrics.plot_lift_curve(self.y_true, self.y_test_pred_proba)\n",
        "        plt.title(f\"{self.model_name}: Lift Curve on Test Set\")\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_cumulative_gain_curve_test(self):\n",
        "        skplt.metrics.plot_cumulative_gain(self.y_true, self.y_test_pred_proba)\n",
        "        plt.title(f\"{self.model_name}: Cumulative Gain Curve on Test Set\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_partial_dependence_test(self, feature):\n",
        "        pdp = self.explainer.model_profile(type=\"partial\", variables=feature)\n",
        "        pdp.plot()\n",
        "\n",
        "    def plot_accumulated_local_effects_test(self, feature):\n",
        "        ale = self.explainer.model_profile(type=\"accumulated\", variables=feature)\n",
        "        ale.plot()\n",
        "\n",
        "    def plot_breakdown_test(self, observation):\n",
        "        breakdown = self.explainer.predict_parts(observation, type=\"break_down\")\n",
        "        breakdown.plot()\n",
        "\n",
        "    def plot_model_explanations_test(self):\n",
        "        feature_importance = self.explainer.model_parts()\n",
        "        feature_importance.plot()\n",
        "\n",
        "        # model_profile = self.explainer.model_profile(type=\"partial\")\n",
        "        # model_profile.plot()\n",
        "\n",
        "    def plot_grid_search(self, log_scale_params):\n",
        "        if self.trainer.param_grid is None:\n",
        "            raise ValueError(\"No hyperparameter grid was provided during model training.\")\n",
        "        \n",
        "        cv_results = pd.DataFrame(self.trainer.get_pipeline().named_steps[\"model_pipeline\"].cv_results_)\n",
        "        \n",
        "        def shorten_param(param_name):\n",
        "            if \"__\" in param_name:\n",
        "                return param_name.rsplit(\"__\", 1)[1]\n",
        "            return param_name\n",
        "        \n",
        "        cv_results = cv_results.rename(shorten_param, axis=1)\n",
        "        \n",
        "        params = {}\n",
        "        for param in log_scale_params:\n",
        "            if cv_results[param].dtype in [\"int64\", \"float64\"]:\n",
        "                params[param] = lambda x: np.log10(x) if x > 0 else 0\n",
        "            else:\n",
        "                params[param] = lambda x: x\n",
        "        \n",
        "        fig = px.parallel_coordinates(\n",
        "            cv_results.apply(\n",
        "                {\n",
        "                    **params,\n",
        "                    'mean_test_score': lambda x: x\n",
        "                }\n",
        "            ),\n",
        "            color=\"mean_test_score\",\n",
        "            color_continuous_scale=px.colors.sequential.Viridis\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def visualize_explanations_test(self, feature_columns=[]):\n",
        "        self.plot_model_explanations()\n",
        "\n",
        "        if not feature_columns:\n",
        "            feature_columns = self.trainer.data_module.feature_columns[0]\n",
        "\n",
        "        self.plot_partial_dependence(feature_columns)\n",
        "        self.plot_accumulated_local_effects(feature_columns)\n",
        "\n",
        "        observation = self.trainer.data_module.X_test.iloc[0]\n",
        "        self.plot_breakdown(observation)\n",
        "\n",
        "        plt.show()"
      ],
      "id": "0c5fd294",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Model: Logistic Regression\n",
        "\n",
        "We will start by training a baseline logistic regression model using a subset of features. The features include the client's age, region, and aggregated balance and volume information from the transactional data. The goal is to establish a baseline performance level that we can compare against more complex models.\n",
        "\n",
        "The logistic regression model is a simple yet effective model for binary classification tasks. It provides interpretable results and can serve as a good starting point for more complex models."
      ],
      "id": "358cd67a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "baseline_feature_columns = [\"age\", \"client_region\"] + [\n",
        "    col\n",
        "    for col in golden_record_df.columns\n",
        "    if \"M_\" in col and (\"_balance\" in col or \"_volume\" in col)\n",
        "]\n",
        "\n",
        "baseline_data_module = create_data_module(golden_record_df, baseline_feature_columns)\n",
        "\n",
        "print(f\"Number of baseline feature columns: {len(baseline_feature_columns)}\")\n",
        "print(f\"Baseline feature columns: {baseline_feature_columns}\")"
      ],
      "id": "f0e47f56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filtering the relevant columns for the baseline model, we have 26 columns."
      ],
      "id": "5b4cbc32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "baseline_trainer = (\n",
        "    Trainer(baseline_data_module, LogisticRegression(max_iter=10000, random_state=SEED)).fit().eval_train()\n",
        ")\n",
        "\n",
        "baseline_visualizer = Visualizer(baseline_trainer, \"Baseline Logistic Regression\")\n",
        "baseline_visualizer.plot_validation_metrics()"
      ],
      "id": "3925339f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The baseline model hits quite high scores in all metrics showing good robustness across folds. \n",
        "\n",
        "The confusion matrix shows that the model is performing well on the test set with a high number of true positives and true negatives. There is an equal number of false positives and false negatives."
      ],
      "id": "8676f09d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "baseline_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "c8076918",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ROC curve shows that the model has a high true positive rate across different thresholds. The AUC score is also quite high, indicating good performance.\n",
        "\n",
        "## Adding more features\n",
        "\n",
        "In order to possibly improve the model performance, we will include more features in the training data. We will include all features except for the ones that are not relevant for the model training.\n",
        "\n",
        "After merging the transactional and non-transactional data, we have many columns that are unnecessary for model training. We will remove all columns containing card-related information, except for the `has_card` column. This decision stems from the fact that 50% of our dataset consists of cardholders and the other 50% consists of non-cardholders, which we matched with the cardholders. Therefore, the data in the non-target card-related columns come from the actual cardholders.\n",
        "\n",
        "Additionally we will remove all columns that contain time-dependent information, such as dates and IDs, as they are not relevant for the model."
      ],
      "id": "b016f6ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_cols_before = len(golden_record_df.columns)\n",
        "print(f\"Number of columns before filtering: {num_cols_before}\")\n",
        "\n",
        "golden_record_df = golden_record_df.loc[\n",
        "    :,\n",
        "    ~golden_record_df.columns.str.contains(\"card\")\n",
        "    | golden_record_df.columns.str.contains(\"has_card\"),\n",
        "]\n",
        "print(\n",
        "    f\"Removed {num_cols_before - len(golden_record_df.columns)} card-related columns. Now {len(golden_record_df.columns)} columns remain.\"\n",
        ")\n",
        "\n",
        "num_cols_before = len(golden_record_df.columns)\n",
        "golden_record_df = golden_record_df.drop(\n",
        "    columns=[\"loan_granted_date\", \"birth_date\", \"account_created\"]\n",
        ")\n",
        "print(\n",
        "    f\"Removed {num_cols_before - len(golden_record_df.columns)} time-dependent columns. Now {len(golden_record_df.columns)} columns remain.\"\n",
        ")\n",
        "\n",
        "num_cols_before = len(golden_record_df.columns)\n",
        "golden_record_df = golden_record_df.drop(\n",
        "    columns=[\n",
        "        \"loan_account_id\",\n",
        "        \"loan_loan_id\",\n",
        "        \"order_account_id\",\n",
        "        \"client_district_name\",\n",
        "        \"disp_id\",\n",
        "        \"account_id\",\n",
        "        \"account_district_name\",\n",
        "    ]\n",
        ")\n",
        "print(\n",
        "    f\"Removed {num_cols_before - len(golden_record_df.columns)} ID columns. Now {len(golden_record_df.columns)} columns remain.\"\n",
        ")\n",
        "\n",
        "num_cols_before = len(golden_record_df.columns)\n",
        "golden_record_df = golden_record_df.drop(\n",
        "    columns=[col for col in golden_record_df.columns if \"std\" in col]\n",
        ")\n",
        "print(\n",
        "    f\"Removed {num_cols_before - len(golden_record_df.columns)} std columns. Now {len(golden_record_df.columns)} columns remain.\"\n",
        ")\n",
        "\n",
        "cols_to_exclude_in_train = [\"client_id\", \"has_card\"]\n",
        "all_cols_data_module = create_data_module(\n",
        "    golden_record_df, golden_record_df.drop(columns=cols_to_exclude_in_train).columns\n",
        ")\n",
        "\n",
        "print(f\"Number of all feature columns: {len(all_cols_data_module.feature_columns)}\")\n",
        "del num_cols_before"
      ],
      "id": "766fe371",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In total we remove 30 columns from the dataset. The remaining columns are used for training the models.\n",
        "\n",
        "## Candidate Models\n",
        "\n",
        "We will now train multiple candidate models using the expanded feature set and evaluate their performance. The candidate models include:\n",
        "\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- Decision Tree\n",
        "- Gradient Boosting\n",
        "\n",
        "We will train each model using the same cross-validation strategy and evaluation metrics to ensure a fair comparison. After training the models, we will evaluate their performance across folds. \n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "As a direct extension of the baseline model, we will train a logistic regression model using the expanded feature set. The hypothesis is that the additional features will improve the model's performance by capturing more complex relationships but also potentially introduce noise and reduce generalization."
      ],
      "id": "5e2181e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_reg_trainer = (\n",
        "    Trainer(all_cols_data_module, LogisticRegression(max_iter=10000, random_state=SEED)).fit().eval_train()\n",
        ")\n",
        "\n",
        "log_reg_visualizer = Visualizer(log_reg_trainer, \"Logistic Regression\")\n",
        "log_reg_visualizer.plot_validation_metrics()"
      ],
      "id": "176bba2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As hypothesized, the logistic regression model with the expanded feature set performs worse than the baseline model. This indicates that the additional features might have introduced noise or overfitting. The model's performance is still quite good, but it is slightly worse than the baseline model. We still have a fairly high AUC score as in the baseline model."
      ],
      "id": "9165c38c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_reg_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "97cd3a89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking closer at the ROC curves for each fold, we can see some expected variance in the performance across different folds. \n",
        "\n",
        "### Random Forest\n",
        "\n",
        "Next, we will train a Random Forest model to see if it can capture more complex relationships in the data and outperform the logistic regression model."
      ],
      "id": "be535211"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_trainer = (\n",
        "    Trainer(\n",
        "        all_cols_data_module,\n",
        "        RandomForestClassifier(random_state=SEED),\n",
        "    )\n",
        "    .fit()\n",
        "    .eval_train()\n",
        ")\n",
        "\n",
        "rf_visualizer = Visualizer(rf_trainer, \"Random Forest\")\n",
        "rf_visualizer.plot_validation_metrics()"
      ],
      "id": "c4593ad2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "2a42419e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Tree\n",
        "\n",
        "We will also train a Decision Tree model to see how it performs compared to the other models. Decision Trees are known for their interpretability and simplicity."
      ],
      "id": "4b9aa06e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "decision_tree_trainer = (\n",
        "    Trainer(\n",
        "        all_cols_data_module,\n",
        "        DecisionTreeClassifier(random_state=SEED),\n",
        "    )\n",
        "    .fit()\n",
        "    .eval_train()\n",
        ")\n",
        "\n",
        "decision_tree_visualizer = Visualizer(decision_tree_trainer, \"Decision Tree\")\n",
        "decision_tree_visualizer.plot_validation_metrics()"
      ],
      "id": "ba83d7b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "decision_tree_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "4f889094",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "Finally, we will train a Gradient Boosting model to see if it can outperform the other models. Gradient Boosting models are known for their high accuracy and ability to capture complex relationships in the data."
      ],
      "id": "95569c3a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradient_boost_trainer = (\n",
        "    Trainer(\n",
        "        all_cols_data_module,\n",
        "        GradientBoostingClassifier(random_state=SEED),\n",
        "    )\n",
        "    .fit()\n",
        "    .eval_train()\n",
        ")\n",
        "\n",
        "gradient_boost_visualizer = Visualizer(gradient_boost_trainer, \"Gradient Boosting\")\n",
        "gradient_boost_visualizer.plot_validation_metrics()"
      ],
      "id": "e4f0c4a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gradient_boost_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "1751189b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison & Selection\n",
        "\n",
        "We have trained and evaluated multiple candidate models using the expanded feature set. We will now compare the models' performance across the  evaluation metrics and select the best-performing model for further analysis. The evaluation metrics include accuracy, F1 score, AUC-ROC, precision, and recall."
      ],
      "id": "db81f460"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "candidate_trainers = [\n",
        "    baseline_trainer,\n",
        "    log_reg_trainer,\n",
        "    rf_trainer,\n",
        "    decision_tree_trainer,\n",
        "    gradient_boost_trainer,\n",
        "]\n",
        "candidate_visualizers = [\n",
        "    baseline_visualizer,\n",
        "    log_reg_visualizer,\n",
        "    rf_visualizer,\n",
        "    decision_tree_visualizer,\n",
        "    gradient_boost_visualizer,\n",
        "]"
      ],
      "id": "364845f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Visualizer.compare_evaluation_metrics(candidate_visualizers)"
      ],
      "id": "71726bbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The comparison of evaluation metrics across the candidate models shows that the Random Forest model has one of the highest mean scores along with the Baseline and Gradient Boosting models. The Decision Tree model as well as the Logistic Regression model have lower mean scores across the metrics.\n",
        "\n",
        "Especially the high recall of the Random Forest model is promising, as it indicates that the model can effectively classify positive samples (clients who have a card) without missing many of them."
      ],
      "id": "566da30c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Visualizer.compare_roc_curves(candidate_visualizers, dataset=\"eval\")"
      ],
      "id": "85e5f47d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ROC curves move all above the diagonal line, which is a good sign and does not show any problems with the models. The ROC curve of a decision tree model tends to be very linear because decision trees make hard predictions, assigning instances to either one class or the other without providing probability estimates. This results in a stair-step or piecewise linear ROC curve.\n",
        "\n",
        "\n",
        "The curves of the baseline, Gradient Boosting, and Random Forest models are very similar, indicating that they perform similarly across different thresholds. The Logistic Regression model has a slightly lower curve, indicating that it performs worse than the other models.\n",
        "\n",
        "The AUC scores of the models are also quite high, with the Random Forest model having one of the highest scores.\n",
        "\n",
        "## Top-N Customer Lists\n",
        "\n",
        "We will now use the trained models to generate a list of the top N% customers who are most likely to get a card (according to the model). Therefore we will look at the customers who are most likely to get a card but don't have one yet. This list can be used by the marketing team to target potential customers who are likely to get a card."
      ],
      "id": "4af6e41f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_top_n_customers_list(model, data):\n",
        "    \"\"\" Create a list of top N% customers who are most likely to get a card according to the model \"\"\"\n",
        "    mandatory_columns = [\"client_id\", \"has_card\"]\n",
        "\n",
        "    if not hasattr(model, \"predict_proba\"):\n",
        "        raise ValueError(\"Model does not support probability predictions\")\n",
        "\n",
        "    if not all(col in data.columns for col in mandatory_columns):\n",
        "        raise ValueError(\"Mandatory columns not found in data: 'client_id', 'has_card'\")\n",
        "\n",
        "    data = data[data[\"has_card\"] == 0]\n",
        "\n",
        "    probabilities = model.predict_proba(data.copy())\n",
        "    # Probability of having a card (class 1). This essentially gives the clients who should most likely have a card based on the model but don't have one.\n",
        "    probabilities = probabilities[:, 1]\n",
        "\n",
        "    results = pd.DataFrame(\n",
        "        {\"Client ID\": data[\"client_id\"], \"Probability\": probabilities}\n",
        "    )\n",
        "\n",
        "    return results.sort_values(by=\"Probability\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def compare_top_n_lists(*lists, labels, top_n_percent):\n",
        "    \"\"\" Compare the overlap of top N% customer lists generated by different models \"\"\"\n",
        "    if len(lists) != len(labels):\n",
        "        raise ValueError(\"Each list must have a corresponding label\")\n",
        "\n",
        "    if len(set([len(l) for l in lists])) != 1:\n",
        "        raise ValueError(\"All lists must have the same length\")\n",
        "\n",
        "    for l in lists:\n",
        "        if not l[\"Probability\"].is_monotonic_decreasing:\n",
        "            raise ValueError(\"Lists must be sorted in descending order of probability\")\n",
        "\n",
        "    top_n = int(len(lists[0]) * top_n_percent)\n",
        "    lists = [l.head(top_n) for l in lists]\n",
        "\n",
        "    overlap_matrix = pd.DataFrame(0, index=labels, columns=labels)\n",
        "\n",
        "    for i, list1 in enumerate(lists):\n",
        "        set1 = set(list1[\"Client ID\"])\n",
        "        for j, list2 in enumerate(lists):\n",
        "            set2 = set(list2[\"Client ID\"])\n",
        "            overlap_matrix.iloc[i, j] = len(set1.intersection(set2))\n",
        "\n",
        "    overlap_matrix = overlap_matrix / len(lists[0])\n",
        "    return overlap_matrix\n",
        "\n",
        "\n",
        "def visualize_overlap_matrix(overlap_matrix, title):\n",
        "    \"\"\" Visualize the overlap matrix as a heatmap \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    mask = np.tril(np.ones_like(overlap_matrix, dtype=bool))\n",
        "    overlap_matrix = overlap_matrix.mask(mask)\n",
        "\n",
        "    sns.heatmap(\n",
        "        overlap_matrix,\n",
        "        annot=True,\n",
        "        cmap=\"Blues\",\n",
        "        cbar_kws={\"label\": \"Common Customers [%]\"},\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"List from Model/Method\")\n",
        "    plt.xlabel(\"List from Model/Method\")\n",
        "    plt.xticks(\n",
        "        ticks=np.arange(len(overlap_matrix.columns)) + 0.5,\n",
        "        labels=overlap_matrix.columns,\n",
        "        rotation=45,\n",
        "        ha=\"right\",\n",
        "    )\n",
        "    plt.yticks(\n",
        "        ticks=np.arange(len(overlap_matrix.index)) + 0.5,\n",
        "        labels=overlap_matrix.index,\n",
        "        rotation=0,\n",
        "    )\n",
        "    plt.show()"
      ],
      "id": "54d2a386",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top-10% Customer Selection\n",
        "\n",
        "We will select the top 10% of customers who are most likely to get a card according to each model."
      ],
      "id": "98dc84bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_lists = [\n",
        "    create_top_n_customers_list(trainer.get_pipeline(), golden_record_df)\n",
        "    for trainer in candidate_trainers\n",
        "]\n",
        "\n",
        "candidate_labels = [\n",
        "    \"Baseline\",\n",
        "    \"Logistic Regression\",\n",
        "    \"Random Forest\",\n",
        "    \"Decision Tree\",\n",
        "    \"Gradient Boosting\",\n",
        "]\n",
        "\n",
        "top_10_overlap_matrix = compare_top_n_lists(\n",
        "    *customer_lists, labels=candidate_labels, top_n_percent=0.1\n",
        ")\n",
        "visualize_overlap_matrix(\n",
        "    top_10_overlap_matrix, \"Overlap of Top-10% Customer Lists by Model\"\n",
        ")"
      ],
      "id": "7319ad82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the overlap matrix of the top 10% customer lists, we can see that the Random Forest model has a high overlap with the other tree-based models (Decision Tree and Gradient Boosting). The Baseline model seems to have only 50% overlap with the Logistic Regression model, indicating that they still share some common predictions as they are both linear models.\n",
        "\n",
        "### Top-5% Customer Selection\n",
        "\n",
        "We will select the top 5% of customers who are most likely to get a card according to each model."
      ],
      "id": "68e7668f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_5_overlap_matrix = compare_top_n_lists(\n",
        "    *customer_lists, labels=candidate_labels, top_n_percent=0.05\n",
        ")\n",
        "visualize_overlap_matrix(\n",
        "    top_5_overlap_matrix, \"Overlap of Top-5% Customer Lists by Model\"\n",
        ")"
      ],
      "id": "b49dbf87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the overlap matrix of the top 5% customer lists, we can see that the overlap between the tree-based models is even higher. Especially the overlap between the Decision Tree and Gradient Boosting models got higher. The Logistic Regression model still has a lower overlap with the other models but gained some overlap with the Gradient Boosting model.\n",
        "\n",
        "## Selected Model: Random Forest\n",
        "\n",
        "After evaluating the candidate models, we have selected the Random Forest model as the selected model for further analysis. The Random Forest model has shown one of the highest mean scores across the evaluation metrics and has a high recall, indicating that it can effectively classify positive samples without missing many of them. The model also has a high AUC score, indicating good performance across different thresholds.\n",
        "\n",
        "After we have now selected the Random Forest model, we will further optimize its hyperparameters using grid search with cross-validation to improve its performance"
      ],
      "id": "22e6393d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_model_trainer = rf_trainer\n",
        "best_model_visualizer = rf_visualizer"
      ],
      "id": "24fb81f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Optimization\n",
        "\n",
        "We will perform a GridSearch on a param grid with reasonable values for the Random Forest model to find the best hyperparameters. The GridSearch will be performed using cross-validation with the same settings as the training of the candidate model. The best model will be selected based on the mean AUC score across folds.\n",
        "\n",
        "The hyperparameters we will tune are:\n",
        "\n",
        "- `n_estimators`: The number of trees in the forest.\n",
        "- `max_depth`: The maximum depth of the tree.\n",
        "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "We will use the same data and feature set as before to ensure a fair comparison.\n",
        "\n",
        "As this process can be computationally expensive, we will cache the trained model to avoid retraining it multiple times."
      ],
      "id": "50dc2302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_param_grid = {\n",
        "    \"model__n_estimators\": [50, 100, 200],\n",
        "    \"model__max_depth\": [5, 10, 20],\n",
        "    \"model__min_samples_split\": [2, 5, 10],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "\n",
        "try:\n",
        "    gs_trainer = joblib.load(gs_cache_file)\n",
        "    print(\"Loaded cached model\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No cached model found, proceeding with training...\")\n",
        "    gs_trainer = Trainer(\n",
        "        all_cols_data_module,\n",
        "        RandomForestClassifier(random_state=SEED),\n",
        "        param_grid=gs_param_grid,\n",
        "        verbose=False\n",
        "    ).fit().eval_train()\n",
        "    \n",
        "    joblib.dump(gs_trainer, gs_cache_file)  \n",
        "\n",
        "print(\"Best Parameters:\", gs_trainer.get_best_params())"
      ],
      "id": "1e64f200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best hyperparameters found by the GridSearch are:\n",
        "\n",
        "- `n_estimators`: 200\n",
        "- `max_depth`: 10\n",
        "- `min_samples_split`: 2\n",
        "- `min_samples_leaf`: 4\n",
        "\n",
        "This indicates that a Random Forest model profits from a higher number of trees in the forest and a higher maximum depth of the trees. The minimum number of samples required to split an internal node and the minimum number of samples required to be at a leaf node are relatively low, indicating that the model can split nodes with fewer samples."
      ],
      "id": "b990b092"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_visualizer = Visualizer(gs_trainer, \"Random Forest Grid Search\")\n",
        "gs_visualizer.plot_grid_search(log_scale_params=[\"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\"])"
      ],
      "id": "c46decfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the coordinate plot of the GridSearch results, we can see that the model's performance increases with the number of estimators and the maximum depth of the trees. Overall the impact of the `min_samples_split` and `min_samples_leaf` hyperparameters is less pronounced. The best model is found at the highest values of `n_estimators` and possibly mid-range values of `max_depth`. In the end the optimizations are minimal though with a mean test score being between 0.81 and 0.835."
      ],
      "id": "e9742e4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_visualizer.plot_validation_metrics()"
      ],
      "id": "e48723a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Visualizer.compare_evaluation_metrics([best_model_visualizer, gs_visualizer])"
      ],
      "id": "ca43975c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Random Forest model after GridSearch optimization shows a slight improvement in the mean scores across the evaluation metrics. Especially the recall profited from the hyperparameter optimization, getting both higher in its mean and lower in its variance."
      ],
      "id": "26d0a9ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_visualizer.plot_roc_curve_eval(show_folds=True)"
      ],
      "id": "66be3fce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Visualizer.compare_roc_curves([best_model_visualizer, gs_visualizer], dataset=\"eval\")"
      ],
      "id": "d782d80c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ROC curves of the Random Forest model before and after hyperparameter optimization are moving closely towards the upper left corner, indicating good performance across different thresholds. However the mean AUC score did not change after the optimization with the mean curves of both models being almost identical and the difference negligible.\n",
        "\n",
        "As we now optimised the model we will evaluate it on the test set to get a final performance estimate."
      ],
      "id": "a1f02130"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_trainer.eval_test()\n",
        "gs_visualizer.plot_test_metrics()"
      ],
      "id": "fafc9312",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model performs well on the test set, with high scores across the evaluation metrics. The AUC score is high, indicating good performance across different thresholds."
      ],
      "id": "4ffc9885"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_, _ = (\n",
        "    gs_visualizer.plot_confusion_matrix_test(),\n",
        "    gs_visualizer.plot_classification_report_test()\n",
        ")"
      ],
      "id": "7008e63c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The confusion matrix shows that the model is performing well on the test set with a high number of true positives and true negatives. However, there is an unequal number of false positives and false negatives: The false positives are higher than the false negatives. This hints that the model may be slightly biased towards predicting positive samples (clients who have a card)."
      ],
      "id": "2a77a625"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs_visualizer.plot_lift_curve_test()"
      ],
      "id": "063de555",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the lift curve we can see some interesting patterns. For true class there is a sharper drop of precision for around 0.25 percentage of samples which then recovers again, before continuing with the expected decline, this indicates that there appears to be a problem in certain classifications in the top 0.20 percentage of probabilities. Looking at the false class, everything seems fine, with a high lift in the beginning and a steady smooth decline."
      ],
      "id": "82186681"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_model_visualizer.plot_roc_curve_test()"
      ],
      "id": "f25aaa4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_, _ = (\n",
        "    best_model_visualizer.plot_confusion_matrix_test(),\n",
        "    best_model_visualizer.plot_classification_report_test(),\n",
        ")"
      ],
      "id": "fb7ae39b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the confusion matrix and classification report of the Random Forest model, we can see that the model performs well on the test set. The confusion matrix shows a high number of true positives and true negatives, with a low number of false positives and false negatives. There is a slight imbalace in the false positives and false negatives, with the false positives being higher than the false negatives. This can also be seen in the recall for False class being lower at 0.79.\n",
        "\n",
        "# Model Explanation & Reduction"
      ],
      "id": "d8688cb7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    reduced_best_model_trainer = joblib.load(reduced_model_cache_file)\n",
        "    print(\"Loaded cached reduced best model\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No cached reduced best model found, proceeding with training...\")\n",
        "    reduced_best_model_trainer = (\n",
        "        Trainer(\n",
        "            all_cols_data_module,\n",
        "            RandomForestClassifier(**gs_trainer.get_best_params(), random_state=SEED),\n",
        "            select_features=True,\n",
        "        )\n",
        "        .fit()\n",
        "        .eval_train()\n",
        "    )\n",
        "    joblib.dump(reduced_best_model_trainer, reduced_model_cache_file)\n",
        "\n",
        "selected_features = reduced_best_model_trainer.get_selected_features()\n",
        "print(\"Selected Features:\", selected_features)\n",
        "print(\"Number of selected features:\", len(selected_features))\n",
        "\n",
        "reduced_best_model_visualizer = Visualizer(reduced_best_model_trainer, \"Reduced Random Forest\")"
      ],
      "id": "501e777e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the selected features we see some confirmation of our previous analysis. The selection contains a every of base balance feature (M_2 to M_12). This makes sense as our analysis showed that across the board card holders in the golden record appear to have higher balances than non-card holders."
      ],
      "id": "3bfb6f41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Visualizer.compare_evaluation_metrics(\n",
        "    [best_model_visualizer, gs_visualizer, reduced_best_model_visualizer]\n",
        ")"
      ],
      "id": "811a10c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The comparison shows that the reduced Random Forest model performs in some cases slightly worse than the original Random Forest model. Across all metrics we see a negligble decrease of about 1% in the mean scores. The variance of the scores is also slightly higher, indicating that the model may be less robust. This is a trade-off we have to make when reducing the number of features. Given the gained explainability and interpretability of the model, this trade-off is worth it."
      ],
      "id": "1ff22f71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_best_model_visualizer.plot_confusion_matrix_test()"
      ],
      "id": "4a434184",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The confusion matrix is almost identical with the exception of one fewer false positive. Therefore the reduced model performs better than the original model in this regard."
      ],
      "id": "1e092d94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_best_model_visualizer.plot_model_explanations_test()"
      ],
      "id": "7b97bbc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the feature important of the reduced model. The most important feature is the `M_2_balance` feature, which is the base balance of the client. This confirms our previous analysis that the base balance is a strong indicator of whether a client has a card or not. In appear all balance features as important features in the model.\n",
        "\n",
        "## Lift Curve on reduced model vs grid search model"
      ],
      "id": "2f0b17aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_best_model_visualizer.plot_lift_curve_test()\n",
        "gs_visualizer.plot_lift_curve_test()"
      ],
      "id": "f04c9178",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also comparing the lift curves of the grid search model and the reduced model we can see very little difference. There appears to be a slightly earlier drop at already 0.18 percentage of samples for the reduced model, but the overall shape of the curve is very similar.\n",
        "\n",
        "## Top-N Customer List\n",
        "\n",
        "We will generate a list of the top 10% and top 5% customers who are most likely to get a card according to the reduced Random Forest model."
      ],
      "id": "bffe7b07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_models = [rf_trainer, gs_trainer, reduced_best_model_trainer]\n",
        "\n",
        "\n",
        "rf_customer_lists = [\n",
        "    create_top_n_customers_list(trainer.get_pipeline(), golden_record_df)\n",
        "    for trainer in rf_models\n",
        "]\n",
        "\n",
        "rf_labels = [\"Random Forest\", \"Grid Search Random Forest\", \"Reduced Random Forest\"]\n",
        "\n",
        "top_10_overlap_matrix_rf = compare_top_n_lists(\n",
        "    *rf_customer_lists, labels=rf_labels, top_n_percent=0.1\n",
        ")\n",
        "\n",
        "visualize_overlap_matrix(\n",
        "    top_10_overlap_matrix_rf, \"Overlap of Top-10% Customer Lists by Model\"\n",
        ")"
      ],
      "id": "d891e730",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_5_overlap_matrix_rf = compare_top_n_lists(\n",
        "    *rf_customer_lists, labels=rf_labels, top_n_percent=0.05\n",
        ")\n",
        "\n",
        "visualize_overlap_matrix(\n",
        "    top_5_overlap_matrix_rf, \"Overlap of Top-5% Customer Lists by Model\"\n",
        ")"
      ],
      "id": "c2c76e08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the top 10% and top 5% customer lists again across all Random Forest models we can see that the overlap is partially very high. The reduced and the grid search model have a rather high overlap with 0.78 and 0.62 respectively for top 10% and top 5% of customers. While the reduced and the normal Random Forest model share less overlap at 0.66 for top 10% and 0.56 for top 5% of customers. We do however see that the grid search and normal Random Forest share a similar overlap to the grid search and reduced model.\n",
        "\n",
        "## Breakdown of selected model on top and bottom prediction.\n",
        "\n",
        "To better understand the model's predictions, we will visualize the breakdown of the top and bottom predictions from the reduced Random Forest model. "
      ],
      "id": "1a49d2c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_lists_reduced_rf = customer_lists[-1] \n",
        "print(customer_lists_reduced_rf.head())\n",
        "top_prob_client = customer_lists_reduced_rf['Client ID'][0]\n",
        "\n",
        "top_client = golden_record_df[golden_record_df['client_id'] == top_prob_client]\n",
        "top_client  = top_client.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# drop has card and label required for explainer to work properly\n",
        "top_client = top_client.drop(columns=['has_card', 'client_id'])\n",
        "\n",
        "reduced_best_model_visualizer.plot_breakdown_test(top_client)"
      ],
      "id": "7b2036bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at a specific prediction we see how the features contribute to the prediction. The base balance is the most important feature, followed by the volume of the client. The other features have a much smaller impact on the prediction."
      ],
      "id": "febc750d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bottom_prob_client = customer_lists_reduced_rf.iloc[-1]['Client ID']\n",
        "print(customer_lists_reduced_rf.tail())\n",
        "bottom_client = golden_record_df[golden_record_df['client_id'] == bottom_prob_client]\n",
        "bottom_client  = bottom_client.apply(pd.to_numeric, errors='coerce')\n",
        "# drop has card and label\n",
        "bottom_client = bottom_client.drop(columns=['has_card', 'client_id'])\n",
        "\n",
        "reduced_best_model_visualizer.plot_breakdown_test(bottom_client)"
      ],
      "id": "344eda2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the lowest probability feature we see how the features contribute to the prediction. \n",
        "\n",
        "# Conclusion\n",
        "\n",
        "In this project, we have analyzed a dataset containing information about clients and their transactions to predict which clients are most likely to get a card. We looked at multiple different models and evaluated their performance using various evaluation metrics. \n",
        "\n",
        "## To Non-Technical Stakeholders\n",
        "\n",
        "Across the board it appears as if high balances are important in predicting whether a client has a card or not. Generally speaking is a high balance count in the past 12 (+ 1 lag) months a strong indicator for a potential card buyer. The analysis also showed that with card holders having higher median balances across all months. "
      ],
      "id": "e7c5d9fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_best_model_visualizer.plot_lift_curve_test()"
      ],
      "id": "38b49dcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the Lift Curve we can see that our model outperforms a random model by a big margin. Especially it is very good at identifying non-buyers, reducing risk of spending effort and time on clients that are unlikely to buy a card.\n",
        "\n",
        "\n",
        "Here is the top 5 client ids according to our model which currently do not have a card but are prime candidates for getting one:"
      ],
      "id": "5dac0ca7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_lists_reduced_rf.head()"
      ],
      "id": "15b0435c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We hope that this helps you in the decision making process and that you can use this information to target potential customers who are likely to get a card.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "Dominik & Noah\n"
      ],
      "id": "dc67a5e0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
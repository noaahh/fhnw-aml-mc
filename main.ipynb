{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "4366d59ebb934e458257bbcf3850ed3a",
    "deepnote_cell_type": "markdown"
   },
   "source": "# AML Mini-Challenge - Credit Card Affinity Modelling\n\n> Dominik Filliger & Noah Leuenberger\n\nThe task can be found [here](https://spaces.technik.fhnw.ch/storage/uploads/spaces/82/exercises/20240218__AML_Trainingscenter_MiniChallenge_Kreditkarten_Aufgabenstellung-1708412668.pdf).\n\n# Setup",
   "block_group": "d474c6c491654df981433d9dc4ae30be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "#plt.style.use('seaborn-white')\n",
    "#plt.style.use('ggplot')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_categorical_variables(df, categorical_columns, fill_na_value='NA'):\n",
    "    \"\"\"\n",
    "    Plots count plots for categorical variables in a DataFrame, filling NA values with a specified string.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    - categorical_vars: list of strings, names of the categorical variables in df to plot.\n",
    "    - fill_na_value: string, the value to use for filling NA values in the categorical variables.\n",
    "    \"\"\"\n",
    "    # Fill NA values in the specified categorical variables\n",
    "    for var in categorical_columns:\n",
    "        if df[var].isna().any():\n",
    "            df[var] = df[var].fillna(fill_na_value)\n",
    "\n",
    "    total = float(len(df))\n",
    "    fig, axes = plt.subplots(nrows=len(categorical_columns), figsize=(14, len(categorical_columns) * 4.5))\n",
    "\n",
    "    if len(categorical_columns) == 1:  # If there's only one categorical variable, wrap axes in a list\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, var in enumerate(categorical_columns):\n",
    "        ax = sns.countplot(x=var, data=df, ax=axes[i], order=df[var].value_counts().index)\n",
    "\n",
    "        axes[i].set_title(f'Distribution of {var}')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_xlabel(var)\n",
    "\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width() / 2.,\n",
    "                    height + 3,\n",
    "                    '{:1.2f}%'.format((height / total) * 100),\n",
    "                    ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_numerical_distributions(df, numerical_columns, kde=True, bins=30):\n",
    "    \"\"\"\n",
    "    Plots the distribution of all numerical variables in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the number of rows needed for subplots based on the number of numerical variables\n",
    "    nrows = len(numerical_columns)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(8, 5 * nrows))\n",
    "\n",
    "    if nrows == 1:  # If there's only one numerical variable, wrap axes in a list\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, var in enumerate(numerical_columns):\n",
    "        sns.histplot(df[var], ax=axes[i], kde=kde, bins=bins)\n",
    "        axes[i].set_title(f'Distribution of {var}')\n",
    "        axes[i].set_xlabel(var)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_date_monthly_counts(df, date_column, title):\n",
    "    \"\"\"\n",
    "    Plots the monthly counts of a date column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    - date_column: string, name of the date column in df to plot.\n",
    "    - title: string, title of the plot.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['month'] = df[date_column].dt.to_period('M')\n",
    "\n",
    "    monthly_counts = df['month'].value_counts().sort_index()\n",
    "    monthly_counts.plot(kind='bar', figsize=(14, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "def add_percentage_labels(ax, hue_order):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        width = p.get_width()\n",
    "        x = p.get_x()\n",
    "        y = p.get_y()\n",
    "        label_text = f'{height:.1f}%'\n",
    "        label_x = x + width / 2\n",
    "        label_y = y + height / 2\n",
    "        ax.text(label_x, label_y, label_text, ha='center', va='center', fontsize=9, color='white', weight='bold')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Import & Wrangling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remap_values(df, column, mapping):\n",
    "    # assert that all values in the column are in the mapping except for NaN\n",
    "    assert df[column].dropna().isin(mapping.keys()).all()\n",
    "    \n",
    "    df[column] = df[column].map(mapping, na_action='ignore')\n",
    "    return df\n",
    "\n",
    "def map_empty_to_nan(df, column):\n",
    "    if df[column].dtype != 'object':\n",
    "        return df\n",
    "\n",
    "    df[column] = df[column].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    return df\n",
    "\n",
    "def read_csv(file_path, sep=\";\", dtypes=None):\n",
    "    df = pd.read_csv(file_path, sep=sep, dtype=dtypes)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df = map_empty_to_nan(df, col)\n",
    "        \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accounts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accounts = read_csv(\"data/account.csv\")\n",
    "\n",
    "# Translated frequency from Czech to English\n",
    "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "accounts = remap_values(accounts, 'frequency', {\n",
    "    \"POPLATEK MESICNE\": \"MONTHLY_ISSUANCE\",\n",
    "    \"POPLATEK TYDNE\": \"WEEKLY_ISSUANCE\",\n",
    "    \"POPLATEK PO OBRATU\": \"ISSUANCE_AFTER_TRANSACTION\"\n",
    "})\n",
    "\n",
    "accounts['date'] = pd.to_datetime(accounts['date'], format='%y%m%d')\n",
    "\n",
    "accounts.rename(columns={'date': 'account_created',\n",
    "                         'frequency': 'account_frequency'}, inplace=True)\n",
    "\n",
    "accounts.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "accounts.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accounts.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(accounts, ['account_frequency'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(accounts, ['account_created'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "clients = read_csv(\"data/client.csv\")\n",
    "\n",
    "def parse_birth_number(birth_number):\n",
    "    birth_number_str = str(birth_number)\n",
    "\n",
    "    # Extract year, month, and day from birth number from string\n",
    "    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "    year = int(birth_number_str[:2])\n",
    "    month = int(birth_number_str[2:4])\n",
    "    day = int(birth_number_str[4:6])\n",
    "\n",
    "    # Determine sex based on month and adjust month for female clients\n",
    "    # according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "    if month > 50:\n",
    "        sex = \"Female\"\n",
    "        month -= 50\n",
    "    else:\n",
    "        sex = \"Male\"\n",
    "\n",
    "    # Validate date\n",
    "    assert 1 <= month <= 12\n",
    "    assert 1 <= day <= 31\n",
    "    assert 0 <= year <= 99\n",
    "\n",
    "    if month in [4, 6, 9, 11]:\n",
    "        assert 1 <= day <= 30\n",
    "    elif month == 2:\n",
    "        assert 1 <= day <= 29\n",
    "    else:\n",
    "        assert 1 <= day <= 31\n",
    "\n",
    "    # Assuming all dates are in the 1900s\n",
    "    birth_date = datetime(1900 + year, month, day)\n",
    "    return pd.Series([sex, birth_date])\n",
    "\n",
    "\n",
    "clients[['sex', 'birth_date']] = clients['birth_number'].apply(parse_birth_number)\n",
    "\n",
    "# Calculate 'age' assuming the reference year is 1999\n",
    "clients['age'] = clients['birth_date'].apply(lambda x: 1999 - x.year)\n",
    "\n",
    "# Drop 'birth_number' column as it is no longer needed\n",
    "clients = clients.drop(columns=['birth_number'])\n",
    "\n",
    "clients.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "clients.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "clients.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(clients, ['birth_date', 'age'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dispositions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions = read_csv(\"data/disp.csv\")\n",
    "\n",
    "\n",
    "dispositions.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(dispositions, ['type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the goal of this model is to address accounts and not client directly we will focus on the clients which own an account and focus solely on them. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions = dispositions[dispositions['type'] == 'OWNER']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Orders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "orders = read_csv(\"data/order.csv\")\n",
    "\n",
    "# Translated from Czech to English\n",
    "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "orders = remap_values(orders, 'k_symbol', {\n",
    "    \"POJISTNE\": \"Insurance_Payment\",\n",
    "    \"SIPO\": \"Household\",\n",
    "    \"LEASING\": \"Leasing\",\n",
    "    \"UVER\": \"Loan_Payment\"\n",
    "})\n",
    "\n",
    "orders['account_to'] = orders['account_to'].astype('category')\n",
    "\n",
    "orders = orders.rename(columns={'amount': 'debited_amount'})\n",
    "\n",
    "orders.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "orders.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "orders.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "orders.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There appear to be as many order ids as there are rows. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(orders, ['k_symbol', 'bank_to'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(orders, ['debited_amount'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transactions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# column 8 is the 'bank' column which contains NaNs and must be read as string\n",
    "transactions = read_csv(\"data/trans.csv\", dtypes={8: str})\n",
    "\n",
    "transactions['date'] = pd.to_datetime(transactions['date'], format='%y%m%d')\n",
    "\n",
    "# Translated type, operations and characteristics from Czech to English\n",
    "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "transactions = remap_values(transactions, 'type', {\n",
    "    \"VYBER\": \"Withdrawal\", # Also withdrawal as it is against the documentation present in the dataset\n",
    "    \"PRIJEM\": \"Credit\",\n",
    "    \"VYDAJ\": \"Withdrawal\"\n",
    "})\n",
    "\n",
    "transactions = remap_values(transactions, 'operation', {\n",
    "    \"VYBER KARTOU\": \"Credit Card Withdrawal\",\n",
    "    \"VKLAD\": \"Credit in Cash\",\n",
    "    \"PREVOD Z UCTU\": \"Collection from Another Bank\",\n",
    "    \"VYBER\": \"Withdrawal in Cash\",\n",
    "    \"PREVOD NA UCET\": \"Remittance to Another Bank\"\n",
    "})\n",
    "\n",
    "transactions = remap_values(transactions, 'k_symbol', {\n",
    "    \"POJISTNE\": \"Insurance Payment\",\n",
    "    \"SLUZBY\": \"Payment on Statement\",\n",
    "    \"UROK\": \"Interest Credited\",\n",
    "    \"SANKC. UROK\": \"Sanction Interest\",\n",
    "    \"SIPO\": \"Household\",\n",
    "    \"DUCHOD\": \"Old-age Pension\",\n",
    "    \"UVER\": \"Loan Payment\"\n",
    "})\n",
    "\n",
    "# Set the amount to negative for withdrawals and positive for credits\n",
    "transactions['amount'] = np.where(transactions['type'] == \"Credit\", transactions['amount'], -transactions['amount'])\n",
    "\n",
    "transactions.rename(columns={'type': 'transaction_type'}, inplace=True)\n",
    "\n",
    "transactions.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "transactions.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transactions.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(transactions, ['transaction_type', 'operation', 'k_symbol'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(transactions, ['date', 'amount', 'balance'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the distributions of the transaction table we can see that the count of transactions per year increase over time. So we can conclude that the bank has a growing client base. \n",
    "\n",
    "However, the other plots are not very useful. For one the transaction amount seems to be very sparse, ranging from values between -80000 and 80000. \n",
    "\n",
    "The balance distribution also showcases that there are accounts with a negative balance after a transaction, which would only make sense if debt is also included in this value. \n",
    "\n",
    "According to description of the field balance: \"balance after transaction\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Getting a list of unique years from the dataset\n",
    "transactions['year'] = transactions['date'].dt.year\n",
    "transactions['month'] = transactions['date'].dt.month\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "years = sorted(transactions['year'].unique())\n",
    "\n",
    "# Creating a figure with subplots for each year: one row for each year with two plots (box plot and bar chart)\n",
    "fig, axs = plt.subplots(len(years) * 2, 1, figsize=(15, 6 * len(years)), sharex=True, gridspec_kw={'height_ratios': [3, 1] * len(years)})\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    # Filter transactions for the current year\n",
    "    yearly_transactions = transactions[transactions['year'] == year]\n",
    "\n",
    "    # Preparing data for the box plot: a list of amounts for each month for the current year\n",
    "    amounts_per_month_yearly = [yearly_transactions[yearly_transactions['month'] == month]['amount'] for month in range(1, 13)]\n",
    "\n",
    "    # Preparing data for the bar chart for the current year\n",
    "    monthly_summary_yearly = yearly_transactions.groupby('month').agg(TotalAmount=('amount', 'sum'), TransactionCount=('amount', 'count')).reset_index()\n",
    "\n",
    "    # Box plot for transaction amounts by month for the current year\n",
    "    axs[i*2].boxplot(amounts_per_month_yearly, patch_artist=True)\n",
    "    # now with seaborn\n",
    "    # sns.boxplot(data=yearly_transactions, x='month', y='amount', ax=axs[i*2])\n",
    "    axs[i*2].set_title(f'Transaction Amounts Per Month in {year} (Box Plot)')\n",
    "    axs[i*2].set_yscale('symlog')\n",
    "    axs[i*2].set_ylabel('Transaction Amounts (log scale)')\n",
    "    axs[i*2].grid(True, which='both')\n",
    "\n",
    "    # Bar chart for transaction count by month for the current year\n",
    "    axs[i*2 + 1].bar(monthly_summary_yearly['month'], monthly_summary_yearly['TransactionCount'], color='tab:red', alpha=0.6)\n",
    "    axs[i*2 + 1].set_ylabel('Transaction Count')\n",
    "    axs[i*2 + 1].grid(True, which='both')\n",
    "\n",
    "# Setting x-ticks and labels for the last bar chart (shared x-axis for all)\n",
    "axs[-1].set_xticks(range(1, 13))\n",
    "axs[-1].set_xticklabels(months)\n",
    "axs[-1].set_xlabel('Month')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importing required libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'transactions' DataFrame already contains 'year' and 'month' columns\n",
    "# and is ready for visualization\n",
    "\n",
    "# Getting a list of unique years and defining month labels\n",
    "years = sorted(transactions['year'].unique())\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Adjusting the figure layout to place visualizations for each year next to each other\n",
    "fig, axs = plt.subplots(2, len(years), figsize=(40 * len(years) / 2, 30), sharey='row', gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    # Filter transactions for the current year\n",
    "    yearly_transactions = transactions[transactions['year'] == year]\n",
    "\n",
    "    # Preparing data for the box plot: a list of amounts for each month for the current year\n",
    "    amounts_per_month_yearly = [yearly_transactions[yearly_transactions['month'] == month]['amount'] for month in range(1, 13)]\n",
    "\n",
    "    # Preparing data for the bar chart for the current year\n",
    "    monthly_summary_yearly = yearly_transactions.groupby('month').agg(TotalAmount=('amount', 'sum'), TransactionCount=('amount', 'count')).reset_index()\n",
    "\n",
    "    # Selecting the appropriate axes for multiple or single year scenarios\n",
    "    ax_box = axs[0, i] if len(years) > 1 else axs[0]\n",
    "    ax_bar = axs[1, i] if len(years) > 1 else axs[1]\n",
    "    \n",
    "    ax_box.boxplot(amounts_per_month_yearly, patch_artist=True)\n",
    "    ax_box.set_title(f'{year} (Box Plot)')\n",
    "    ax_box.set_yscale('symlog')\n",
    "    ax_box.set_ylabel('Transaction Amounts (log scale)')\n",
    "    ax_box.grid(True, which='both')\n",
    "\n",
    "    ax_bar.bar(monthly_summary_yearly['month'], monthly_summary_yearly['TransactionCount'], color='tab:red', alpha=0.6)\n",
    "    ax_bar.set_ylabel('Transaction Count')\n",
    "    ax_bar.grid(True, which='both')\n",
    "\n",
    "    # Setting common x-ticks and labels for all axes\n",
    "    ax_bar.set_xticks(range(1, 13))\n",
    "    ax_bar.set_xticklabels(months)\n",
    "\n",
    "# Setting a common x-label for the entire figure\n",
    "fig.text(0.5, 0.04, 'Month', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Filtering transactions with balances below 0\n",
    "negative_balances = transactions[transactions['balance'] < 0]\n",
    "\n",
    "print(len(negative_balances))\n",
    "\n",
    "plot_numerical_distributions(negative_balances, ['balance', 'amount'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There appear to be 2999 transactions which have a negative balance, therefore after the transaction the account balance was negative. This implies that these accounts are in some kind of debt. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loans = read_csv(\"data/loan.csv\")\n",
    "\n",
    "loans['date'] = pd.to_datetime(loans['date'], format='%y%m%d')\n",
    "\n",
    "loans['status'] = loans['status'].map({\n",
    "    \"A\": \"Contract finished, no problems\",\n",
    "    \"B\": \"Contract finished, loan not paid\",\n",
    "    \"C\": \"Contract running, OK thus-far\",\n",
    "    \"D\": \"Contract running, client in debt\"\n",
    "})\n",
    "\n",
    "loans.rename(columns={\n",
    "    'date': 'granted_date',\n",
    "    'amount': 'amount',\n",
    "    'duration': 'duration',\n",
    "    'payments': 'monthly_payments',\n",
    "    'status': 'status'\n",
    "}, inplace=True)\n",
    "\n",
    "loans.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "loans.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loans.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loans.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems as if one account can have at max one loan."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(loans, ['duration', 'status'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of durations seems to be even."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(loans, ['granted_date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Credit Cards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cards = read_csv(\"data/card.csv\")\n",
    "\n",
    "cards['issued'] = pd.to_datetime(cards['issued'], format='%y%m%d %H:%M:%S').dt.date\n",
    "\n",
    "cards.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cards.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cards.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(cards, ['type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(cards, ['issued'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Demographic data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "districts = read_csv(\"data/district.csv\")\n",
    "\n",
    "# Rename columns\n",
    "# according to https://sorry.vse.cz/~berka/challenge/PAST/index.html\n",
    "districts.rename(columns={\n",
    "    'A1': 'district_id',\n",
    "    'A2': 'district_name',\n",
    "    'A3': 'region',\n",
    "    'A4': 'inhabitants',\n",
    "    'A5': 'small_municipalities',\n",
    "    'A6': 'medium_municipalities',\n",
    "    'A7': 'large_municipalities',\n",
    "    'A8': 'huge_municipalities',\n",
    "    'A9': 'cities',\n",
    "    'A10': 'ratio_urban_inhabitants',\n",
    "    'A11': 'average_salary',\n",
    "    'A12': 'unemployment_rate_1995',\n",
    "    'A13': 'unemployment_rate_1996',\n",
    "    'A14': 'entrepreneurs_per_1000_inhabitants',\n",
    "    'A15': 'crimes_committed_1995',\n",
    "    'A16': 'crimes_committed_1996'\n",
    "}, inplace=True)\n",
    "\n",
    "for col in ['unemployment_rate_1995', 'unemployment_rate_1996', 'crimes_committed_1995', 'crimes_committed_1996']:\n",
    "    districts[col] = pd.to_numeric(districts[col], errors='coerce')\n",
    "\n",
    "districts.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears as if there is 1 null value for unemployment rate in 1995 and crimes committed in 1995."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo add some basic eda here\n",
    "districts.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DeepnoteChart(districts, \"\"\"{\"layer\":[{\"layer\":[{\"mark\":{\"clip\":true,\"type\":\"bar\",\"tooltip\":true},\"encoding\":{\"x\":{\"sort\":null,\"type\":\"nominal\",\"field\":\"region\",\"scale\":{\"type\":\"linear\"},\"stack\":\"zero\"},\"y\":{\"sort\":null,\"type\":\"quantitative\",\"field\":\"crimes_committed_1995\",\"scale\":{\"type\":\"linear\"},\"stack\":\"zero\",\"format\":{\"type\":\"default\",\"decimals\":null},\"formatType\":\"numberFormatFromNumberType\"},\"color\":{\"sort\":null,\"type\":\"nominal\",\"field\":\"cities\",\"scale\":{\"scheme\":\"deepnote10\"}}}}]}],\"title\":\"\",\"config\":{\"legend\":{}},\"$schema\":\"https://vega.github.io/schema/vega-lite/v5.json\",\"encoding\":{},\"usermeta\":{}}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "districts.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "districts.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_numerical_distributions(districts, ['crimes_committed_1995'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_categorical_variables(districts, ['region'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to differentiate between the domicile of the client and account, as they can be different.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Relationships\n",
    "\n",
    "Following the documentation of the dataset, there are multiple relationships that need to be validated. https://sorry.vse.cz/~berka/challenge/PAST/index.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ERD according to the descriptions on https://sorry.vse.cz/~berka/challenge/PAST/index.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![](https://mermaid.ink/img/pako:eNqtV1Fv4jgQ_itWXu6l7SZ0gQatTsqGdhddCxVQrXSqFJnEgLWJnbOd7bGl__3GTgA3JGyvWh7ajPPNZ898nrHz7MQ8Ic7AIWJI8Urg7JEh-AVhOHkYz9FzaeqfVIKyFcJxzAumIpqg-7_QoxOUNhoNH50jcEL1Q2zQNxo9rAZq8AQrUv45EIaCYEU5Q0MYb-BeCvJPQVi8AZ-RlAVmMUE3u8Gdw0v5L7wdXTeGE6eUWNGExmxa3YIKtY5YkS2IAOBnbbYt7S1hVwsbjmb3k9loPpqMG1YHRPl-bUCSc0lNThqzfYjlpjWWYxVvfqmi2uRamMkTI-LDgySiFsNkOryeNqyei4SI_fIn2voNq1lg9j1SHGBTEtPcxPgZxk7w1tAVve2wTDlWCGdmWkg1WVBFEhRkdWDF_D2Sm2zBU8De402mWcM1FjhWRNCfZt_WkjSfBuNZELYIrQRmcp-qubaAq1XptyXMriqbsmXXVjJD3SVUffhG1ToR-AmnDVCeE1EW52vmO2gmJ9JqQ49TW4IXODWVDCVWPQVLSCqyfE_rYU_SpkltOxkVhYLt_YudZAFrm6hS-XYSNMkLkbG9urdg_C5ZDdcXiFjv1rqutewb7HHaKbxMir2cBjWs7GO2vNztEpB3nKl1ukFVAciGgKTCqpA71pmx6p05mA6b-jIWyaErg9F6wOR2lz3dIHc7XNPN4fkoqxTOEUjkbsbSstN66Nrz6SicN7fsfe-_P3HkVXCGM2KDxmA3wARZlfJMzUNdP8YjvowoW2PoW7iUZ8wvEF-i0WGw2SsrGIXOiFNIHJHRE9S9zRSlKuq6rtb7FRB9QjD8PkpwhJ4ceb7vH_PCy3P95n3UHbfk9hu59dtz_93kKxV5wNCQjD_NeDNrXEL2ioTGPi4tU3LaoRAL3S5eqfmgx2wx0VTDG8r9B7TmFYkkdE-h70ZBOYBmZuDYoWAky1NuajiCNZDI7-oJrWE9F0F_-N03efdavHvN6QGEILkgjBRCRnCwmBzXwr-2QQhAyGvPN88yqqAhRrGgGeho4gnNcz2IU14926tndYBaD9huz8_58_7KPDDtLsa6I4_2lVpHVxfSgSlpSROY5ADeUVVY-46oHQIpeUzNBPqU3nlVlP_PyYZpz-227Mja5SuWLSsqb3wa8wUk0bK3Ie1rTxlsDNfCZvS2PD4Hpv8y9BW2ssY5Z05GRIZpAt8ppuE-OmpNdJ_U0IQscZGaI-0FoLhQfLZhsTNQoiBnTpHrzl593DiDJU4ljOaY_c15tgPpGw8Xd-WnkPkiMhBn8Oz86wyuOhf9bsf96Lndvuu5ff_M2TgDz-1ddK763qXvel237131X86cn4bUvbi67PU6Xt_z3Y--e9ntv_wHEyE3kA?type=png)](https://mermaid.live/edit#pako:eNqtV1Fv4jgQ_itWXu6l7SZ0gQatTsqGdhddCxVQrXSqFJnEgLWJnbOd7bGl__3GTgA3JGyvWh7ajPPNZ898nrHz7MQ8Ic7AIWJI8Urg7JEh-AVhOHkYz9FzaeqfVIKyFcJxzAumIpqg-7_QoxOUNhoNH50jcEL1Q2zQNxo9rAZq8AQrUv45EIaCYEU5Q0MYb-BeCvJPQVi8AZ-RlAVmMUE3u8Gdw0v5L7wdXTeGE6eUWNGExmxa3YIKtY5YkS2IAOBnbbYt7S1hVwsbjmb3k9loPpqMG1YHRPl-bUCSc0lNThqzfYjlpjWWYxVvfqmi2uRamMkTI-LDgySiFsNkOryeNqyei4SI_fIn2voNq1lg9j1SHGBTEtPcxPgZxk7w1tAVve2wTDlWCGdmWkg1WVBFEhRkdWDF_D2Sm2zBU8De402mWcM1FjhWRNCfZt_WkjSfBuNZELYIrQRmcp-qubaAq1XptyXMriqbsmXXVjJD3SVUffhG1ToR-AmnDVCeE1EW52vmO2gmJ9JqQ49TW4IXODWVDCVWPQVLSCqyfE_rYU_SpkltOxkVhYLt_YudZAFrm6hS-XYSNMkLkbG9urdg_C5ZDdcXiFjv1rqutewb7HHaKbxMir2cBjWs7GO2vNztEpB3nKl1ukFVAciGgKTCqpA71pmx6p05mA6b-jIWyaErg9F6wOR2lz3dIHc7XNPN4fkoqxTOEUjkbsbSstN66Nrz6SicN7fsfe-_P3HkVXCGM2KDxmA3wARZlfJMzUNdP8YjvowoW2PoW7iUZ8wvEF-i0WGw2SsrGIXOiFNIHJHRE9S9zRSlKuq6rtb7FRB9QjD8PkpwhJ4ceb7vH_PCy3P95n3UHbfk9hu59dtz_93kKxV5wNCQjD_NeDNrXEL2ioTGPi4tU3LaoRAL3S5eqfmgx2wx0VTDG8r9B7TmFYkkdE-h70ZBOYBmZuDYoWAky1NuajiCNZDI7-oJrWE9F0F_-N03efdavHvN6QGEILkgjBRCRnCwmBzXwr-2QQhAyGvPN88yqqAhRrGgGeho4gnNcz2IU14926tndYBaD9huz8_58_7KPDDtLsa6I4_2lVpHVxfSgSlpSROY5ADeUVVY-46oHQIpeUzNBPqU3nlVlP_PyYZpz-227Mja5SuWLSsqb3wa8wUk0bK3Ie1rTxlsDNfCZvS2PD4Hpv8y9BW2ssY5Z05GRIZpAt8ppuE-OmpNdJ_U0IQscZGaI-0FoLhQfLZhsTNQoiBnTpHrzl593DiDJU4ljOaY_c15tgPpGw8Xd-WnkPkiMhBn8Oz86wyuOhf9bsf96Lndvuu5ff_M2TgDz-1ddK763qXvel237131X86cn4bUvbi67PU6Xt_z3Y--e9ntv_wHEyE3kA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This ERD shows how the data appears in the dataset:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNqtV99P2zAQ_lesvOyFbjCJSq2mSSHlRzRoUVq0F6TITdzWIrEz2xnqgP99ZydNTeIUhOgD5JzvPvvufJ-dJy_hKfHGHhETitcC5_cMwc8PgtnddIGeKlP_pBKUrRFOEl4yFdMU3f5C955f2Sic3HsdcEr1Q2LQFxo9qQda8BQrUv3ZEwaCYEU5QxMYd3CvBPlTEpZswSeUssQsIehiN7hzeKn-BdfhuTOcJKPEiiYwpmt1SyrUJmZlviQCgGfa7Fvae8KuFzYJ57ezebgIZ1PH6oCoaNYGJAWX1OTEme19LBe9sXSrePFmFdW20IWZPTIivt1JIloxzKLJeeRYPRcpEc3yZ9r6hNUsMXuIFQdYRBJamBjPYOwAbwtd09sOq4xjhXBupoVUkyVVJEV-3gbWzA-x3OZLngH2Fm9zzRpssMCJIoL-M_u2laRF5E_nftBTaCUwk02qFtoCrt5Kvy9hdlfZlD27ti4z9F1K1bffVG1SgR9x5oDygoiqOV8z34CYHEirDe2mtgIvcWY6GVqsfvJXkFRk-R6uhz1JX01a28lUUSjY3m_sJAvY2kR1la9nvqu8EBlrqnsNxmeV1XBdQsR6t7br2sq-wXbTTuFlWjblNKhJbXfZimq3S0DecKY22RbVDSAdAUmFVSl3rHNjtZXZjyYuXcYi3asyGL0HTGGr7GGB3O1wTbeA505WKZwjkMjdjJVlp3Wv2osoDBZuyW60__bAkVfDGc6JDZqC7YAJsq7KE5mHdv0Yj_kqpmyDQbdwVZ4p_4r4CoX7QbdXXjIKyogzSByR8SP0vc0UZyo-PT7W9X4FRD8QDH-MEhxBk-OT0WjU5YWXA_3mY9TfjyvukZNbvx2MPky-VvEJMDiS8dOMu1mTCtJUJDB2t7VMy2mHUiy1XLyq5p0es4uJIg13tPtfkOY1iSWop9B3I78aQHMz0HUoGcmLjJsejmENJB6d6gmtYT0XQV9Gp-_yHvZ4D93pAYQghSCMlELGcLCYHLfCP7dBCEDopD_fPM-pAkGME0FzqKOJJzDP7SAOeQ1tr6GlAC0NeH4eDPhTc2UeG7lLsFbksOnUNrq-kI5NS0uawiR78I6qxtp3RO3gS8kTaibQp_TOq6bUTs_P73WyYbWnUWTtcoVlz4qqG5_GXEJJdNn7kPa1pwo2gWuhEw1Tm-NzbPSXoSvYyhrnHXk5ETmmKXynGMG999SGaJ3U0JSscJmZI-0FoLhUfL5liTdWoiRHXlloZa8_brzxCmcSRvUVh4ub6tsn4WxF197LfwBRISI?type=png)](https://mermaid.live/edit#pako:eNqtV99P2zAQ_lesvOyFbjCJSq2mSSHlRzRoUVq0F6TITdzWIrEz2xnqgP99ZydNTeIUhOgD5JzvPvvufJ-dJy_hKfHGHhETitcC5_cMwc8PgtnddIGeKlP_pBKUrRFOEl4yFdMU3f5C955f2Sic3HsdcEr1Q2LQFxo9qQda8BQrUv3ZEwaCYEU5QxMYd3CvBPlTEpZswSeUssQsIehiN7hzeKn-BdfhuTOcJKPEiiYwpmt1SyrUJmZlviQCgGfa7Fvae8KuFzYJ57ezebgIZ1PH6oCoaNYGJAWX1OTEme19LBe9sXSrePFmFdW20IWZPTIivt1JIloxzKLJeeRYPRcpEc3yZ9r6hNUsMXuIFQdYRBJamBjPYOwAbwtd09sOq4xjhXBupoVUkyVVJEV-3gbWzA-x3OZLngH2Fm9zzRpssMCJIoL-M_u2laRF5E_nftBTaCUwk02qFtoCrt5Kvy9hdlfZlD27ti4z9F1K1bffVG1SgR9x5oDygoiqOV8z34CYHEirDe2mtgIvcWY6GVqsfvJXkFRk-R6uhz1JX01a28lUUSjY3m_sJAvY2kR1la9nvqu8EBlrqnsNxmeV1XBdQsR6t7br2sq-wXbTTuFlWjblNKhJbXfZimq3S0DecKY22RbVDSAdAUmFVSl3rHNjtZXZjyYuXcYi3asyGL0HTGGr7GGB3O1wTbeA505WKZwjkMjdjJVlp3Wv2osoDBZuyW60__bAkVfDGc6JDZqC7YAJsq7KE5mHdv0Yj_kqpmyDQbdwVZ4p_4r4CoX7QbdXXjIKyogzSByR8SP0vc0UZyo-PT7W9X4FRD8QDH-MEhxBk-OT0WjU5YWXA_3mY9TfjyvukZNbvx2MPky-VvEJMDiS8dOMu1mTCtJUJDB2t7VMy2mHUiy1XLyq5p0es4uJIg13tPtfkOY1iSWop9B3I78aQHMz0HUoGcmLjJsejmENJB6d6gmtYT0XQV9Gp-_yHvZ4D93pAYQghSCMlELGcLCYHLfCP7dBCEDopD_fPM-pAkGME0FzqKOJJzDP7SAOeQ1tr6GlAC0NeH4eDPhTc2UeG7lLsFbksOnUNrq-kI5NS0uawiR78I6qxtp3RO3gS8kTaibQp_TOq6bUTs_P73WyYbWnUWTtcoVlz4qqG5_GXEJJdNn7kPa1pwo2gWuhEw1Tm-NzbPSXoSvYyhrnHXk5ETmmKXynGMG999SGaJ3U0JSscJmZI-0FoLhUfL5liTdWoiRHXlloZa8_brzxCmcSRvUVh4ub6tsn4WxF197LfwBRISI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Verify 1:1 relationships between CLIENT, LOAN and DISPOSITION\n",
    "assert dispositions['client_id'].is_unique, \"Each client_id should appear exactly once in the DISPOSITION DataFrame.\"\n",
    "assert loans['account_id'].is_unique, \"Each account_id should appear exactly once in the LOAN DataFrame.\"\n",
    "\n",
    "# Verify 1:M relationships between ACCOUNT and DISPOSITION\n",
    "#assert dispositions['account_id'].is_unique == False, \"An account_id should appear more than once in the DISPOSITION DataFrame.\"\n",
    "assert dispositions['account_id'].is_unique == True, \"An account_id should appear once in the DISPOSITION DataFrame.\"\n",
    "# TODO check if in accordance to decision to remove disponents from dispositions\n",
    "\n",
    "# Verify each district_id in ACCOUNT and CLIENT exists in DISTRICT\n",
    "assert set(accounts['district_id']).issubset(\n",
    "    set(districts['district_id'])), \"All district_ids in ACCOUNT should exist in DISTRICT.\"\n",
    "assert set(clients['district_id']).issubset(\n",
    "    set(districts['district_id'])), \"All district_ids in CLIENT should exist in DISTRICT.\"\n",
    "\n",
    "# Verify each account_id in DISPOSITION, ORDER, TRANSACTION, and LOAN exists in ACCOUNT\n",
    "assert set(dispositions['account_id']).issubset(\n",
    "    set(accounts['account_id'])), \"All account_ids in DISPOSITION should exist in ACCOUNT.\"\n",
    "assert set(orders['account_id']).issubset(\n",
    "    set(accounts['account_id'])), \"All account_ids in ORDER should exist in ACCOUNT.\"\n",
    "assert set(transactions['account_id']).issubset(\n",
    "    set(accounts['account_id'])), \"All account_ids in TRANSACTION should exist in ACCOUNT.\"\n",
    "assert set(loans['account_id']).issubset(\n",
    "    set(accounts['account_id'])), \"All account_ids in LOAN should exist in ACCOUNT.\"\n",
    "\n",
    "# Verify each client_id in DISPOSITION exists in CLIENT\n",
    "assert set(dispositions['client_id']).issubset(\n",
    "    set(clients['client_id'])), \"All client_ids in DISPOSITION should exist in CLIENT.\"\n",
    "\n",
    "# Verify each disp_id in CARD exists in DISPOSITION\n",
    "assert set(cards['disp_id']).issubset(set(dispositions['disp_id'])), \"All disp_ids in CARD should exist in DISPOSITION.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions[dispositions[\"account_id\"] == 3980]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# are there any cards issued to dispositions that are not of type 'OWNER'?\n",
    "cards[~cards['disp_id'].isin(dispositions[dispositions['type'] == \"OWNER\"]['disp_id'])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dispositions[dispositions['type'] != \"OWNER\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get all clients associated with account 3980\n",
    "clients[clients['client_id'].isin(dispositions[dispositions['account_id'] == 3980]['client_id'])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non-transactional Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "orders_pivot = orders.pivot_table(index='account_id', \n",
    "                                 columns='k_symbol', \n",
    "                                 values='debited_amount',\n",
    "                                 aggfunc='sum', \n",
    "                                 fill_value=0)\n",
    "\n",
    "orders_pivot.columns = [f'k_symbol_debited_sum_{col.lower()}' for col in orders_pivot.columns]\n",
    "\n",
    "# TODO: find something better than this\n",
    "orders_pivot = orders_pivot.reset_index() # Use created index as account_id\n",
    "orders_pivot.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def merge_non_transactional_data(clients, districts, dispositions, accounts, orders, loans, cards):\n",
    "    # Rename district_id for clarity in clients and accounts DataFrames\n",
    "    clients = clients.rename(columns={'district_id': 'client_district_id'})\n",
    "    accounts = accounts.rename(columns={'district_id': 'account_district_id'})\n",
    "    \n",
    "    # Prepare districts dataframe for merge with prefix for clients and accounts\n",
    "    districts_client_prefixed = districts.add_prefix('client_')\n",
    "    districts_account_prefixed = districts.add_prefix('account_')\n",
    "    \n",
    "    # Merge district information for clients and accounts with prefixed columns\n",
    "    clients_with_districts = pd.merge(clients, districts_client_prefixed, left_on='client_district_id', right_on='client_district_id', how='left')\n",
    "    accounts_with_districts = pd.merge(accounts, districts_account_prefixed, left_on='account_district_id', right_on='account_district_id', how='left')\n",
    "\n",
    "    # Merge cards with dispositions and prefix card-related columns to avoid confusion\n",
    "    cards_prefixed = cards.add_prefix('card_')\n",
    "    dispositions_with_cards = pd.merge(dispositions, cards_prefixed, left_on='disp_id', right_on='card_disp_id', how='left')\n",
    "    \n",
    "    # Merge clients (with district info) with dispositions and cards\n",
    "    # Assuming dispositions might have columns that overlap with clients, prefix those if necessary\n",
    "    clients_dispositions_cards = pd.merge(dispositions_with_cards, clients_with_districts, on='client_id', how='left')\n",
    "    \n",
    "    # Merge the above with accounts (with district info) on account_id\n",
    "    accounts_clients_cards = pd.merge(accounts_with_districts, clients_dispositions_cards, on='account_id', how='left')\n",
    "    \n",
    "    # Merge orders DataFrame, assuming orders might contain columns that could overlap, prefix as needed\n",
    "    orders_prefixed = orders.add_prefix('order_')\n",
    "    comprehensive_df_with_orders = pd.merge(accounts_clients_cards, orders_prefixed, left_on='account_id', right_on='order_account_id', how='left')\n",
    "    \n",
    "    # Merge loans with the comprehensive dataframe (now including orders) on account_id\n",
    "    # Prefix loan-related columns to maintain clarity\n",
    "    loans_prefixed = loans.add_prefix('loan_')\n",
    "    final_df = pd.merge(comprehensive_df_with_orders, loans_prefixed, left_on='account_id', right_on='loan_account_id', how='left')\n",
    "\n",
    "    final_df['account_created'] = pd.to_datetime(final_df['account_created'])\n",
    "    final_df['card_issued'] = pd.to_datetime(final_df['card_issued'])\n",
    "    final_df['has_card'] = final_df['card_issued'].notna()\n",
    "    return final_df\n",
    "\n",
    "non_transactional_df = merge_non_transactional_data(clients, districts, dispositions, accounts, orders_pivot, loans, cards)\n",
    "non_transactional_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_transactional_df.to_csv(\"./data/non_transactional.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Card Holders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f'Distribution of Age for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}')\n",
    "sns.histplot(non_transactional_df[non_transactional_df['card_type'] == 'junior']['age'], kde=True, bins=30)\n",
    "plt.xlabel('Age of Client (presumably in 1999)')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the age distribution of Junior Card holders paints a picture on this group, however only looking at the current age may be misleading as we need to understand how old they were when the card was issued to determine if they could have been eligble for a Classic/Gold card (at least 18 when the card was issued). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_transactional_df['card_issued'] = pd.to_datetime(non_transactional_df['card_issued'])\n",
    "\n",
    "non_transactional_df['age_at_card_issuance'] = non_transactional_df['card_issued'] - non_transactional_df['birth_date']\n",
    "non_transactional_df['age_at_card_issuance'] = non_transactional_df['age_at_card_issuance'].dt.days // 365\n",
    "\n",
    "# plot the distribution of age at card issuance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f'Distribution of Age at Card Issuance for Junior Card Holders\\n total count = {len(non_transactional_df[non_transactional_df[\"card_type\"] == \"junior\"])}')\n",
    "sns.histplot(non_transactional_df[non_transactional_df['card_type'] == 'junior']['age_at_card_issuance'], kde=True, bins=30)\n",
    "plt.xlabel('Age at Card Issuance')\n",
    "plt.plot()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that roughly 1/3 of the Junior Card holders were not of legal age (assuming legal age is 18) when receiving their Junior Card."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# POTENTIAL FIX FOR THE PLOT BELOW\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f'Distribution of Age at Card Issuance for All Card Types\\n total count = {len(non_transactional_df)}')\n",
    "sns.histplot(non_transactional_df[non_transactional_df['card_type'] == 'junior']['age_at_card_issuance'], kde=True, bins=10, color='blue', label='Junior Card Holders')\n",
    "sns.histplot(non_transactional_df[non_transactional_df['card_type'] != 'junior']['age_at_card_issuance'], kde=True, bins=30, color='red', label='Non-Junior Card Holders')\n",
    "plt.legend()\n",
    "plt.xlabel('Age at Card Issuance')\n",
    "plt.plot()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing the age at issue date between Junior and non-Junior (Classic/Gold) card holders shows that there is no overlap between the two groups, which makes intutively sense. \n",
    "\n",
    "Therefore removing the subset of Junior Cards seems as valid as there is no reason to believe that there are Junior Cards issued wrongly, the subset being relatively small compared to the remaining issued cards and the fact that our target is specifically Classic/Gold Card owners.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the age distribution of Junior card holders and their occurence in comparison it seems valid to remove them as they are not the target group and make up a small subset of the complete dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# something is wrong here.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Number of Clients by Card Type')\n",
    "sns.barplot(x=['No Card', 'Classic/Gold Card Holders', 'Junior Card Holders'], y=[non_transactional_df['card_type'].isna().sum(), non_transactional_df['card_type'].isin(['gold', 'classic']).sum(), non_transactional_df['card_type'].eq('junior').sum()])\n",
    "# ensure that the number of clients is shown on the bars\n",
    "for i, v in enumerate([non_transactional_df['card_type'].isna().sum(), non_transactional_df['card_type'].isin(['gold', 'classic']).sum(), non_transactional_df['card_type'].eq('junior').sum()]):\n",
    "    plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the distribution of card holders in general we can see that the most clients are not in a possession of a credit card. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time factors on Card Status"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The time between creating an account and issuing a card may also be important when filtering customers based on their history. We should avoid filtering out potentially interesting periods and understand how the timespans between account creation and card issuance are distributed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filtered_df = non_transactional_df[non_transactional_df['card_issued'].notna() & non_transactional_df['account_created'].notna()]\n",
    "filtered_df['duration_days'] = (filtered_df['card_issued'] - filtered_df['account_created']).dt.days\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(filtered_df['duration_days'], bins=50, edgecolor='black', kde=True)\n",
    "plt.title('Distribution of Duration Between Account Creation and Card Issuance')\n",
    "plt.xlabel('Duration in Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The histogram displays a distribution with multiple peaks, indicating that there are several typical time frames for card issuance after account creation. The highest peak occurs within the first 250 days, suggesting that a significant number of cards are issued during this period. The frequency decreases as duration increases, with noticeable peaks that may correspond to specific processing batch cycles or policy changes over time. The distribution also has a long tail, suggesting that in some cases, card issuance can take a very long time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyzing the length of time a client has been with the bank in relation to their account creation date and card ownership can provide valuable insights for a bank's customer relationship management and product targeting strategies. Long-standing clients may exhibit different banking behaviors, such as product adoption and loyalty patterns, compared to newer clients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "max_account_creation_date = non_transactional_df['card_issued'].max()\n",
    "\n",
    "non_transactional_df['client_tenure_years_relative'] = (max_account_creation_date - non_transactional_df['account_created']).dt.days / 365.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(\n",
    "    data=non_transactional_df, \n",
    "    x='client_tenure_years_relative', \n",
    "    hue='has_card', \n",
    "    multiple='stack', \n",
    "    binwidth=1,\n",
    "    stat=\"percent\"\n",
    ")\n",
    "\n",
    "# Call the function to add labels\n",
    "add_percentage_labels(ax, non_transactional_df['has_card'].unique())\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('Client Tenure Relative to Latest Card Issued Date and Card Ownership')\n",
    "plt.xlabel('Client Tenure (Years, Relative to Latest Card Issuance)')\n",
    "plt.ylabel('Percentage of Clients')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bar chart shows the tenure of clients in years, categorized by whether they own a credit card (True) or not (False). Each bar represents the percentage of clients within a specific tenure range, allowing for comparison of the distribution of card ownership among clients with different lengths of association with the bank.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Demographics\n",
    "\n",
    "Using the available demographic data, we can investigate the potential correlation between demographic data and card status. The average salary may indicate a difference between cardholders and non-cardholders, as it is reasonable to assume that cardholders have a higher average salary than non-cardholders."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_transactional_df['has_card'] = ~non_transactional_df['card_card_id'].isna()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='has_card', y='client_average_salary', data=non_transactional_df)\n",
    "plt.title(\"Average Salary in Client's Region by Card Ownership\")\n",
    "plt.xlabel('Has Card')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.xticks([0, 1], ['No Card Owner', 'Card Owner'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The box plot compares the average salaries of clients who own a credit card with those who do not. Both groups have a substantial overlap in salary ranges, suggesting that while there might be a trend for card owners to have higher salaries, the difference is not significant. The median salary for card owners is slightly higher than that for non-card owners, as indicated by the median line within the respective boxes.\n",
    "\n",
    "Both distributions have outliers on the higher end, indicating that some individuals have salaries significantly above the average in both groups. However, these outliers do not dominate the general trend. \n",
    "\n",
    "It should also be noted that this plot assumes that the average salary of the region's clients remained constant over the years, which is unlikely to be true."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The group of bar charts represents the distribution of credit card ownership across various demographics, showing the percentage of clients with and without cards within different age groups, sexes, and regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_transactional_df['age_group'] = pd.cut(non_transactional_df['age'], bins=[0, 25, 40, 55, 70, 100], labels=['<25', '25-40', '40-55', '55-70', '>70'])\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Age Group\n",
    "plt.subplot(3, 1, 1)\n",
    "age_group_counts = non_transactional_df.groupby(['age_group', 'has_card']).size().unstack(fill_value=0)\n",
    "age_group_percentages = (age_group_counts.T / age_group_counts.sum(axis=1)).T * 100\n",
    "age_group_plot = age_group_percentages.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "age_group_plot.set_title('Card Ownership by Age Group')\n",
    "age_group_plot.set_ylabel('Percentage')\n",
    "add_percentage_labels(age_group_plot, non_transactional_df['has_card'].unique())\n",
    "\n",
    "# Sex\n",
    "plt.subplot(3, 1, 2)\n",
    "sex_counts = non_transactional_df.groupby(['sex', 'has_card']).size().unstack(fill_value=0)\n",
    "sex_percentages = (sex_counts.T / sex_counts.sum(axis=1)).T * 100\n",
    "sex_plot = sex_percentages.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "sex_plot.set_title('Card Ownership by Sex')\n",
    "sex_plot.set_ylabel('Percentage')\n",
    "add_percentage_labels(sex_plot, non_transactional_df['has_card'].unique())\n",
    "\n",
    "# Client Region\n",
    "plt.subplot(3, 1, 3)\n",
    "region_counts = non_transactional_df.groupby(['client_region', 'has_card']).size().unstack(fill_value=0)\n",
    "region_percentages = (region_counts.T / region_counts.sum(axis=1)).T * 100\n",
    "region_plot = region_percentages.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "region_plot.set_title('Card Ownership by Client Region')\n",
    "region_plot.set_ylabel('Percentage')\n",
    "region_plot.tick_params(axis='x', rotation=45)\n",
    "add_percentage_labels(region_plot, non_transactional_df['has_card'].unique())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Card Ownership by Age Group:**\n",
    "The bar chart displays the proportion of cardholders in different age groups. The percentage of cardholders is lowest in the age group of over 70, followed by the age group of 55-70, indicating that card ownership is more prevalent among younger demographics.\n",
    "\n",
    "**Card Ownership by Sex:**\n",
    "The bar chart shows the breakdown of card ownership by sex. The data reveals that the percentage of cardholders is comparable between both sexes, and no significant difference is present.\n",
    "\n",
    "**Card Ownership by Region**\n",
    "The bar chart at the bottom illustrates card ownership across different regions, showing a relatively consistent pattern among most regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Impact of Loans / Debt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "simplified_loan_status_mapping = {\n",
    "    \"Contract finished, no problems\": \"Finished\",\n",
    "    \"Contract finished, loan not paid\": \"Not Paid\",\n",
    "    \"Contract running, OK thus-far\": \"Running\",\n",
    "    \"Contract running, client in debt\": \"In Debt\",\n",
    "    \"No Loan\": \"No Loan\"\n",
    "}\n",
    "\n",
    "non_transactional_df['loan_status_simplified'] = non_transactional_df['loan_status'].map(simplified_loan_status_mapping)\n",
    "\n",
    "loan_status_simplified_card_ownership_counts = non_transactional_df.groupby(['loan_status_simplified', 'has_card']).size().unstack(fill_value=0)\n",
    "loan_status_simplified_card_ownership_percentages = (loan_status_simplified_card_ownership_counts.T / loan_status_simplified_card_ownership_counts.sum(axis=1)).T * 100\n",
    "\n",
    "loan_status_simplified_card_ownership_percentages.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title('Interaction Between Simplified Loan Status and Card Ownership')\n",
    "plt.xlabel('Simplified Loan Status')\n",
    "plt.ylabel('Percentage of Clients')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Has Card', labels=['No Card', 'Has Card'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transactional Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "zero_amount_transactions = transactions[transactions['amount'] == 0]\n",
    "\n",
    "zero_amount_transactions_info = {\n",
    "    'total_zero_amount_transactions': len(zero_amount_transactions),\n",
    "    'unique_accounts_with_zero_amount': zero_amount_transactions['account_id'].nunique(),\n",
    "    'transaction_type_distribution': zero_amount_transactions['transaction_type'].value_counts(normalize=True),\n",
    "    'operation_distribution': zero_amount_transactions['operation'].value_counts(normalize=True),\n",
    "    'k_symbol_distribution': zero_amount_transactions['k_symbol'].value_counts(normalize=True)\n",
    "}\n",
    "\n",
    "zero_amount_transactions_info, len(zero_amount_transactions_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accounts_with_zero_amount_transactions = zero_amount_transactions['account_id'].unique()\n",
    "accounts_features = accounts[accounts['account_id'].isin(accounts_with_zero_amount_transactions)]\n",
    "\n",
    "accounts_features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregate Transactions on a Monthly Basis\n",
    "\n",
    "Validating first transactions where the amount equals the balance is essential for the integrity of our aggregated data analysis. This specific assertion underpins the reliability of our subsequent aggregation operations by ensuring each account's financial history starts from a verifiable point."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def validate_first_transactions(transactions):\n",
    "    \"\"\"\n",
    "    Validates that for each account in the transactions DataFrame, there is at least\n",
    "    one transaction where the amount equals the balance on the account's first transaction date.\n",
    "\n",
    "    Parameters:\n",
    "    - transactions (pd.DataFrame): DataFrame containing transaction data with columns\n",
    "      'account_id', 'date', 'amount', and 'balance'.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If not every account has a first transaction where the amount equals the balance.\n",
    "    \"\"\"\n",
    "\n",
    "    first_dates = transactions.groupby('account_id')['date'].min().reset_index(name='first_date')\n",
    "\n",
    "    first_trans = pd.merge(transactions, first_dates, how='left', on=['account_id'])\n",
    "\n",
    "    first_trans_filtered = first_trans[(first_trans['date'] == first_trans['first_date']) & (first_trans['amount'] == first_trans['balance'])]\n",
    "\n",
    "    first_trans_filtered = first_trans_filtered.drop_duplicates(subset=['account_id'])\n",
    "\n",
    "    unique_accounts = transactions['account_id'].nunique()\n",
    "    assert unique_accounts == first_trans_filtered['account_id'].nunique(), \"Not every account has a first transaction where the amount equals the balance.\"\n",
    "\n",
    "    return \"Validation successful: Each account has a first transaction where the amount equals the balance.\"\n",
    "\n",
    "validate_first_transactions(transactions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can confirm the truth of the assertions made. It is certain that there is a transaction with an amount equal to the balance in the transaction history of any account on the first date."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function `aggregate_transactions_monthly` is designed to process and summarize financial transactions on a monthly basis for each account within a dataset. The explanation of its workings, step by step, is as follows:\n",
    "\n",
    "1. **Sorting Transactions**: Initially, the function sorts the transactions in the provided DataFrame `df` based on `account_id` and the transaction `date`. This ensures that all transactions for a given account are ordered chronologically, which is crucial for accurate monthly aggregation and cumulative balance calculation.\n",
    "\n",
    "2. **Monthly Grouping**: Each transaction's date is then converted to a monthly period using `dt.to_period(\"M\")`. This step categorizes each transaction by the month and year it occurred, facilitating the aggregation of transactions on a monthly basis.\n",
    "\n",
    "3. **Aggregation of Monthly Data**: The function groups the sorted transactions by `account_id` and the newly created `month` column. For each group, it calculates several metrics:\n",
    "   - `volume`: The sum of all transactions' amounts for the month, representing the total money flow.\n",
    "   - `total_abs_amount`: The sum of the absolute values of the transactions' amounts, indicating the total amount of money moved, disregarding the direction.\n",
    "   - `transaction_count`: The count of transactions, providing a sense of activity level.\n",
    "   - `positive_transaction_count` and `negative_transaction_count`: The counts of positive (inflows) and negative (outflows) transactions, respectively. This distinction can help identify the balance between income and expenses.\n",
    "   - Statistical measures like `average_amount`, `median_amount`, `min_amount`, `max_amount`, and `std_amount` offer insights into the distribution of transaction amounts.\n",
    "   - `type_count`, `operation_count`, and `k_symbol_count`: The counts of unique transaction types, operations, and transaction symbols (k_symbol), respectively, indicating the diversity of transaction characteristics.\n",
    "\n",
    "4. **Cumulative Balance Calculation**: After aggregating the monthly data, the function computes a cumulative balance (`balance`) for each account by cumulatively summing the `volume` (total transaction amount) over time. This step provides insight into how the account balance evolves over the months.\n",
    "\n",
    "5. **Validation of Aggregated Data**: The `validate_monthly_aggregated_transactions` function is invoked to ensure the integrity and correctness of the aggregated data through several assertions:\n",
    "   - The balance should consistently increase or decrease based on whether the total monthly transaction volume is positive or negative, respectively.\n",
    "   - For each account, the balance in the first month should equal the total transaction volume of that month.\n",
    "   - The sum of positive and negative transaction counts must equal the total transaction count for each month.\n",
    "   - The number of unique accounts in the aggregated data should match that in the original dataset.\n",
    "   - The final balances of accounts in the aggregated data should closely match their last recorded transactions in the original dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aggregate_transactions_monthly(df):\n",
    "    \"\"\"\n",
    "    Aggregate financial transaction data on a monthly basis per account.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing financial transaction data with 'account_id', 'date', and other relevant columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Monthly aggregated financial transaction data per account.\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(by=[\"account_id\", \"date\"])\n",
    "    df_sorted[\"month\"] = df_sorted[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "    monthly_aggregated_data = (\n",
    "            df_sorted.groupby([\"account_id\", \"month\"])\n",
    "            .agg(\n",
    "                volume=(\"amount\", \"sum\"),\n",
    "                total_abs_amount=(\"amount\", lambda x: x.abs().sum()),\n",
    "                transaction_count=(\"amount\", \"count\"),\n",
    "                positive_transaction_count=(\"amount\", lambda x: (x >= 0).sum()), # TODO: it seems that there are some transactions with 0 amount, how to handle those?\n",
    "                negative_transaction_count=(\"amount\", lambda x: (x < 0).sum()),\n",
    "                average_amount=(\"amount\", \"mean\"),\n",
    "                median_amount=(\"amount\", \"median\"),\n",
    "                min_amount=(\"amount\", \"min\"),\n",
    "                max_amount=(\"amount\", \"max\"),\n",
    "                std_amount=(\"amount\", \"std\"),\n",
    "                type_count=(\"transaction_type\", \"nunique\"),\n",
    "                operation_count=(\"operation\", \"nunique\"),\n",
    "                k_symbol_count=(\"k_symbol\", \"nunique\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "            .sort_values(by=[\"account_id\", \"month\"])\n",
    "        )\n",
    "\n",
    "    monthly_aggregated_data[\"balance\"] = monthly_aggregated_data.groupby(\"account_id\")[\"volume\"].cumsum()\n",
    "\n",
    "    validate_monthly_aggregated_transactions(monthly_aggregated_data, df)\n",
    "\n",
    "    return monthly_aggregated_data\n",
    "\n",
    "def validate_monthly_aggregated_transactions(aggregated_data, original_df):\n",
    "    \"\"\"\n",
    "    Validate the integrity and correctness of aggregated monthly financial transactions.\n",
    "\n",
    "    Parameters:\n",
    "    - aggregated_data (pd.DataFrame): Aggregated monthly transaction data.\n",
    "    - original_df (pd.DataFrame): Original dataset of financial transactions.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If validation conditions are not met.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert (aggregated_data[\"volume\"] >= 0).all() == (\n",
    "        aggregated_data[\"balance\"].diff() >= 0\n",
    "    ).all(), \"If the total amount is positive, the balance should go up.\"\n",
    "\n",
    "    assert (aggregated_data[\"volume\"] < 0).all() == (\n",
    "        aggregated_data[\"balance\"].diff() < 0\n",
    "    ).all(), \"If the total amount is negative, the balance should go down.\"\n",
    "\n",
    "    first_month = aggregated_data.groupby(\"account_id\").nth(0)\n",
    "    assert (\n",
    "        first_month[\"volume\"] == first_month[\"balance\"]\n",
    "    ).all(), \"The balance should equal the volume for the first month.\"\n",
    "\n",
    "    assert (\n",
    "        aggregated_data[\"positive_transaction_count\"]\n",
    "        + aggregated_data[\"negative_transaction_count\"]\n",
    "        == aggregated_data[\"transaction_count\"]\n",
    "    ).all(), \"The sum of positive and negative transaction counts should equal the total transaction count.\"\n",
    "    \n",
    "    assert (\n",
    "        aggregated_data[\"account_id\"].nunique() == original_df[\"account_id\"].nunique()\n",
    "    ), \"The number of unique account_ids in the aggregated DataFrame should be the same as the original DataFrame.\"\n",
    "\n",
    "    assert (\n",
    "        pd.merge(\n",
    "            aggregated_data.groupby(\"account_id\")\n",
    "            .last()\n",
    "            .reset_index()[[\"account_id\", \"balance\"]],\n",
    "            original_df[original_df.groupby(\"account_id\")[\"date\"].transform(\"max\") == original_df[\"date\"]][\n",
    "                [\"account_id\", \"balance\"]\n",
    "            ],\n",
    "            on=\"account_id\",\n",
    "            suffixes=(\"_final\", \"_last\"),\n",
    "        )\n",
    "        .apply(\n",
    "            lambda x: np.isclose(x[\"balance_final\"], x[\"balance_last\"], atol=5), axis=1\n",
    "        )\n",
    "        .any()\n",
    "    ), \"Some accounts' final balances do not match their last transactions.\"\n",
    "    \n",
    "\n",
    "transactions_monthly = aggregate_transactions_monthly(transactions)\n",
    "validate_monthly_aggregated_transactions(transactions_monthly, transactions)\n",
    "transactions_monthly.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monthly Balance Difference and Volume\n",
    "\n",
    "This plot gives a clear picture of how money moves in and out of an account each month and how these movements affect the overall balance. It does this by showing two things:\n",
    "\n",
    "- **Balance Difference**: This line shows whether the account balance went up or down each month. If the line goes up, it means the account gained money that month. If it goes down, the account lost money.\n",
    "- **Volume**: This line shows the total amount of money that moved in the account each month, regardless of whether it was coming in or going out.\n",
    "\n",
    "**What to Look For**:\n",
    "- A direct link between the amount of money moved (volume) and changes in the account balance. High incoming money should lead to an uptick in the balance, and lots of outgoing money should lead to a downturn.\n",
    "- This visual check helps to understand how active the account is and whether it’s generally getting fuller or emptier over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_monthly_balance_diff_and_volume(transactions_monthly, account_id, figsize=(12, 8)):\n",
    "    account_transactions = transactions_monthly[transactions_monthly['account_id'] == account_id].sort_values(by='month')\n",
    "    account_transactions['balance_diff'] = account_transactions['balance'].diff()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.plot(account_transactions['month'].astype(str), account_transactions['balance_diff'], marker='o', label='Balance Difference')\n",
    "    plt.plot(account_transactions['month'].astype(str), account_transactions['volume'], marker='x', linestyle='--', label='Volume')\n",
    "\n",
    "    plt.title(f'Monthly Balance Difference and Volume for Account {account_id}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_monthly_balance_diff_and_volume(transactions_monthly, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monthly Transactions, Balance, and Volume Plot Explanation\n",
    "\n",
    "This visualization offers a snapshot of an account’s activity over time by comparing money movement each month with the overall account balance. It helps to understand:\n",
    "\n",
    "- **Volume**: How much money came in or went out of the account each month. Incoming money is shown as up, and outgoing money as down.\n",
    "- **Balance**: The total money in the account at the end of each month, showing how it's changed over time due to the monthly transactions.\n",
    "\n",
    "**What to Look For**:\n",
    "- How the monthly money movement impacts the account's growing or shrinking balance. For example, a few months of high income should visibly increase the balance.\n",
    "- This simple visual guide helps spot trends, like if the account is steadily growing, holding steady, or facing issues, giving quick insights into financial well-being and further validates the aggregation made in the previous step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_monthly_transactions_balance_and_volume(transactions_monthly, account_id):\n",
    "    account_transactions = transactions_monthly[transactions_monthly['account_id'] == account_id]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.plot(account_transactions['month'].astype(str), account_transactions['volume'], marker='o', label='Volume')\n",
    "    plt.plot(account_transactions['month'].astype(str), account_transactions['balance'], marker='x', linestyle='--', label='Balance')\n",
    "\n",
    "    plt.title(f'Monthly Transactions and Balance for Account {account_id}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_monthly_transactions_balance_and_volume(transactions_monthly, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Delieverable: Closer Look at account 14 and 18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Account 14 "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_monthly_transactions_balance_and_volume(transactions_monthly, 14)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Account 14 shows a rather conservative transaction history. The spending habits are all withing range of 10k to -10k per month. We can see little volatility, the account shows a slight trend of growing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Account 18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_monthly_transactions_balance_and_volume(transactions_monthly, 18)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Account 18 paints a different picture in comparison to account 14. \n",
    "\n",
    "The volatility here is a lot higher, indiciating a potential for a business account or high income household. Especially March 1994 to December 1994 show some volatile transaction habits. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Analysis\n",
    "Looking at the balance and volume per month for the accounts 14 and 18 we can notice some interesting patterns.\n",
    "\n",
    "###### Account 14:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set artificial issue date for non-card holders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_months_since_account_to_card(df):\n",
    "    df['months_since_account_to_card'] = df.apply(\n",
    "        lambda row: (row['card_issued'].to_period('M') - row['account_created'].to_period('M')).n\n",
    "        if pd.notnull(row['card_issued']) and pd.notnull(row['account_created']) else np.nan, axis=1)\n",
    "    return df\n",
    "\n",
    "def filter_clients_without_sufficient_history(non_transactional_df, min_history_months=25):\n",
    "    if 'months_since_account_to_card' not in non_transactional_df.columns:\n",
    "        print(\"Warning: months_since_account_to_card column not found. Calculating history length.\")\n",
    "        non_transactional_df = add_months_since_account_to_card(non_transactional_df)\n",
    "\n",
    "    count_before = len(non_transactional_df)\n",
    "    filtered_df = non_transactional_df[non_transactional_df['months_since_account_to_card'].isnull() | (non_transactional_df['months_since_account_to_card'] >= min_history_months)]\n",
    "    print(f\"Filtered out {count_before - len(filtered_df)} records with less than {min_history_months} months of history. Percentage: {(count_before - len(filtered_df)) / count_before * 100:.2f}%.\")\n",
    "    return filtered_df\n",
    "\n",
    "filtered_non_transactional_df = filter_clients_without_sufficient_history(non_transactional_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_transactional_df_w_card = filtered_non_transactional_df.dropna(subset=['card_issued']).copy()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(non_transactional_df_w_card['months_since_account_to_card'], kde=True, bins=30)\n",
    "plt.title('Distribution of Months from Account Creation to Card Issuance')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following approaches are in discussion:\n",
    "- Looking at the distributions above extract the amount of history a buyer most likely has at the issue data of the card\n",
    "- For each non buyer, find a buyer which was active in a similar time window (Jaccard similarity on the Year-Month sets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Match by similar transaction activity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NEW IDEA\n",
    "\n",
    "Instead of looking at the full activity of a buyer, we only look at the pre-purchase activity as there is reason to believe that clients may change their patterns after purchasing date and therefore add unwanted bias. \n",
    "\n",
    "![](./docs/IMG_BBEF82A6C6B5-1.jpeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_eligibility_for_matching(non_cardholder, cardholder, verbose=False):\n",
    "    if cardholder['card_issued'] <= non_cardholder['account_created']:\n",
    "        return False    \n",
    "\n",
    "    period_diff = (cardholder['card_issued'].to_period('M') - non_cardholder['account_created'].to_period('M')).n\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Card issued: {cardholder['card_issued']}, Account created: {non_cardholder['account_created']}, Period diff: {period_diff}, Eligible: {period_diff >= 25}\")\n",
    "\n",
    "    return period_diff >= 25\n",
    "\n",
    "def prepare_activity_matrix(transactions):\n",
    "    transactions['month_year'] = transactions['date'].dt.to_period('M')\n",
    "    transactions['active'] = 1\n",
    "    \n",
    "    activity_matrix = transactions.pivot_table(index='account_id', \n",
    "                                    columns='month_year', \n",
    "                                    values='active', \n",
    "                                    fill_value=0)\n",
    "    \n",
    "    activity_matrix.columns = [f'active_{str(col)}' for col in activity_matrix.columns]\n",
    "    return activity_matrix\n",
    "\n",
    "def match_cardholders_with_non_cardholders(non_transactional, transactions, top_n=5):\n",
    "    with_card = non_transactional[non_transactional['card_issued'].notna()]\n",
    "    without_card = non_transactional[non_transactional['card_issued'].isna()]\n",
    "\n",
    "    activity_matrix = prepare_activity_matrix(transactions)\n",
    "    \n",
    "    with_card_activity = with_card.join(activity_matrix, on='account_id', how='left')\n",
    "    without_card_activity = without_card.join(activity_matrix, on='account_id', how='left')\n",
    "\n",
    "    matched_non_cardholders = set()\n",
    "    matches = []\n",
    "\n",
    "    for idx, cardholder in tqdm(with_card_activity.iterrows(), total=len(with_card_activity), desc='Matching cardholders'):\n",
    "        issue_period = cardholder['card_issued'].to_period('M')\n",
    "        eligible_cols = [col for col in activity_matrix if col.startswith('active') and pd.Period(col.split('_')[1]) <= issue_period]\n",
    "\n",
    "        if not eligible_cols:\n",
    "            print(f\"No eligible months found for cardholder client_id {cardholder['client_id']}.\")\n",
    "            continue\n",
    "\n",
    "        cardholder_vector = cardholder[eligible_cols].fillna(0).astype(bool).values.reshape(1, -1)\n",
    "        non_cardholder__matrix = without_card_activity[eligible_cols].fillna(0).astype(bool).values        \n",
    "        assert cardholder_vector.shape[1] == non_cardholder__matrix.shape[1], \"Dimension mismatch between cardholder and applicant activity matrix.\"\n",
    "\n",
    "        distances = pairwise_distances(cardholder_vector, non_cardholder__matrix, metric='jaccard').flatten()\n",
    "        eligible_non_cardholders = [i for i, applicant in without_card_activity.iterrows()\n",
    "                                    if check_eligibility_for_matching(applicant, cardholder) and i not in matched_non_cardholders]\n",
    "\n",
    "        if eligible_non_cardholders:\n",
    "            select_applicants(distances, eligible_non_cardholders, matches, matched_non_cardholders, cardholder, without_card_activity, top_n)\n",
    "        else:\n",
    "            print(f\"No eligible non-cardholders found for cardholder client_id {cardholder['client_id']}.\")\n",
    "            \n",
    "    return matches\n",
    "\n",
    "def select_applicants(distances, eligible_non_cardholders, matches, matched_applicants, cardholder, without_card_activity, top_n):\n",
    "    eligible_distances = distances[eligible_non_cardholders]\n",
    "    sorted_indices = np.argsort(eligible_distances)[:top_n]\n",
    "\n",
    "    if sorted_indices.size > 0:\n",
    "        selected_index = np.random.choice(sorted_indices)\n",
    "        actual_selected_index = eligible_non_cardholders[selected_index]\n",
    "\n",
    "        if actual_selected_index not in matched_applicants:\n",
    "            matched_applicants.add(actual_selected_index)\n",
    "            applicant = without_card_activity.iloc[actual_selected_index]\n",
    "            similarity = 1 - eligible_distances[selected_index]\n",
    "            \n",
    "            matches.append((cardholder['client_id'], applicant['client_id'], similarity))\n",
    "\n",
    "def augment_with_artificial_issue_dates(non_transactional_df, matches):\n",
    "    augmented_df = non_transactional_df.copy()\n",
    "    augmented_df['has_card'] = True\n",
    "\n",
    "    for cardholder_id, non_cardholder_id, _ in matches:\n",
    "        card_issue_date = augmented_df.loc[augmented_df['client_id'] == cardholder_id, 'card_issued'].values[0]\n",
    "        augmented_df.loc[augmented_df['client_id'] == non_cardholder_id, ['card_issued', 'has_card']] = [card_issue_date, False]\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "matches = match_cardholders_with_non_cardholders(filtered_non_transactional_df, transactions)\n",
    "\n",
    "print(f\"Percentage of clients with card issued: {filtered_non_transactional_df['card_issued'].notna().mean() * 100:.2f}%\")\n",
    "augmented_df = augment_with_artificial_issue_dates(filtered_non_transactional_df, matches)\n",
    "print(f\"Percentage of clients with card issued after matching: {augmented_df['card_issued'].notna().mean() * 100:.2f}%\")\n",
    "augmented_df = augmented_df.dropna(subset=['card_issued'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_activity_matrix_sparsity(activity_matrix, non_transactional_df=None):\n",
    "    sparse_matrix = activity_matrix.astype(bool)    \n",
    "    \n",
    "    if non_transactional_df is not None:\n",
    "        sparse_matrix = sparse_matrix.loc[activity_matrix.index]\n",
    "        sparse_matrix = sparse_matrix.join(non_transactional_df.set_index('account_id')['account_created'])\n",
    "        sparse_matrix = sparse_matrix.sort_values(by='account_created')\n",
    "        sparse_matrix = sparse_matrix.drop(columns='account_created')\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(sparse_matrix, cmap='viridis', cbar=True, yticklabels=False)\n",
    "    plt.title(f'Activity Matrix Sparsity Visualization')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Accounts')\n",
    "    plt.show()\n",
    "\n",
    "activity_matrix = prepare_activity_matrix(transactions)\n",
    "plot_activity_matrix_sparsity(activity_matrix, non_transactional_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rolling Window Aggregations for Card Holders\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "The `process_cardholder_transactions` function is designed to process and aggregate the transactional data for cardholders based on the history length from account creation to card issuance.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Filtering Card Issued Records**: Initially, the function filters out all records from a non-transactional dataset where no card has been issued (`card_issued` is NaN). This ensures that subsequent operations are only performed on accounts where a card has indeed been issued.\n",
    "\n",
    "2. **Calculating History Length**: For each cardholder in the filtered dataset, the function calculates the history length in months from the account creation date (`account_created`) to the card issuance date (`card_issued`). This calculation helps in identifying the duration for which the account has been active before a card was issued.\n",
    "\n",
    "3. **Identifying Eligible Cardholders**: The function then filters out cardholders based on a minimum history length criterion, specified by `min_history_months`. Only cardholders who have an account history equal to or longer than this threshold are considered eligible for further analysis.\n",
    "\n",
    "4. **Merging Transaction Data**: The eligible cardholders' information is merged with monthly transaction data based on `account_id`. This step associates each transaction with the corresponding cardholder's account information, preparing the data for detailed transaction analysis.\n",
    "\n",
    "5. **Assigning Card Issuance Date to Transactions**: For each transaction in the merged dataset, the function assigns the card issuance date from the cardholder's information. This is crucial for calculating the period before card issuance during which the transactions occurred.\n",
    "\n",
    "6. **Filtering Transactions Before Card Issuance**: Transactions are filtered based on the number of months before card issuance, defined by the `months_before_card_range` parameter. This step ensures that only transactions occurring within the specified period before card issuance are considered for aggregation.\n",
    "\n",
    "7. **Aggregating Transactions**: The function aggregates the filtered transactions by `account_id` and the number of months before card issuance. Aggregation metrics include sum, mean, median, min, max, and standard deviation for various transaction attributes like volume, absolute amount, transaction counts, and balance. This aggregation provides a comprehensive overview of spending patterns.\n",
    "\n",
    "8. **Pivoting Aggregated Data**: The aggregated transaction data is pivoted to create a wide-format DataFrame where each row represents an account, and columns represent aggregated transaction metrics for different months before card issuance. This format is useful for analyzing trends over time leading up to card issuance.\n",
    "\n",
    "9. **Final Dataset Preparation**: Finally, the function resets the index of the pivoted DataFrame, ensuring that `account_id` is a column in the final dataset, which makes it easier to identify each account's aggregated transaction metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pivot_transactions(non_transactional_df, transactions_monthly_df, months_before_card_range=(2, 13)):\n",
    "    merged_df = transactions_monthly_df.merge(non_transactional_df[['account_id']], on='account_id')\n",
    "\n",
    "    merged_df['card_issued_date'] = merged_df['account_id'].map(non_transactional_df.set_index('account_id')['card_issued'])\n",
    "    merged_df['months_before_card'] = merged_df.apply(lambda row: (row['card_issued_date'].to_period('M') - row['month']).n, axis=1)\n",
    "\n",
    "    start_month, end_month = months_before_card_range\n",
    "    filtered_df = merged_df.query(f\"{start_month} <= months_before_card <= {end_month}\")\n",
    "    \n",
    "    aggregated_data = filtered_df.groupby(['account_id', 'months_before_card']).agg({\n",
    "        'volume': 'sum',\n",
    "        'total_abs_amount': 'sum',\n",
    "        'transaction_count': 'sum',\n",
    "        'positive_transaction_count': 'sum',\n",
    "        'negative_transaction_count': 'sum',\n",
    "        'average_amount': 'mean',\n",
    "        'median_amount': 'median',\n",
    "        'min_amount': 'min',\n",
    "        'max_amount': 'max',\n",
    "        'std_amount': 'std',\n",
    "        'type_count': 'sum',\n",
    "        'operation_count': 'sum',\n",
    "        'k_symbol_count': 'sum',\n",
    "        'balance': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    pivoted_data = aggregated_data.pivot(index='account_id', columns='months_before_card')\n",
    "    pivoted_data.columns = ['_'.join(['M', str(col[1]), col[0]]) for col in pivoted_data.columns.values]\n",
    "\n",
    "    final_dataset = pivoted_data.reset_index()\n",
    "    return final_dataset\n",
    "\n",
    "transactions_pivoted = pivot_transactions(augmented_df, transactions_monthly)\n",
    "transactions_pivoted.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transactions_monthly.to_csv(\"./data/transactions_monthly.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge everything together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_df = augmented_df.merge(transactions_pivoted, on='account_id', how='left')\n",
    "assert final_df['client_id'].is_unique, \"Each client_id should appear exactly once in the final DataFrame.\"\n",
    "assert final_df['account_id'].is_unique, \"Each account_id should appear exactly once in the final DataFrame.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_df.to_csv(\"data/bronze_record.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Number of Clients by Card Issuance Status')\n",
    "sns.countplot(x='has_card', data=final_df)\n",
    "plt.xlabel('Card Issued')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Distribution of Card Issuance Dates')\n",
    "sns.histplot(final_df, x='card_issued', hue='has_card', kde=True, bins=30)\n",
    "plt.xlabel('Card Issuance Date')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deepnote_app_block_visible": true,
    "cell_id": "b24a489b767844ef836fd1cdfa1b0f01",
    "deepnote_cell_type": "markdown"
   },
   "source": "# Model Construction",
   "block_group": "9e7c7ebe286b4072a4a847cb4b9a3ca3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deepnote_app_block_visible": true,
    "cell_id": "e17f12d4fb6c4d229254c6643a32b03f",
    "deepnote_cell_type": "markdown"
   },
   "source": "# Feature Engineering",
   "block_group": "e374362f5e424ba2960cc73e371c80fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deepnote_app_block_visible": true,
    "cell_id": "4ba58e15b16d44b0a7c66a47cf498bbf",
    "deepnote_cell_type": "markdown"
   },
   "source": "# Model Engineering",
   "block_group": "9b651bd42a6f4807b1c79965f9bfd495"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deepnote_app_block_visible": true,
    "cell_id": "a203aa44aad04b7ca6e4f774e2cf2c59",
    "deepnote_cell_type": "markdown"
   },
   "source": "# Model Comparison & Selection",
   "block_group": "5eb9a9a5e13045f0bf1f1ae01e99ff81"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "2eb5522ce55941058bee1877f3f6348f",
    "deepnote_cell_type": "markdown"
   },
   "source": "# WORKBENCH - remove later",
   "block_group": "25d1c57ff9ad4323a6c5cab047569c2e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "4e33f271b19141638d6ea6e2703d4ec8",
    "deepnote_cell_type": "markdown"
   },
   "source": "### Some older input\n\nWe need some categorical indicator wheter a transactions is a transactions incoming or outgoing from the perspective of the account holder. This will be important for the feature engineering later on. We will create a column called `transaction_direction` using the amount to engineer this feature.\n\nBalance is the wealth on the account after the transaction.\n\nk_symbol is the purpose of the transaction. This is often use in the context of budgeting in E-Banking applications or just personal finance management. A lot of NA values are present in this column. We will have to deal with this later on and weigh the importance of this column.\n\nTrack the time series of a given account to get a better understanding of the datasets nature.\n\nIt seems that there can be multiple transactions on the same day. We will have to aggregate the transactions on the same day to get a better understanding of the transactions as the timestamp resolution is not high enough to track the transactions on a daily basis.\n\nWe need some handling for this as the ID is not informative as well (Dani). \n\nFor the feature enginnering a per month evaluation of the transactions is sufficient (Dani). \n\nWe need to make sure across the board that for the prediction we only use the data that is available at the time of the prediction. This means that we can only use the data from the past to predict the future. This is important to keep in mind when we engineer the features as some entities do not have any information about the date and therefore we cannot use them for the prediction as we cannot rule out that they are not from the future.\n\nFrequency analysis of the transactions could be interesting as the hypothesis might be that the more frequent the transactions the more likely the account holder is to be interested in a credit card. Fourier transformation could be used to get a better understanding of the frequency of the transactions.",
   "block_group": "d90b72435e31493bbeb38618dccd19c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "4e848e8323d948e1b72e86c1a7b02828",
    "deepnote_cell_type": "markdown"
   },
   "source": "### JITT 05.03.24\n- New customers are handled differently\n- Customer without the required history should be ignored otherwise they are treated as irrelevant\n- Lag is ignored like (12 + 1) months\n- Age should be in relation to the time of the event (card issued / reference date for refrence clients)\n- How old are customers with a Junior Card? This should be evaluated based on the data\n    - Example with Junior Card model with Age as most important feature as a negative example\n- Reference clients\n    - They should not be as similar as possible (Twin brother problem)\n    - Same external market conditions\n    - Same environment\n    - See slide 6\n- Owner and disponents cannot be distinguished directly and assumptions are required\n    - MasterCards vs Visa war: as much cards as possible for both client of an account\n    - AGain the Twin brother problem as features are too similar possibly\n\n#### General notes to self\n- Visualise monthly product puchases\n- Viz environment of selected client and reference clients and answer the questions are they from a comparable environment",
   "block_group": "604ffe812ee84bc0bfad9159040419c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "1d780d3f5250465281e7cc6370c1cac3",
    "deepnote_cell_type": "markdown"
   },
   "source": "",
   "block_group": "b119353e3b8b4fe885a230370e8f5ca3"
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7d865ccc-7b5c-4f8d-b4e6-4e008791345d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_table_of_contents_enabled": false,
  "deepnote_app_hide_all_code_blocks_enabled": false,
  "deepnote_app_reactivity_enabled": true,
  "deepnote_notebook_id": "2ef3858b0ef64bcd97d256cee39e7aba",
  "deepnote_execution_queue": []
 }
}

```{python}
import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import joblib

SEED = 1337

golden_record_df = pd.read_parquet("temp/golden_record.parquet")
gs_cache_file = "data/grid_search_cache.pkl"
reduced_model_cache_file = "data/reduced_best_model.pkl"

np.random.seed(1337)
random.seed(1337)
```

# Data Partitioning

The data is split in a 80/20 ratio for training and testing purposes. The stratification ensures that the distribution of the target variable is maintained in both sets. When actually training the models, we will additionally use cross-validation to ensure robust evaluation.

Additionally, we will create a `DataModule` class to encapsulate the training and testing data, as well as the feature columns used in the model. This class will help us manage the data and features throughout the model training and evaluation process.


```{python}
from sklearn.model_selection import train_test_split


class DataModule:
    def __init__(self, X_train, X_test, y_train, y_test, feature_columns=None):
        self.feature_columns = (
            feature_columns if feature_columns is not None else X_train.columns
        )

        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test


def create_data_module(df, feature_cols, target_col="has_card", test_size=0.2):
    X = df.drop(columns=[target_col])[feature_cols]
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, shuffle=True
    )

    return DataModule(X_train, X_test, y_train, y_test)


data_module = create_data_module(
    golden_record_df, golden_record_df.drop(columns=["has_card"]).columns
)

print(f"Train set size: {len(data_module.X_train)}")
print(f"Test set size: {len(data_module.X_test)}")

print(f"Train set distribution:\n{data_module.y_train.value_counts(normalize=True)}")
print(f"Test set distribution:\n{data_module.y_test.value_counts(normalize=True)}")
```

As we can see the distribution of the target variable is maintained in both sets after the split. The ratios are as specified in the 80/20 split.


# Model Construction

We will now construct a pipeline for training and evaluating machine learning models. The pipeline will handle preprocessing, model training, cross-validation, and evaluation. We will use this pipeline to train and evaluate multiple candidate models.

## Pipeline for Training and Evaluation

The `train_evaluate_model` function is designed to streamline the process of training and evaluating machine learning models. It performs the following steps:

1. **Preprocessing**: The function automatically handles numerical and categorical features, imputing missing values, scaling numerical features, and one-hot encoding categorical features.
2. **Model Training**: The specified model is trained on the training data.
3. **Cross-Validation**: The model is evaluated using cross-validation with specified evaluation metrics.
4. **Model Evaluation**: The model is evaluated on the test set using various metrics, including accuracy, F1 score, AUC-ROC, precision, and recall.

The pipeline is flexible and can accommodate various models and feature sets, making it a versatile tool for model development and evaluation. It returns a summary of evaluation metrics for both training and test sets, as well as the true labels and predicted probabilities for the test set.

Additionally, the pipeline supports feature selection using Recursive Feature Elimination with Cross-Validation (RFECV). This feature selection method automatically selects the most relevant features based on the model's performance during cross-validation. The selected features can be retrieved from the pipeline after training.

Last but not least, the pipeline supports hyperparameter tuning using Grid Search with Cross-Validation. This functionality allows for optimizing the model's hyperparameters to improve performance. The best hyperparameters can be retrieved after training the model.

```{python}
from sklearn.feature_selection import RFECV
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn.metrics import (
    make_scorer,
    f1_score,
    roc_auc_score,
    precision_score,
    recall_score,
)
import scikitplot as skplt
import dalex as dx


class Trainer:
    def __init__(
        self,
        data_module,
        model,
        cv=10,
        select_features=False,
        param_grid=None,
        verbose=False,
        n_jobs=-1,
    ):
        self.data_module = data_module
        self.model = model
        self.cv = cv
        self.verbose = verbose
        self.preprocessor = self._create_preprocessor()
        self.select_features = select_features
        self.param_grid = param_grid
        self.n_jobs = n_jobs
        self.pipeline = None
        self.train_metrics_report = None
        self.test_metrics_report = None

    def _create_preprocessor(self):
        numerical_features = [
            col
            for col in self.data_module.X_train.columns
            if self.data_module.X_train[col].dtype in ["int64", "float64"]
        ]
        categorical_features = [
            col
            for col in self.data_module.X_train.columns
            if col not in numerical_features
        ]

        other_features = [
            col
            for col in self.data_module.X_train.columns
            if col not in numerical_features + categorical_features
        ]
        if len(other_features) > 0:
            raise ValueError(
                f"Columns with unsupported data types found: {other_features}"
            )

        numerical_pipeline = Pipeline(
            [("imputer", SimpleImputer(strategy="mean")), ("scaler", StandardScaler())]
        )

        categorical_pipeline = Pipeline(
            [
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore")),
            ]
        )

        return ColumnTransformer(
            transformers=[
                ("num", numerical_pipeline, numerical_features),
                ("cat", categorical_pipeline, categorical_features),
            ]
        )

    def fit(self):
        model_pipeline_steps = [("model", self.model)]
        if self.select_features:
            model_pipeline_steps.insert(
                0,
                (
                    "feature_selection",
                    RFECV(self.model, verbose=3 if self.verbose else 0, cv=self.cv),
                )
            )
            
        model_pipeline = Pipeline(model_pipeline_steps)
        
        if self.param_grid is not None:
            model_pipeline = GridSearchCV(
                model_pipeline,
                self.param_grid,
                cv=self.cv,
                verbose=3 if self.verbose else 0,
                n_jobs=self.n_jobs
            )

        self.pipeline = Pipeline(
            [("preprocessor", self.preprocessor), ("model_pipeline", model_pipeline)]
        )

        self.pipeline.fit(self.data_module.X_train, self.data_module.y_train)
        return self

    @staticmethod
    def get_scoring_metrics():
        return ["accuracy", "f1_macro", "roc_auc", "precision", "recall"]

    def eval_train(self):
        scoring = {
            "accuracy": "accuracy",
            "f1_macro": make_scorer(f1_score),
            "roc_auc": "roc_auc",
            "precision": make_scorer(precision_score),
            "recall": make_scorer(recall_score),
        }

        cv_results = cross_validate(
            self.pipeline,
            self.data_module.X_train,
            self.data_module.y_train,
            scoring=scoring,
            cv=self.cv,
            return_train_score=False,
            n_jobs=self.n_jobs,
            verbose=3 if self.verbose else 0,
            return_estimator=True,
            return_indices=True,
            error_score="raise",
        )

        self.train_metrics_report = {
            metric: {
                "folds": cv_results[f"test_{metric}"].tolist(),
                "mean": cv_results[f"test_{metric}"].mean(),
                "std": cv_results[f"test_{metric}"].std(),
            }
            for metric in scoring
        }

        roc_data = []
        for i in range(self.cv):
            estimator = cv_results["estimator"][i]
            train_indices, test_indices = (
                cv_results["indices"]["train"][i],
                cv_results["indices"]["test"][i],
            )

            true_labels = self.data_module.y_train.iloc[test_indices]
            y_pred_proba = estimator.predict_proba(
                self.data_module.X_train.iloc[test_indices]
            )
            roc_data.append((true_labels, y_pred_proba))

        self.train_metrics_report["roc_data"] = roc_data

        return self

    def eval_test(self):
        X_test, y_test = self.data_module.X_test, self.data_module.y_test
        y_pred_proba = (
            self.pipeline.predict_proba(X_test)[:, 1]
            if hasattr(self.pipeline, "predict_proba")
            else np.nan
        )
        test_metrics = {
            "accuracy": self.pipeline.score(X_test, y_test),
            "f1_macro": f1_score(
                y_test, self.pipeline.predict(X_test), average="macro"
            ),
            "roc_auc": (
                roc_auc_score(y_test, y_pred_proba)
                if hasattr(self.pipeline, "predict_proba")
                else np.nan
            ),
            "precision": precision_score(y_test, self.pipeline.predict(X_test)),
            "recall": recall_score(y_test, self.pipeline.predict(X_test)),
        }
        self.test_metrics_report = {
            metric: test_metrics[metric] for metric in test_metrics
        }

        return self

    def get_pipeline(self):
        return self.pipeline

    def get_preprocessor(self):
        return self.preprocessor

    def get_train_metrics_report(self):
        return self.train_metrics_report

    def get_test_metrics_report(self):
        return self.test_metrics_report

    def get_best_params(self):
        if self.param_grid is None:
            raise ValueError(
                "No hyperparameter grid was provided during model training."
            )

        best_param = self.pipeline["model_pipeline"].best_params_
        return {key.split('__')[1]: value for key, value in best_param.items()}

    def get_selected_features(self):
        if not self.select_features:
            raise ValueError("Feature selection was not enabled during model training.")

        if (
            self.pipeline is None
            or "feature_selection"
            not in self.pipeline.named_steps["model_pipeline"].named_steps
        ):
            raise ValueError(
                "Feature selection has not been performed or the model is not fitted."
            )

        rfe = self.pipeline.named_steps["model_pipeline"].named_steps[
            "feature_selection"
        ]
        feature_mask = rfe.support_

        feature_names = self._get_feature_names_from_preprocessor()

        selected_features = [
            feature
            for feature, is_selected in zip(feature_names, feature_mask)
            if is_selected
        ]
        return [
            feature
            for feature in self.data_module.feature_columns
            if any([feature in col for col in selected_features])
        ]

    def _get_feature_names_from_preprocessor(self):
        transformers = self.preprocessor.transformers_
        feature_names = []

        for name, transformer, column in transformers:
            if hasattr(transformer, "get_feature_names_out"):
                feature_names.extend(transformer.get_feature_names_out(column))
            else:
                feature_names.extend(column)

        return feature_names
```

Similarly to the `Trainer` class, the `Visualizer` class is designed to streamline the process of visualizing model performance and explanations. It provides a variety of visualization methods for evaluating models, including confusion matrices, classification reports, ROC curves, precision-recall curves, and feature importances.

```{python}
from sklearn.metrics import roc_curve, classification_report, precision_recall_curve

import plotly.express as px

class Visualizer:
    def __init__(self, trainer, model_name):
        self.trainer = trainer
        self.model_name = model_name

        X_train, X_test, y_train, y_test = (
            self.trainer.data_module.X_train,
            self.trainer.data_module.X_test,
            self.trainer.data_module.y_train,
            self.trainer.data_module.y_test,
        )

        self.explainer = dx.Explainer(trainer.get_pipeline(), X_test, y_test)

        self.X_test = X_test
        self.y_true = y_test
        self.y_test_pred_proba = trainer.get_pipeline().predict_proba(X_test)

    @staticmethod
    def compare_evaluation_metrics(visualizers):
        model_names = [viz.model_name for viz in visualizers]
        metrics = Trainer.get_scoring_metrics()
        
        means = {metric: [] for metric in metrics}
        stds = {metric: [] for metric in metrics}
        for viz in visualizers:
            train_metrics = viz.trainer.get_train_metrics_report()
            for metric in metrics:
                means[metric].append(np.mean(train_metrics[metric]["folds"]))
                stds[metric].append(np.std(train_metrics[metric]["folds"]))
        
        n_groups = len(metrics)
        bar_width = 0.15
        index = np.arange(n_groups)
        opacity = 0.8
        
        plt.figure(figsize=(12, 8))
        colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))
        
        for i, model_name in enumerate(model_names):
            bar_positions = index + bar_width * i
            bar_values = [means[metric][i] for metric in metrics]
            error_values = [stds[metric][i] for metric in metrics]
            
            bars = plt.bar(
                bar_positions,
                bar_values,
                bar_width,
                alpha=opacity,
                color=colors[i],
                yerr=error_values,
                capsize=5,
                label=model_name
            )
            
            for bar, error in zip(bars, error_values):
                yval = bar.get_height()
                plt.text(
                    bar.get_x() + bar.get_width() / 2,
                    yval + error + 0.01,
                    f"{yval:.2f} ± {error:.2f}",
                    ha='center',
                    va='bottom',
                    fontsize=9,
                    rotation=90
                )
        
        plt.xlabel('Metrics', fontsize=14)
        plt.ylabel('Scores', fontsize=14)
        plt.title('Cross-Validation (k={}) Evaluation Metrics Comparison'.format(visualizers[0].trainer.cv), fontsize=16)
        plt.xticks(index + bar_width * (len(model_names) - 1) / 2, metrics, fontsize=12, rotation=90)
        plt.ylim(0, 1.1)
        plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
        plt.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey')
        plt.tight_layout()
        plt.show()

    @staticmethod
    def compare_roc_curves(visualizers, dataset):
        if dataset not in ["test", "eval"]:
            raise ValueError("Invalid dataset option. Choose 'test' or 'eval'.")

        plt.figure(figsize=(8, 8))
        colors = plt.cm.viridis(np.linspace(0, 1, len(visualizers)))

        for i, viz in enumerate(visualizers):
            if dataset == "test":
                y_true = viz.trainer.data_module.y_test
                y_scores = viz.trainer.get_trained_model().predict_proba(
                    viz.trainer.data_module.X_test
                )[:, 1]
            elif dataset == "eval":
                y_true = []
                y_scores = []
                for fold in viz.trainer.get_train_metrics_report()["roc_data"]:
                    y_true.extend(fold[0])
                    y_scores.extend(fold[1][:, 1])

            fpr, tpr, _ = roc_curve(y_true, y_scores)
            auc_score = roc_auc_score(y_true, y_scores)
            plt.plot(
                fpr,
                tpr,
                label=f"{viz.model_name} (AUC = {auc_score:.2f})",
                color=colors[i],
            )

        plt.plot([0, 1], [0, 1], "k--")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        
        title = None
        if dataset == "test":
            title = "ROC Curve Comparison on Test Set"
        elif dataset == "eval":
            title = "ROC Curve Comparison on Evaluation Set (Averaged over Folds with CV=10)"
        
        plt.title(title)
        plt.legend(loc="lower right")
        plt.show()

    def plot_validation_metrics(self):
        train_metrics = self.trainer.get_train_metrics_report()
        cv = len(train_metrics["accuracy"]["folds"])
        metrics = self.trainer.get_scoring_metrics()
        fold_scores = {metric: train_metrics[metric]["folds"] for metric in metrics}

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
        bp = ax1.boxplot(fold_scores.values(), labels=metrics, notch=True, patch_artist=True, positions=np.arange(len(metrics))+1)
        for box in bp['boxes']:
            box.set(color='blue', linewidth=2)
            box.set(facecolor='lightblue')
        ax1.set_title('Boxplot of Metrics')
        ax1.set_ylabel('Scores')
        ax1.set_ylim(0, 1)
        ax1.grid(True)

        means = [np.mean(values) for values in fold_scores.values()]
        std_devs = [np.std(values) for values in fold_scores.values()]
        bar_positions = np.arange(1, len(metrics)+1)
        ax2.bar(bar_positions, means, align='center', alpha=0.7, color='green', capsize=10)
        ax2.set_xticks(bar_positions)
        ax2.set_xticklabels(metrics)
        ax2.set_title('Bar Chart of Average Metrics')
        ax2.set_ylabel('Average Score')
        ax2.set_ylim(0, 1)
        ax2.grid(True)

        for idx, (mean, std) in enumerate(zip(means, std_devs)):
            ax2.text(idx + 1, mean, f'{mean:.2f}±{std:.2f}', ha='center', va='bottom', fontsize=9, color='darkred')

        plt.tight_layout()
        plt.suptitle(f"{self.model_name}: Validation Metrics Comparison (CV={cv})", fontsize=16)
        plt.subplots_adjust(top=0.85)
        plt.show()

    def plot_test_metrics(self):
        test_metrics = self.trainer.get_test_metrics_report()
        test_values = list(test_metrics.values())
        test_names = list(test_metrics.keys())

        sns.barplot(x=test_names, y=test_values)
        plt.title(f"{self.model_name}: Test Metrics")
        plt.xlabel("Metrics")
        plt.ylabel("Score")
        for i, v in enumerate(test_values):
            if np.isnan(v):
                plt.text(i, 0.5, "N/A", ha="center", va="bottom")
            else:
                plt.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom")
        plt.ylim(0, 1)
        plt.grid(True)
        plt.show()

    def plot_confusion_matrix_test(self):
        preds = self.y_test_pred_proba.argmax(axis=1)
        skplt.metrics.plot_confusion_matrix(self.y_true, preds)
        plt.title(f"{self.model_name}: Confusion Matrix")
        plt.show()
        
    def plot_confusion_matrix_eval(self):
        y_true = []
        y_pred = []
        for fold in self.trainer.get_train_metrics_report()["roc_data"]:
            y_true.extend(fold[0])
            y_pred.extend(fold[1][:, 1].argmax(axis=1))
                
    def plot_classification_report_test(self):
        preds = self.y_test_pred_proba.argmax(axis=1)
        report = classification_report(self.y_true, preds, output_dict=True)

        report_df = pd.DataFrame(report).transpose()
        report_df = report_df.round(2)

        table = plt.table(
            cellText=report_df.values,
            colLabels=report_df.columns,
            rowLabels=report_df.index,
            cellLoc="center",
            rowLoc="center",
            loc="center",
            fontsize=12,
        )
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1.2, 1.2)

        plt.axis("off")
        plt.title(f"{self.model_name}: Classification Report")
        plt.show()

    def plot_roc_curve_test(self):
        skplt.metrics.plot_roc(
            self.y_true, self.y_test_pred_proba, plot_micro=False, plot_macro=False
        )
        plt.title(f"{self.model_name}: ROC Curve on Test Set")
        plt.show()

    def plot_roc_curve_eval(self, show_folds=False):
        fig, ax = plt.subplots(figsize=(8, 8))
        colors = plt.cm.viridis(np.linspace(0, 1, self.trainer.cv))

        roc_data = self.trainer.get_train_metrics_report()["roc_data"]
        for k in range(self.trainer.cv):
            true_labels, y_pred_proba = roc_data[k]
            fpr, tpr, _ = roc_curve(true_labels, y_pred_proba[:, 1])
            auc_score = roc_auc_score(true_labels, y_pred_proba[:, 1])
            ax.plot(
                fpr, tpr, color=colors[k], label=f"Fold {k + 1} (AUC = {auc_score:.2f})"
            )

        plt.title(
            f"{self.model_name}: ROC Curves for each fold (CV={self.trainer.cv}, "
            f'Mean AUROC={self.trainer.train_metrics_report["roc_auc"]["mean"]:.2f})'
        )
        if show_folds:
            plt.legend(loc="lower right")
            
        plt.plot([0, 1], [0, 1], color="gray", linestyle="--")

        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.grid(True)
        plt.show()

    def plot_precision_recall_curve_test(self):
        skplt.metrics.plot_precision_recall(self.y_true, self.y_test_pred_proba)
        plt.title(f"{self.model_name}: Precision-Recall Curve on Test Set")
        plt.show()

    def plot_lift_curve_test(self):
        skplt.metrics.plot_lift_curve(self.y_true, self.y_test_pred_proba)
        plt.title(f"{self.model_name}: Lift Curve on Test Set")
        plt.show()

    def plot_cumulative_gain_curve_test(self):
        skplt.metrics.plot_cumulative_gain(self.y_true, self.y_test_pred_proba)
        plt.title(f"{self.model_name}: Cumulative Gain Curve on Test Set")
        plt.show()

    def plot_partial_dependence_test(self, feature):
        pdp = self.explainer.model_profile(type="partial", variables=feature)
        pdp.plot()

    def plot_accumulated_local_effects_test(self, feature):
        ale = self.explainer.model_profile(type="accumulated", variables=feature)
        ale.plot()

    def plot_breakdown_test(self, observation):
        breakdown = self.explainer.predict_parts(observation, type="break_down")
        breakdown.plot()

    def plot_model_explanations_test(self):
        feature_importance = self.explainer.model_parts()
        feature_importance.plot()

        # model_profile = self.explainer.model_profile(type="partial")
        # model_profile.plot()

    def plot_grid_search(self, log_scale_params):
        if self.trainer.param_grid is None:
            raise ValueError("No hyperparameter grid was provided during model training.")
        
        cv_results = pd.DataFrame(self.trainer.get_pipeline().named_steps["model_pipeline"].cv_results_)
        
        def shorten_param(param_name):
            if "__" in param_name:
                return param_name.rsplit("__", 1)[1]
            return param_name
        
        cv_results = cv_results.rename(shorten_param, axis=1)
        
        params = {}
        for param in log_scale_params:
            if cv_results[param].dtype in ["int64", "float64"]:
                params[param] = lambda x: np.log10(x) if x > 0 else 0
            else:
                params[param] = lambda x: x
        
        fig = px.parallel_coordinates(
            cv_results.apply(
                {
                    **params,
                    'mean_test_score': lambda x: x
                }
            ),
            color="mean_test_score",
            color_continuous_scale=px.colors.sequential.Viridis
        )
        fig.show()

    def visualize_explanations_test(self, feature_columns=[]):
        self.plot_model_explanations()

        if not feature_columns:
            feature_columns = self.trainer.data_module.feature_columns[0]

        self.plot_partial_dependence(feature_columns)
        self.plot_accumulated_local_effects(feature_columns)

        observation = self.trainer.data_module.X_test.iloc[0]
        self.plot_breakdown(observation)

        plt.show()
```

## Baseline Model: Logistic Regression

We will start by training a baseline logistic regression model using a subset of features. The features include the client's age, region, and aggregated balance and volume information from the transactional data. The goal is to establish a baseline performance level that we can compare against more complex models.

The logistic regression model is a simple yet effective model for binary classification tasks. It provides interpretable results and can serve as a good starting point for more complex models.

```{python}
baseline_feature_columns = ["age", "client_region"] + [
    col
    for col in golden_record_df.columns
    if "M_" in col and ("_balance" in col or "_volume" in col)
]

baseline_data_module = create_data_module(golden_record_df, baseline_feature_columns)

print(f"Number of baseline feature columns: {len(baseline_feature_columns)}")
print(f"Baseline feature columns: {baseline_feature_columns}")
```

Filtering the relevant columns for the baseline model, we have 26 columns.

```{python}
from sklearn.linear_model import LogisticRegression

baseline_trainer = (
    Trainer(baseline_data_module, LogisticRegression(max_iter=10000, random_state=SEED)).fit().eval_train()
)

baseline_visualizer = Visualizer(baseline_trainer, "Baseline Logistic Regression")
baseline_visualizer.plot_validation_metrics()
```

The baseline model hits quite high scores in all metrics showing good robustness across folds. 

The confusion matrix shows that the model is performing well on the test set with a high number of true positives and true negatives. There is an equal number of false positives and false negatives.

```{python}
baseline_visualizer.plot_roc_curve_eval(show_folds=True)
```

The ROC curve shows that the model has a high true positive rate across different thresholds. The AUC score is also quite high, indicating good performance.

## Adding more features

In order to possibly improve the model performance, we will include more features in the training data. We will include all features except for the ones that are not relevant for the model training.

After merging the transactional and non-transactional data, we have many columns that are unnecessary for model training. We will remove all columns containing card-related information, except for the `has_card` column. This decision stems from the fact that 50% of our dataset consists of cardholders and the other 50% consists of non-cardholders, which we matched with the cardholders. Therefore, the data in the non-target card-related columns come from the actual cardholders.

Additionally we will remove all columns that contain time-dependent information, such as dates and IDs, as they are not relevant for the model.

```{python}
num_cols_before = len(golden_record_df.columns)
print(f"Number of columns before filtering: {num_cols_before}")

golden_record_df = golden_record_df.loc[
    :,
    ~golden_record_df.columns.str.contains("card")
    | golden_record_df.columns.str.contains("has_card"),
]
print(
    f"Removed {num_cols_before - len(golden_record_df.columns)} card-related columns. Now {len(golden_record_df.columns)} columns remain."
)

num_cols_before = len(golden_record_df.columns)
golden_record_df = golden_record_df.drop(
    columns=["loan_granted_date", "birth_date", "account_created"]
)
print(
    f"Removed {num_cols_before - len(golden_record_df.columns)} time-dependent columns. Now {len(golden_record_df.columns)} columns remain."
)

num_cols_before = len(golden_record_df.columns)
golden_record_df = golden_record_df.drop(
    columns=[
        "loan_account_id",
        "loan_loan_id",
        "order_account_id",
        "client_district_name",
        "disp_id",
        "account_id",
        "account_district_name",
    ]
)
print(
    f"Removed {num_cols_before - len(golden_record_df.columns)} ID columns. Now {len(golden_record_df.columns)} columns remain."
)

num_cols_before = len(golden_record_df.columns)
golden_record_df = golden_record_df.drop(
    columns=[col for col in golden_record_df.columns if "std" in col]
)
print(
    f"Removed {num_cols_before - len(golden_record_df.columns)} std columns. Now {len(golden_record_df.columns)} columns remain."
)

cols_to_exclude_in_train = ["client_id", "has_card"]
all_cols_data_module = create_data_module(
    golden_record_df, golden_record_df.drop(columns=cols_to_exclude_in_train).columns
)

print(f"Number of all feature columns: {len(all_cols_data_module.feature_columns)}")
del num_cols_before
```

In total we remove 30 columns from the dataset. The remaining columns are used for training the models.

## Candidate Models

We will now train multiple candidate models using the expanded feature set and evaluate their performance. The candidate models include:

- Logistic Regression
- Random Forest
- Decision Tree
- Gradient Boosting

We will train each model using the same cross-validation strategy and evaluation metrics to ensure a fair comparison. After training the models, we will evaluate their performance across folds. 

### Logistic Regression

As a direct extension of the baseline model, we will train a logistic regression model using the expanded feature set. The hypothesis is that the additional features will improve the model's performance by capturing more complex relationships but also potentially introduce noise and reduce generalization.

```{python}
log_reg_trainer = (
    Trainer(all_cols_data_module, LogisticRegression(max_iter=10000, random_state=SEED)).fit().eval_train()
)

log_reg_visualizer = Visualizer(log_reg_trainer, "Logistic Regression")
log_reg_visualizer.plot_validation_metrics()
```

As hypothesized, the logistic regression model with the expanded feature set performs worse than the baseline model. This indicates that the additional features might have introduced noise or overfitting. The model's performance is still quite good, but it is slightly worse than the baseline model. We still have a fairly high AUC score as in the baseline model.

```{python}
log_reg_visualizer.plot_roc_curve_eval(show_folds=True)
```

Looking closer at the ROC curves for each fold, we can see some expected variance in the performance across different folds. 

### Random Forest

Next, we will train a Random Forest model to see if it can capture more complex relationships in the data and outperform the logistic regression model.

```{python}
#| lines_to_next_cell: 0
from sklearn.ensemble import RandomForestClassifier

rf_trainer = (
    Trainer(
        all_cols_data_module,
        RandomForestClassifier(random_state=SEED),
    )
    .fit()
    .eval_train()
)

rf_visualizer = Visualizer(rf_trainer, "Random Forest")
rf_visualizer.plot_validation_metrics()
```

```{python}
rf_visualizer.plot_roc_curve_eval(show_folds=True)
```

### Decision Tree

We will also train a Decision Tree model to see how it performs compared to the other models. Decision Trees are known for their interpretability and simplicity.

```{python}
#| lines_to_next_cell: 0
from sklearn.tree import DecisionTreeClassifier

decision_tree_trainer = (
    Trainer(
        all_cols_data_module,
        DecisionTreeClassifier(random_state=SEED),
    )
    .fit()
    .eval_train()
)

decision_tree_visualizer = Visualizer(decision_tree_trainer, "Decision Tree")
decision_tree_visualizer.plot_validation_metrics()
```

```{python}
decision_tree_visualizer.plot_roc_curve_eval(show_folds=True)
```

### Gradient Boosting

Finally, we will train a Gradient Boosting model to see if it can outperform the other models. Gradient Boosting models are known for their high accuracy and ability to capture complex relationships in the data.

```{python}
#| lines_to_next_cell: 0
from sklearn.ensemble import GradientBoostingClassifier

gradient_boost_trainer = (
    Trainer(
        all_cols_data_module,
        GradientBoostingClassifier(random_state=SEED),
    )
    .fit()
    .eval_train()
)

gradient_boost_visualizer = Visualizer(gradient_boost_trainer, "Gradient Boosting")
gradient_boost_visualizer.plot_validation_metrics()
```

```{python}
gradient_boost_visualizer.plot_roc_curve_eval(show_folds=True)
```

# Model Comparison & Selection

We have trained and evaluated multiple candidate models using the expanded feature set. We will now compare the models' performance across the  evaluation metrics and select the best-performing model for further analysis. The evaluation metrics include accuracy, F1 score, AUC-ROC, precision, and recall.

```{python}
candidate_trainers = [
    baseline_trainer,
    log_reg_trainer,
    rf_trainer,
    decision_tree_trainer,
    gradient_boost_trainer,
]
candidate_visualizers = [
    baseline_visualizer,
    log_reg_visualizer,
    rf_visualizer,
    decision_tree_visualizer,
    gradient_boost_visualizer,
]
```

```{python}
Visualizer.compare_evaluation_metrics(candidate_visualizers)
```

The comparison of evaluation metrics across the candidate models shows that the Random Forest model has one of the highest mean scores along with the Baseline and Gradient Boosting models. The Decision Tree model as well as the Logistic Regression model have lower mean scores across the metrics.

Especially the high recall of the Random Forest model is promising, as it indicates that the model can effectively classify positive samples (clients who have a card) without missing many of them.

```{python}
Visualizer.compare_roc_curves(candidate_visualizers, dataset="eval")
```

The ROC curves move all above the diagonal line, which is a good sign and does not show any problems with the models. The ROC curve of a decision tree model tends to be very linear because decision trees make hard predictions, assigning instances to either one class or the other without providing probability estimates. This results in a stair-step or piecewise linear ROC curve.


The curves of the baseline, Gradient Boosting, and Random Forest models are very similar, indicating that they perform similarly across different thresholds. The Logistic Regression model has a slightly lower curve, indicating that it performs worse than the other models.

The AUC scores of the models are also quite high, with the Random Forest model having one of the highest scores.

## Top-N Customer Lists

We will now use the trained models to generate a list of the top N% customers who are most likely to get a card (according to the model). Therefore we will look at the customers who are most likely to get a card but don't have one yet. This list can be used by the marketing team to target potential customers who are likely to get a card.

```{python}
def create_top_n_customers_list(model, data):
    """ Create a list of top N% customers who are most likely to get a card according to the model """
    mandatory_columns = ["client_id", "has_card"]

    if not hasattr(model, "predict_proba"):
        raise ValueError("Model does not support probability predictions")

    if not all(col in data.columns for col in mandatory_columns):
        raise ValueError("Mandatory columns not found in data: 'client_id', 'has_card'")

    data = data[data["has_card"] == 0]

    probabilities = model.predict_proba(data.copy())
    # Probability of having a card (class 1). This essentially gives the clients who should most likely have a card based on the model but don't have one.
    probabilities = probabilities[:, 1]

    results = pd.DataFrame(
        {"Client ID": data["client_id"], "Probability": probabilities}
    )

    return results.sort_values(by="Probability", ascending=False).reset_index(drop=True)


def compare_top_n_lists(*lists, labels, top_n_percent):
    """ Compare the overlap of top N% customer lists generated by different models """
    if len(lists) != len(labels):
        raise ValueError("Each list must have a corresponding label")

    if len(set([len(l) for l in lists])) != 1:
        raise ValueError("All lists must have the same length")

    for l in lists:
        if not l["Probability"].is_monotonic_decreasing:
            raise ValueError("Lists must be sorted in descending order of probability")

    top_n = int(len(lists[0]) * top_n_percent)
    lists = [l.head(top_n) for l in lists]

    overlap_matrix = pd.DataFrame(0, index=labels, columns=labels)

    for i, list1 in enumerate(lists):
        set1 = set(list1["Client ID"])
        for j, list2 in enumerate(lists):
            set2 = set(list2["Client ID"])
            overlap_matrix.iloc[i, j] = len(set1.intersection(set2))

    overlap_matrix = overlap_matrix / len(lists[0])
    return overlap_matrix


def visualize_overlap_matrix(overlap_matrix, title):
    """ Visualize the overlap matrix as a heatmap """
    plt.figure(figsize=(10, 8))

    mask = np.tril(np.ones_like(overlap_matrix, dtype=bool))
    overlap_matrix = overlap_matrix.mask(mask)

    sns.heatmap(
        overlap_matrix,
        annot=True,
        cmap="Blues",
        cbar_kws={"label": "Common Customers [%]"},
    )
    plt.title(title)
    plt.ylabel("List from Model/Method")
    plt.xlabel("List from Model/Method")
    plt.xticks(
        ticks=np.arange(len(overlap_matrix.columns)) + 0.5,
        labels=overlap_matrix.columns,
        rotation=45,
        ha="right",
    )
    plt.yticks(
        ticks=np.arange(len(overlap_matrix.index)) + 0.5,
        labels=overlap_matrix.index,
        rotation=0,
    )
    plt.show()
```

### Top-10% Customer Selection

We will select the top 10% of customers who are most likely to get a card according to each model.

```{python}
customer_lists = [
    create_top_n_customers_list(trainer.get_pipeline(), golden_record_df)
    for trainer in candidate_trainers
]

candidate_labels = [
    "Baseline",
    "Logistic Regression",
    "Random Forest",
    "Decision Tree",
    "Gradient Boosting",
]

top_10_overlap_matrix = compare_top_n_lists(
    *customer_lists, labels=candidate_labels, top_n_percent=0.1
)
visualize_overlap_matrix(
    top_10_overlap_matrix, "Overlap of Top-10% Customer Lists by Model"
)
```

Looking at the overlap matrix of the top 10% customer lists, we can see that the Random Forest model has a high overlap with the other tree-based models (Decision Tree and Gradient Boosting). The Baseline model seems to have only 50% overlap with the Logistic Regression model, indicating that they still share some common predictions as they are both linear models.

### Top-5% Customer Selection

We will select the top 5% of customers who are most likely to get a card according to each model.

```{python}
top_5_overlap_matrix = compare_top_n_lists(
    *customer_lists, labels=candidate_labels, top_n_percent=0.05
)
visualize_overlap_matrix(
    top_5_overlap_matrix, "Overlap of Top-5% Customer Lists by Model"
)
```

Looking at the overlap matrix of the top 5% customer lists, we can see that the overlap between the tree-based models is even higher. Especially the overlap between the Decision Tree and Gradient Boosting models got higher. The Logistic Regression model still has a lower overlap with the other models but gained some overlap with the Gradient Boosting model.

## Selected Model: Random Forest

After evaluating the candidate models, we have selected the Random Forest model as the selected model for further analysis. The Random Forest model has shown one of the highest mean scores across the evaluation metrics and has a high recall, indicating that it can effectively classify positive samples without missing many of them. The model also has a high AUC score, indicating good performance across different thresholds.

After we have now selected the Random Forest model, we will further optimize its hyperparameters using grid search with cross-validation to improve its performance

```{python}
best_model_trainer = rf_trainer
best_model_visualizer = rf_visualizer
```

# Model Optimization

We will perform a GridSearch on a param grid with reasonable values for the Random Forest model to find the best hyperparameters. The GridSearch will be performed using cross-validation with the same settings as the training of the candidate model. The best model will be selected based on the mean AUC score across folds.

The hyperparameters we will tune are:

- `n_estimators`: The number of trees in the forest.
- `max_depth`: The maximum depth of the tree.
- `min_samples_split`: The minimum number of samples required to split an internal node.
- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.

We will use the same data and feature set as before to ensure a fair comparison.

As this process can be computationally expensive, we will cache the trained model to avoid retraining it multiple times.

```{python}
gs_param_grid = {
    "model__n_estimators": [50, 100, 200],
    "model__max_depth": [5, 10, 20],
    "model__min_samples_split": [2, 5, 10],
    "model__min_samples_leaf": [1, 2, 4]
}

try:
    gs_trainer = joblib.load(gs_cache_file)
    print("Loaded cached model")
except FileNotFoundError:
    print("No cached model found, proceeding with training...")
    gs_trainer = Trainer(
        all_cols_data_module,
        RandomForestClassifier(random_state=SEED),
        param_grid=gs_param_grid,
        verbose=False
    ).fit().eval_train()
    
    joblib.dump(gs_trainer, gs_cache_file)  

print("Best Parameters:", gs_trainer.get_best_params())
```

The best hyperparameters found by the GridSearch are:

- `n_estimators`: 200
- `max_depth`: 10
- `min_samples_split`: 2
- `min_samples_leaf`: 4

This indicates that a Random Forest model profits from a higher number of trees in the forest and a higher maximum depth of the trees. The minimum number of samples required to split an internal node and the minimum number of samples required to be at a leaf node are relatively low, indicating that the model can split nodes with fewer samples.

```{python}
gs_visualizer = Visualizer(gs_trainer, "Random Forest Grid Search")
gs_visualizer.plot_grid_search(log_scale_params=["n_estimators", "max_depth", "min_samples_split", "min_samples_leaf"])
```

Looking at the coordinate plot of the GridSearch results, we can see that the model's performance increases with the number of estimators and the maximum depth of the trees. Overall the impact of the `min_samples_split` and `min_samples_leaf` hyperparameters is less pronounced. The best model is found at the highest values of `n_estimators` and possibly mid-range values of `max_depth`.

```{python}
gs_visualizer.plot_validation_metrics()
```

```{python}
Visualizer.compare_evaluation_metrics([best_model_visualizer, gs_visualizer])
```

The Random Forest model after GridSearch optimization shows a slight improvement in the mean scores across the evaluation metrics. Especially the recall profited from the hyperparameter optimization, getting both higher in its mean and lower in its variance.

```{python}
gs_visualizer.plot_roc_curve_eval(show_folds=True)
```

```{python}
Visualizer.compare_roc_curves([best_model_visualizer, gs_visualizer], dataset="eval")
```

The ROC curves of the Random Forest model before and after hyperparameter optimization are moving closely towards the upper left corner, indicating good performance across different thresholds. However the mean AUC score did not change after the optimization with the mean curves of both models being almost identical and the difference negligible.

As we now optimised the model we will evaluate it on the test set to get a final performance estimate.

```{python}
gs_trainer.eval_test()
gs_visualizer.plot_test_metrics()
```

The model performs well on the test set, with high scores across the evaluation metrics. The AUC score is high, indicating good performance across different thresholds.

```{python}
_, _ = (
    gs_visualizer.plot_confusion_matrix_test(),
    gs_visualizer.plot_classification_report_test()
)
```

The confusion matrix shows that the model is performing well on the test set with a high number of true positives and true negatives. However, there is an unequal number of false positives and false negatives: The false positives are higher than the false negatives. This hint that the model may be slightly biased towards predicting positive samples (clients who have a card).

```{python}
gs_visualizer.plot_lift_curve_test()
```

```{python}
best_model_visualizer.plot_roc_curve_test()
```

```{python}
_, _ = (
    best_model_visualizer.plot_confusion_matrix_test(),
    best_model_visualizer.plot_classification_report_test(),
)
```

# Model Explanation & Reduction

```{python}
try:
    reduced_best_model_trainer = joblib.load(reduced_model_cache_file)
    print("Loaded cached reduced best model")
except FileNotFoundError:
    print("No cached reduced best model found, proceeding with training...")
    reduced_best_model_trainer = (
        Trainer(
            all_cols_data_module,
            RandomForestClassifier(**gs_trainer.get_best_params(), random_state=SEED),
            select_features=True,
        )
        .fit()
        .eval_train()
    )
    joblib.dump(reduced_best_model_trainer, reduced_model_cache_file)

selected_features = reduced_best_model_trainer.get_selected_features()
print("Selected Features:", selected_features)
```

```{python}
reduced_best_model_visualizer = Visualizer(
    reduced_best_model_trainer, "Reduced Random Forest"
)

reduced_best_model_visualizer.plot_validation_metrics()
```

```{python}
Visualizer.compare_evaluation_metrics(
    [best_model_visualizer, gs_visualizer, reduced_best_model_visualizer]
)
```

```{python}
reduced_best_model_visualizer.plot_confusion_matrix_test()
```

```{python}
reduced_best_model_visualizer.plot_model_explanations_test()
```

## Lift Curve

```{python}
reduced_best_model_visualizer.plot_lift_curve_test()
```

## Top-N Customer List

We will generate a list of the top 10% and top 5% customers who are most likely to get a card according to the reduced Random Forest model.

```{python}
rf_models = [rf_trainer, reduced_best_model_trainer]

rf_customer_lists = [
    create_top_n_customers_list(trainer.get_pipeline(), golden_record_df)
    for trainer in rf_models
]

rf_labels = ["Random Forest", "Reduced Random Forest"]

top_10_overlap_matrix_rf = compare_top_n_lists(
    *rf_customer_lists, labels=rf_labels, top_n_percent=0.1
)
```

```{python}
top_5_overlap_matrix_rf = compare_top_n_lists(
    *rf_customer_lists, labels=rf_labels, top_n_percent=0.05
)
```

Looking at the top 10% and top 5% customer lists, we can see that 

## Breakdown of selected model on top and bottom prediction.

To better understand the model's predictions, we will visualize the breakdown of the top and bottom predictions from the reduced Random Forest model. 

```{python}
customer_lists_reduced_rf = customer_lists[-1] 
print(customer_lists_reduced_rf.head())
top_prob_client = customer_lists_reduced_rf['Client ID'][0]

top_client = golden_record_df[golden_record_df['client_id'] == top_prob_client]
top_client  = top_client.apply(pd.to_numeric, errors='coerce')

# drop has card and label required for explainer to work properly
top_client = top_client.drop(columns=['has_card', 'client_id'])

reduced_best_model_visualizer.plot_breakdown_test(top_client)
```

```{python}
bottom_prob_client = customer_lists_reduced_rf.iloc[-1]['Client ID']
print(customer_lists_reduced_rf.tail())
bottom_client = golden_record_df[golden_record_df['client_id'] == bottom_prob_client]
bottom_client  = bottom_client.apply(pd.to_numeric, errors='coerce')
# drop has card and label
bottom_client = bottom_client.drop(columns=['has_card', 'client_id'])

reduced_best_model_visualizer.plot_breakdown_test(bottom_client)
```

# Conclusion

